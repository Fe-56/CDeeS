{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Overall Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.540440</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.386979</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099370</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652892</td>\n",
       "      <td>3.385207</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.melbands_kurtosis.mean  \\\n",
       "0          1                        -0.290830   \n",
       "1          4                         2.148017   \n",
       "2          5                         4.061978   \n",
       "3          6                         0.609811   \n",
       "4          7                        -0.407720   \n",
       "..       ...                              ...   \n",
       "762      993                        -0.341976   \n",
       "763      996                         0.208018   \n",
       "764      997                        -0.448361   \n",
       "765      999                        -0.296933   \n",
       "766     1000                        -0.599766   \n",
       "\n",
       "     lowlevel.melbands_skewness.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                          -0.161553                      -0.674310   \n",
       "1                           2.355089                       6.176068   \n",
       "2                           3.102229                       2.654601   \n",
       "3                           1.478902                       0.594623   \n",
       "4                          -0.322652                      -0.043632   \n",
       "..                               ...                            ...   \n",
       "762                        -0.376271                      -0.184984   \n",
       "763                        -0.004852                      -0.227602   \n",
       "764                        -0.267697                      -0.099497   \n",
       "765                         0.022632                       0.258205   \n",
       "766                        -0.456381                      -0.371258   \n",
       "\n",
       "     tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                     -0.966508                -0.692188   \n",
       "1                     -0.692793                -0.875022   \n",
       "2                      0.480322                 0.431641   \n",
       "3                     -0.265029                -1.486659   \n",
       "4                      0.343143                 1.010394   \n",
       "..                          ...                      ...   \n",
       "762                    1.148765                 0.172968   \n",
       "763                    1.123782                 0.880366   \n",
       "764                   -1.869459                 0.089641   \n",
       "765                   -1.921844                -0.625854   \n",
       "766                   -0.462222                 0.334123   \n",
       "\n",
       "     tonal.key_edma.strength  tonal.key_temperley.strength  \\\n",
       "0                  -0.466809                     -0.224314   \n",
       "1                  -2.504826                     -2.346037   \n",
       "2                   0.529917                      0.178117   \n",
       "3                  -1.304917                     -1.560061   \n",
       "4                   0.710881                      0.859542   \n",
       "..                       ...                           ...   \n",
       "762                -0.030536                      0.217640   \n",
       "763                 0.113897                      0.233136   \n",
       "764                -0.349415                     -0.176068   \n",
       "765                -0.779872                     -0.526150   \n",
       "766                -1.187954                     -1.193146   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                                  -0.490847   \n",
       "1                                   1.052759   \n",
       "2                                   1.201395   \n",
       "3                                   1.129215   \n",
       "4                                   1.536379   \n",
       "..                                       ...   \n",
       "762                                -0.104210   \n",
       "763                                -1.932951   \n",
       "764                                -0.148750   \n",
       "765                                 0.666036   \n",
       "766                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  ...  tonal.chords_histogram_14  \\\n",
       "0                                  -0.744314  ...                  -0.196902   \n",
       "1                                  -0.515532  ...                   0.004772   \n",
       "2                                  -0.649196  ...                  -0.196902   \n",
       "3                                  -0.547660  ...                  -0.196902   \n",
       "4                                  -0.715476  ...                  -0.196902   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                 1.736366  ...                  -0.196902   \n",
       "763                                 1.310770  ...                  -0.196902   \n",
       "764                                -0.310712  ...                  -0.099370   \n",
       "765                                -1.240647  ...                  -0.196902   \n",
       "766                                 0.160289  ...                   1.652892   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                    -0.312361                  -0.251295   \n",
       "1                     0.540440                   4.181020   \n",
       "2                    -0.312361                  -0.251295   \n",
       "3                     0.386979                  -0.251295   \n",
       "4                    -0.312361                  -0.251295   \n",
       "..                         ...                        ...   \n",
       "762                  -0.312361                  -0.251295   \n",
       "763                  -0.312361                  -0.251295   \n",
       "764                  -0.027929                  -0.251295   \n",
       "765                  -0.312361                  -0.251295   \n",
       "766                   3.385207                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.671714  \n",
       "1                    -0.053671  \n",
       "2                    -0.907128  \n",
       "3                    -0.907128  \n",
       "4                    -0.375831  \n",
       "..                         ...  \n",
       "762                   3.825664  \n",
       "763                  -0.814508  \n",
       "764                  -0.088299  \n",
       "765                  -0.798329  \n",
       "766                  -0.907128  \n",
       "\n",
       "[767 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean = pd.read_csv(get_pmemo_path('processed/features/standardised_essentia_best_overall_features_mean.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_overall_features_mean = df_essentia_best_overall_features_mean[df_essentia_best_overall_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_overall_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   song_id                                  767 non-null    int64  \n",
      " 1   lowlevel.melbands_kurtosis.mean          767 non-null    float64\n",
      " 2   lowlevel.melbands_skewness.mean          767 non-null    float64\n",
      " 3   lowlevel.spectral_energy.mean            767 non-null    float64\n",
      " 4   tonal.chords_strength.mean               767 non-null    float64\n",
      " 5   tonal.hpcp_entropy.mean                  767 non-null    float64\n",
      " 6   tonal.key_edma.strength                  767 non-null    float64\n",
      " 7   tonal.key_temperley.strength             767 non-null    float64\n",
      " 8   rhythm.beats_loudness_band_ratio.mean_0  767 non-null    float64\n",
      " 9   rhythm.beats_loudness_band_ratio.mean_1  767 non-null    float64\n",
      " 10  rhythm.beats_loudness_band_ratio.mean_2  767 non-null    float64\n",
      " 11  rhythm.beats_loudness_band_ratio.mean_3  767 non-null    float64\n",
      " 12  rhythm.beats_loudness_band_ratio.mean_4  767 non-null    float64\n",
      " 13  rhythm.beats_loudness_band_ratio.mean_5  767 non-null    float64\n",
      " 14  tonal.chords_histogram_0                 767 non-null    float64\n",
      " 15  tonal.chords_histogram_1                 767 non-null    float64\n",
      " 16  tonal.chords_histogram_2                 767 non-null    float64\n",
      " 17  tonal.chords_histogram_3                 767 non-null    float64\n",
      " 18  tonal.chords_histogram_4                 767 non-null    float64\n",
      " 19  tonal.chords_histogram_5                 767 non-null    float64\n",
      " 20  tonal.chords_histogram_6                 767 non-null    float64\n",
      " 21  tonal.chords_histogram_7                 767 non-null    float64\n",
      " 22  tonal.chords_histogram_8                 767 non-null    float64\n",
      " 23  tonal.chords_histogram_9                 767 non-null    float64\n",
      " 24  tonal.chords_histogram_10                767 non-null    float64\n",
      " 25  tonal.chords_histogram_11                767 non-null    float64\n",
      " 26  tonal.chords_histogram_12                767 non-null    float64\n",
      " 27  tonal.chords_histogram_13                767 non-null    float64\n",
      " 28  tonal.chords_histogram_14                767 non-null    float64\n",
      " 29  tonal.chords_histogram_15                767 non-null    float64\n",
      " 30  tonal.chords_histogram_16                767 non-null    float64\n",
      " 31  tonal.chords_histogram_17                767 non-null    float64\n",
      " 32  tonal.chords_histogram_18                767 non-null    float64\n",
      " 33  tonal.chords_histogram_19                767 non-null    float64\n",
      " 34  tonal.chords_histogram_20                767 non-null    float64\n",
      " 35  tonal.chords_histogram_21                767 non-null    float64\n",
      " 36  tonal.chords_histogram_22                767 non-null    float64\n",
      " 37  tonal.chords_histogram_23                767 non-null    float64\n",
      "dtypes: float64(37), int64(1)\n",
      "memory usage: 227.8 KB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>-0.154840</td>\n",
       "      <td>...</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>-1.032420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>-1.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>-1.128256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>-0.502120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>-0.198898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>-0.747041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>-1.225680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>-0.090239</td>\n",
       "      <td>...</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                          -0.290830                        -0.161553   \n",
       "1                           2.148017                         2.355089   \n",
       "2                           4.061978                         3.102229   \n",
       "3                           0.609811                         1.478902   \n",
       "4                          -0.407720                        -0.322652   \n",
       "..                               ...                              ...   \n",
       "762                        -0.341976                        -0.376271   \n",
       "763                         0.208018                        -0.004852   \n",
       "764                        -0.448361                        -0.267697   \n",
       "765                        -0.296933                         0.022632   \n",
       "766                        -0.599766                        -0.456381   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                        -0.674310                   -0.966508   \n",
       "1                         6.176068                   -0.692793   \n",
       "2                         2.654601                    0.480322   \n",
       "3                         0.594623                   -0.265029   \n",
       "4                        -0.043632                    0.343143   \n",
       "..                             ...                         ...   \n",
       "762                      -0.184984                    1.148765   \n",
       "763                      -0.227602                    1.123782   \n",
       "764                      -0.099497                   -1.869459   \n",
       "765                       0.258205                   -1.921844   \n",
       "766                      -0.371258                   -0.462222   \n",
       "\n",
       "     tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                  -0.692188                -0.466809   \n",
       "1                  -0.875022                -2.504826   \n",
       "2                   0.431641                 0.529917   \n",
       "3                  -1.486659                -1.304917   \n",
       "4                   1.010394                 0.710881   \n",
       "..                       ...                      ...   \n",
       "762                 0.172968                -0.030536   \n",
       "763                 0.880366                 0.113897   \n",
       "764                 0.089641                -0.349415   \n",
       "765                -0.625854                -0.779872   \n",
       "766                 0.334123                -1.187954   \n",
       "\n",
       "     tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                       -0.224314                                -0.490847   \n",
       "1                       -2.346037                                 1.052759   \n",
       "2                        0.178117                                 1.201395   \n",
       "3                       -1.560061                                 1.129215   \n",
       "4                        0.859542                                 1.536379   \n",
       "..                            ...                                      ...   \n",
       "762                      0.217640                                -0.104210   \n",
       "763                      0.233136                                -1.932951   \n",
       "764                     -0.176068                                -0.148750   \n",
       "765                     -0.526150                                 0.666036   \n",
       "766                     -1.193146                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                  -0.744314   \n",
       "1                                  -0.515532   \n",
       "2                                  -0.649196   \n",
       "3                                  -0.547660   \n",
       "4                                  -0.715476   \n",
       "..                                       ...   \n",
       "762                                 1.736366   \n",
       "763                                 1.310770   \n",
       "764                                -0.310712   \n",
       "765                                -1.240647   \n",
       "766                                 0.160289   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_16  \\\n",
       "0                                   0.149480  ...                  -0.251295   \n",
       "1                                  -0.154840  ...                   4.181020   \n",
       "2                                  -1.032420  ...                  -0.251295   \n",
       "3                                  -1.166668  ...                  -0.251295   \n",
       "4                                  -1.128256  ...                  -0.251295   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                -0.502120  ...                  -0.251295   \n",
       "763                                -0.198898  ...                  -0.251295   \n",
       "764                                -0.747041  ...                  -0.251295   \n",
       "765                                -1.225680  ...                  -0.251295   \n",
       "766                                -0.090239  ...                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  valence_mean_mapped  arousal_mean_mapped  \n",
       "0                     0.671714                0.150               -0.200  \n",
       "1                    -0.053671               -0.425               -0.475  \n",
       "2                    -0.907128               -0.600               -0.700  \n",
       "3                    -0.907128               -0.300                0.025  \n",
       "4                    -0.375831                0.450                0.400  \n",
       "..                         ...                  ...                  ...  \n",
       "762                   3.825664                0.525                0.725  \n",
       "763                  -0.814508                0.125                0.750  \n",
       "764                  -0.088299                0.325                0.425  \n",
       "765                  -0.798329                0.550                0.750  \n",
       "766                  -0.907128                0.150                0.325  \n",
       "\n",
       "[767 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean_whole = pd.merge(df_essentia_best_overall_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_overall_features_mean_whole = df_essentia_best_overall_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_overall_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>-0.154840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.540440</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>-1.032420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>-1.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.386979</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>-1.128256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>-0.502120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>-0.198898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>-0.747041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099370</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>-1.225680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>-0.090239</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652892</td>\n",
       "      <td>3.385207</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                          -0.290830                        -0.161553   \n",
       "1                           2.148017                         2.355089   \n",
       "2                           4.061978                         3.102229   \n",
       "3                           0.609811                         1.478902   \n",
       "4                          -0.407720                        -0.322652   \n",
       "..                               ...                              ...   \n",
       "762                        -0.341976                        -0.376271   \n",
       "763                         0.208018                        -0.004852   \n",
       "764                        -0.448361                        -0.267697   \n",
       "765                        -0.296933                         0.022632   \n",
       "766                        -0.599766                        -0.456381   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                        -0.674310                   -0.966508   \n",
       "1                         6.176068                   -0.692793   \n",
       "2                         2.654601                    0.480322   \n",
       "3                         0.594623                   -0.265029   \n",
       "4                        -0.043632                    0.343143   \n",
       "..                             ...                         ...   \n",
       "762                      -0.184984                    1.148765   \n",
       "763                      -0.227602                    1.123782   \n",
       "764                      -0.099497                   -1.869459   \n",
       "765                       0.258205                   -1.921844   \n",
       "766                      -0.371258                   -0.462222   \n",
       "\n",
       "     tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                  -0.692188                -0.466809   \n",
       "1                  -0.875022                -2.504826   \n",
       "2                   0.431641                 0.529917   \n",
       "3                  -1.486659                -1.304917   \n",
       "4                   1.010394                 0.710881   \n",
       "..                       ...                      ...   \n",
       "762                 0.172968                -0.030536   \n",
       "763                 0.880366                 0.113897   \n",
       "764                 0.089641                -0.349415   \n",
       "765                -0.625854                -0.779872   \n",
       "766                 0.334123                -1.187954   \n",
       "\n",
       "     tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                       -0.224314                                -0.490847   \n",
       "1                       -2.346037                                 1.052759   \n",
       "2                        0.178117                                 1.201395   \n",
       "3                       -1.560061                                 1.129215   \n",
       "4                        0.859542                                 1.536379   \n",
       "..                            ...                                      ...   \n",
       "762                      0.217640                                -0.104210   \n",
       "763                      0.233136                                -1.932951   \n",
       "764                     -0.176068                                -0.148750   \n",
       "765                     -0.526150                                 0.666036   \n",
       "766                     -1.193146                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                  -0.744314   \n",
       "1                                  -0.515532   \n",
       "2                                  -0.649196   \n",
       "3                                  -0.547660   \n",
       "4                                  -0.715476   \n",
       "..                                       ...   \n",
       "762                                 1.736366   \n",
       "763                                 1.310770   \n",
       "764                                -0.310712   \n",
       "765                                -1.240647   \n",
       "766                                 0.160289   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_14  \\\n",
       "0                                   0.149480  ...                  -0.196902   \n",
       "1                                  -0.154840  ...                   0.004772   \n",
       "2                                  -1.032420  ...                  -0.196902   \n",
       "3                                  -1.166668  ...                  -0.196902   \n",
       "4                                  -1.128256  ...                  -0.196902   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                -0.502120  ...                  -0.196902   \n",
       "763                                -0.198898  ...                  -0.196902   \n",
       "764                                -0.747041  ...                  -0.099370   \n",
       "765                                -1.225680  ...                  -0.196902   \n",
       "766                                -0.090239  ...                   1.652892   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                    -0.312361                  -0.251295   \n",
       "1                     0.540440                   4.181020   \n",
       "2                    -0.312361                  -0.251295   \n",
       "3                     0.386979                  -0.251295   \n",
       "4                    -0.312361                  -0.251295   \n",
       "..                         ...                        ...   \n",
       "762                  -0.312361                  -0.251295   \n",
       "763                  -0.312361                  -0.251295   \n",
       "764                  -0.027929                  -0.251295   \n",
       "765                  -0.312361                  -0.251295   \n",
       "766                   3.385207                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.671714  \n",
       "1                    -0.053671  \n",
       "2                    -0.907128  \n",
       "3                    -0.907128  \n",
       "4                    -0.375831  \n",
       "..                         ...  \n",
       "762                   3.825664  \n",
       "763                  -0.814508  \n",
       "764                  -0.088299  \n",
       "765                  -0.798329  \n",
       "766                  -0.907128  \n",
       "\n",
       "[767 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_overall_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bd6be50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 37])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 37])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2848615667519758\n",
      "Valence RMSE: 0.2623395437817343\n",
      "Arousal RMSE: 0.30572894562532393\n",
      "Test R^2 score: tensor([0.2646, 0.3107], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0300, 0.0908], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2876154151963989\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_overall_mean_standardised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0145,  0.0497],\n",
       "        [ 0.0234,  0.0909],\n",
       "        [ 0.3669,  0.4695],\n",
       "        [ 0.1324,  0.2085],\n",
       "        [ 0.4221,  0.5350],\n",
       "        [-0.0135,  0.0494],\n",
       "        [ 0.0444,  0.1136],\n",
       "        [ 0.3839,  0.4897],\n",
       "        [-0.0137,  0.0494],\n",
       "        [ 0.0813,  0.1533],\n",
       "        [ 0.1903,  0.2710],\n",
       "        [-0.0095,  0.0482],\n",
       "        [-0.0090,  0.0481],\n",
       "        [-0.0134,  0.0493],\n",
       "        [ 0.3253,  0.4202],\n",
       "        [ 0.1922,  0.2731],\n",
       "        [ 0.3995,  0.5082],\n",
       "        [ 0.2932,  0.3821],\n",
       "        [-0.0113,  0.0534],\n",
       "        [ 0.0567,  0.1268],\n",
       "        [ 0.0466,  0.1159],\n",
       "        [-0.0133,  0.0512],\n",
       "        [ 0.3890,  0.4958],\n",
       "        [-0.0097,  0.0483],\n",
       "        [-0.0127,  0.0492],\n",
       "        [-0.0151,  0.0498],\n",
       "        [ 0.4579,  0.5774],\n",
       "        [-0.0094,  0.0482],\n",
       "        [ 0.3503,  0.4499],\n",
       "        [ 0.3207,  0.4147],\n",
       "        [-0.0101,  0.0484],\n",
       "        [ 0.2830,  0.3711],\n",
       "        [ 0.2460,  0.3311],\n",
       "        [ 0.1484,  0.2258],\n",
       "        [ 0.1298,  0.2057],\n",
       "        [ 0.3862,  0.4924],\n",
       "        [ 0.3403,  0.4379],\n",
       "        [ 0.4025,  0.5118],\n",
       "        [ 0.3117,  0.4040],\n",
       "        [ 0.3674,  0.4702],\n",
       "        [ 0.4530,  0.5716],\n",
       "        [ 0.4227,  0.5358],\n",
       "        [ 0.3114,  0.4037],\n",
       "        [-0.0137,  0.0510],\n",
       "        [ 0.3407,  0.4384],\n",
       "        [ 0.4042,  0.5138],\n",
       "        [ 0.3336,  0.4300],\n",
       "        [ 0.2136,  0.2961],\n",
       "        [-0.0094,  0.0482],\n",
       "        [ 0.1321,  0.2082],\n",
       "        [-0.0105,  0.0485],\n",
       "        [ 0.1961,  0.2773],\n",
       "        [ 0.2597,  0.3460],\n",
       "        [ 0.3675,  0.4702],\n",
       "        [ 0.0316,  0.0997],\n",
       "        [ 0.4010,  0.5100],\n",
       "        [ 0.0122,  0.0787],\n",
       "        [ 0.0509,  0.1205],\n",
       "        [ 0.4654,  0.5864],\n",
       "        [ 0.4526,  0.5712],\n",
       "        [ 0.3264,  0.4215],\n",
       "        [-0.0129,  0.0492],\n",
       "        [ 0.3533,  0.4534],\n",
       "        [ 0.3776,  0.4822],\n",
       "        [-0.0139,  0.0495],\n",
       "        [-0.0096,  0.0482],\n",
       "        [-0.0154,  0.0499],\n",
       "        [ 0.1023,  0.1760],\n",
       "        [ 0.3480,  0.4471],\n",
       "        [ 0.3194,  0.4132],\n",
       "        [ 0.3869,  0.4933],\n",
       "        [-0.0123,  0.0490],\n",
       "        [ 0.4606,  0.5807],\n",
       "        [ 0.4651,  0.5860],\n",
       "        [-0.0098,  0.0483],\n",
       "        [ 0.3543,  0.4545],\n",
       "        [-0.0118,  0.0489],\n",
       "        [ 0.2866,  0.3750],\n",
       "        [-0.0131,  0.0493],\n",
       "        [ 0.1149,  0.1896],\n",
       "        [ 0.2082,  0.2903],\n",
       "        [-0.0131,  0.0493],\n",
       "        [ 0.4040,  0.5136],\n",
       "        [ 0.4492,  0.5671],\n",
       "        [ 0.2217,  0.3049],\n",
       "        [ 0.1772,  0.2569],\n",
       "        [-0.0118,  0.0489],\n",
       "        [-0.0091,  0.0482],\n",
       "        [ 0.5310,  0.6641],\n",
       "        [-0.0117,  0.0489],\n",
       "        [ 0.3179,  0.4114],\n",
       "        [ 0.1984,  0.2798],\n",
       "        [ 0.3359,  0.4328],\n",
       "        [ 0.0146,  0.0813],\n",
       "        [ 0.1475,  0.2248],\n",
       "        [ 0.2575,  0.3435],\n",
       "        [ 0.4254,  0.5389],\n",
       "        [ 0.4022,  0.5114],\n",
       "        [ 0.3928,  0.5003],\n",
       "        [ 0.1554,  0.2333],\n",
       "        [-0.0147,  0.0497],\n",
       "        [ 0.3559,  0.4565],\n",
       "        [-0.0137,  0.0494],\n",
       "        [ 0.3256,  0.4205],\n",
       "        [-0.0107,  0.0486],\n",
       "        [ 0.0254,  0.0930],\n",
       "        [-0.0111,  0.0487],\n",
       "        [-0.0125,  0.0491],\n",
       "        [ 0.3381,  0.4354],\n",
       "        [ 0.3897,  0.4966],\n",
       "        [ 0.2922,  0.3810],\n",
       "        [ 0.5479,  0.6841],\n",
       "        [-0.0108,  0.0486],\n",
       "        [ 0.4693,  0.5910],\n",
       "        [ 0.1914,  0.2723],\n",
       "        [ 0.4953,  0.6218],\n",
       "        [ 0.2965,  0.3860],\n",
       "        [ 0.3046,  0.3957],\n",
       "        [ 0.3124,  0.4049],\n",
       "        [-0.0099,  0.0483],\n",
       "        [-0.0102,  0.0484],\n",
       "        [-0.0097,  0.0552],\n",
       "        [ 0.1127,  0.1872],\n",
       "        [ 0.1041,  0.1780],\n",
       "        [ 0.1163,  0.1911],\n",
       "        [ 0.2996,  0.3897],\n",
       "        [-0.0134,  0.0494],\n",
       "        [ 0.1143,  0.1890],\n",
       "        [-0.0105,  0.0485],\n",
       "        [ 0.3735,  0.4774],\n",
       "        [-0.0107,  0.0486],\n",
       "        [ 0.1812,  0.2612],\n",
       "        [ 0.4024,  0.5116],\n",
       "        [-0.0118,  0.0489],\n",
       "        [ 0.3374,  0.4346],\n",
       "        [-0.0119,  0.0489],\n",
       "        [ 0.1009,  0.1745],\n",
       "        [ 0.3879,  0.4945],\n",
       "        [-0.0092,  0.0482],\n",
       "        [ 0.3235,  0.4181],\n",
       "        [ 0.4503,  0.5685],\n",
       "        [-0.0098,  0.0483],\n",
       "        [ 0.3429,  0.4411],\n",
       "        [ 0.2672,  0.3540],\n",
       "        [ 0.1060,  0.1800],\n",
       "        [ 0.1653,  0.2440],\n",
       "        [ 0.4306,  0.5451],\n",
       "        [-0.0145,  0.0497],\n",
       "        [ 0.0039,  0.0698],\n",
       "        [ 0.1012,  0.1749],\n",
       "        [ 0.3382,  0.4355],\n",
       "        [ 0.3218,  0.4160],\n",
       "        [-0.0096,  0.0483],\n",
       "        [-0.0123,  0.0490]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2646, 0.3107], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3829316391788403\n",
      "Valence RMSE: 0.3176099438398145\n",
      "Arousal RMSE: 0.43863105697437454\n",
      "Test R^2 score: tensor([-0.0780, -0.4189], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4218, -0.8715], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.24844021471815725\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3815076206326156\n",
      "Valence RMSE: 0.3168122681424735\n",
      "Arousal RMSE: 0.4367220122182313\n",
      "Test R^2 score: tensor([-0.0726, -0.4066], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4147, -0.8553], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2395742228301737\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3801182492323225\n",
      "Valence RMSE: 0.3160526252295839\n",
      "Arousal RMSE: 0.4348453804335287\n",
      "Test R^2 score: tensor([-0.0674, -0.3945], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4079, -0.8393], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.23097429293475957\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.378763411836325\n",
      "Valence RMSE: 0.3153327851235013\n",
      "Arousal RMSE: 0.43299962923545654\n",
      "Test R^2 score: tensor([-0.0626, -0.3827], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4015, -0.8238], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2226392239016547\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3774421895508644\n",
      "Valence RMSE: 0.3146567671944061\n",
      "Arousal RMSE: 0.43118016160842393\n",
      "Test R^2 score: tensor([-0.0580, -0.3711], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3955, -0.8085], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2145657325526351\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37615388286760565\n",
      "Valence RMSE: 0.3140257719731031\n",
      "Arousal RMSE: 0.4293847944786231\n",
      "Test R^2 score: tensor([-0.0538, -0.3597], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3899, -0.7934], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2067489265587239\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3748974328693431\n",
      "Valence RMSE: 0.31343929260303915\n",
      "Arousal RMSE: 0.4276119504837927\n",
      "Test R^2 score: tensor([-0.0498, -0.3485], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3847, -0.7787], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.199180265776564\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3736713407171971\n",
      "Valence RMSE: 0.31289398362418974\n",
      "Arousal RMSE: 0.42586135861165014\n",
      "Test R^2 score: tensor([-0.0462, -0.3375], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3799, -0.7641], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.19184599277108738\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3724741530159041\n",
      "Valence RMSE: 0.31238491066837853\n",
      "Arousal RMSE: 0.4241340082055899\n",
      "Test R^2 score: tensor([-0.0428, -0.3267], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3754, -0.7498], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.18473114887133657\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37130455248573796\n",
      "Valence RMSE: 0.3119073710927526\n",
      "Arousal RMSE: 0.42243098045867356\n",
      "Test R^2 score: tensor([-0.0396, -0.3160], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3712, -0.7358], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.17782194865523115\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37016130326451413\n",
      "Valence RMSE: 0.3114578217854242\n",
      "Arousal RMSE: 0.4207526662038472\n",
      "Test R^2 score: tensor([-0.0366, -0.3056], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3672, -0.7221], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.17110641333037757\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3692878458045309\n",
      "Valence RMSE: 0.31110195317311373\n",
      "Arousal RMSE: 0.4194789635366423\n",
      "Test R^2 score: tensor([-0.0342, -0.2977], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3641, -0.7116], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.16597631759586984\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.368683390515426\n",
      "Valence RMSE: 0.31071421575791563\n",
      "Arousal RMSE: 0.418702234302426\n",
      "Test R^2 score: tensor([-0.0317, -0.2929], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3607, -0.7053], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1622874201628799\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36807728956495833\n",
      "Valence RMSE: 0.31033422456134563\n",
      "Arousal RMSE: 0.41791679943845356\n",
      "Test R^2 score: tensor([-0.0291, -0.2881], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3574, -0.6989], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15860343443544722\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3674720716266858\n",
      "Valence RMSE: 0.3099660117288426\n",
      "Arousal RMSE: 0.4171241043432173\n",
      "Test R^2 score: tensor([-0.0267, -0.2832], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3542, -0.6925], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15494221930848817\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3668696055459032\n",
      "Valence RMSE: 0.3096121608339658\n",
      "Arousal RMSE: 0.4163255034351555\n",
      "Test R^2 score: tensor([-0.0244, -0.2783], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3511, -0.6860], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15131646939329957\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3662712104401011\n",
      "Valence RMSE: 0.30927442492841856\n",
      "Arousal RMSE: 0.415521996144259\n",
      "Test R^2 score: tensor([-0.0221, -0.2733], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3481, -0.6795], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1477349824692864\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3656777785396888\n",
      "Valence RMSE: 0.30895402565716457\n",
      "Arousal RMSE: 0.4147142214413143\n",
      "Test R^2 score: tensor([-0.0200, -0.2684], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3453, -0.6730], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.14420365879309338\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3650899957624731\n",
      "Valence RMSE: 0.3086518662038159\n",
      "Arousal RMSE: 0.41390268844329453\n",
      "Test R^2 score: tensor([-0.0180, -0.2634], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3427, -0.6664], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.14072692956434985\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.364508267436626\n",
      "Valence RMSE: 0.3083684610752393\n",
      "Arousal RMSE: 0.4130876980416996\n",
      "Test R^2 score: tensor([-0.0161, -0.2585], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3403, -0.6599], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1373072976589712\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3639327658944404\n",
      "Valence RMSE: 0.30810400065877125\n",
      "Arousal RMSE: 0.4122693791214853\n",
      "Test R^2 score: tensor([-0.0144, -0.2535], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3380, -0.6533], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1339456737292839\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36336353528641574\n",
      "Valence RMSE: 0.3078584329692851\n",
      "Arousal RMSE: 0.41144781297435623\n",
      "Test R^2 score: tensor([-0.0128, -0.2485], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3358, -0.6467], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.13064203307802058\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36280050832860566\n",
      "Valence RMSE: 0.3076314563203375\n",
      "Arousal RMSE: 0.41062306896863165\n",
      "Test R^2 score: tensor([-0.0113, -0.2435], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3339, -0.6401], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12739550432254476\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.362243522501153\n",
      "Valence RMSE: 0.30742265884846454\n",
      "Arousal RMSE: 0.40979512932028267\n",
      "Test R^2 score: tensor([-0.0099, -0.2385], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3320, -0.6335], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12420460340956785\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3616923071411773\n",
      "Valence RMSE: 0.30723146456135636\n",
      "Arousal RMSE: 0.4089639070550108\n",
      "Test R^2 score: tensor([-0.0087, -0.2335], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3304, -0.6269], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1210671138471151\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3611465660225622\n",
      "Valence RMSE: 0.30705720716431983\n",
      "Arousal RMSE: 0.4081293371325146\n",
      "Test R^2 score: tensor([-0.0075, -0.2284], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3289, -0.6203], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.11798060609077277\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3606059761632885\n",
      "Valence RMSE: 0.30689915285442165\n",
      "Arousal RMSE: 0.4072913577116444\n",
      "Test R^2 score: tensor([-0.0065, -0.2234], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3275, -0.6136], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.11494245506213041\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3600701862439348\n",
      "Valence RMSE: 0.30675650143875216\n",
      "Arousal RMSE: 0.40644990696092015\n",
      "Test R^2 score: tensor([-0.0055, -0.2184], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3263, -0.6070], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.11194983350262067\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3595388492913789\n",
      "Valence RMSE: 0.30662845592108157\n",
      "Arousal RMSE: 0.4056049288642721\n",
      "Test R^2 score: tensor([-0.0047, -0.2133], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3252, -0.6003], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10899995657716222\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3590116060986461\n",
      "Valence RMSE: 0.30651418327409324\n",
      "Arousal RMSE: 0.4047563737347127\n",
      "Test R^2 score: tensor([-0.0040, -0.2082], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3242, -0.5936], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10608995345241179\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35848808952449224\n",
      "Valence RMSE: 0.3064128044418558\n",
      "Arousal RMSE: 0.403904213813025\n",
      "Test R^2 score: tensor([-0.0033, -0.2031], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3233, -0.5869], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10321687982749084\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35796795665920306\n",
      "Valence RMSE: 0.30632348466098436\n",
      "Arousal RMSE: 0.4030484322444164\n",
      "Test R^2 score: tensor([-0.0027, -0.1980], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3225, -0.5802], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10037797917341218\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3574508659541662\n",
      "Valence RMSE: 0.30624537382512407\n",
      "Arousal RMSE: 0.4021890278879778\n",
      "Test R^2 score: tensor([-0.0022, -0.1929], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3219, -0.5735], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.09757050014654378\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3569365109016145\n",
      "Valence RMSE: 0.30617768052901473\n",
      "Arousal RMSE: 0.4013260190607\n",
      "Test R^2 score: tensor([-0.0018, -0.1878], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3213, -0.5667], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.09479194844929839\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3564245822635816\n",
      "Valence RMSE: 0.30611955160499593\n",
      "Arousal RMSE: 0.4004594683718859\n",
      "Test R^2 score: tensor([-0.0014, -0.1827], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3208, -0.5599], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.09203976433349548\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3559147970715327\n",
      "Valence RMSE: 0.3060701931602141\n",
      "Arousal RMSE: 0.39958944231274407\n",
      "Test R^2 score: tensor([-0.0011, -0.1776], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3204, -0.5532], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.08931159727327387\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3554068657011622\n",
      "Valence RMSE: 0.3060287601021646\n",
      "Arousal RMSE: 0.39871603725631566\n",
      "Test R^2 score: tensor([-0.0008, -0.1724], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3200, -0.5464], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.086605020037397\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35490054437724106\n",
      "Valence RMSE: 0.3059944677654796\n",
      "Arousal RMSE: 0.397839387813028\n",
      "Test R^2 score: tensor([-0.0006, -0.1673], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3197, -0.5396], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.08391791660801085\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.354395549254977\n",
      "Valence RMSE: 0.30596639688914545\n",
      "Arousal RMSE: 0.39695966374198216\n",
      "Test R^2 score: tensor([-0.0004, -0.1621], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3195, -0.5328], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.08124783346947717\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3538916262528904\n",
      "Valence RMSE: 0.30594377224871355\n",
      "Arousal RMSE: 0.39607698050512635\n",
      "Test R^2 score: tensor([-0.0002, -0.1570], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3193, -0.5260], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07859262768758724\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3533885365117894\n",
      "Valence RMSE: 0.3059258220572162\n",
      "Arousal RMSE: 0.3951914812777536\n",
      "Test R^2 score: tensor([-1.0666e-04, -1.5179e-01], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3191, -0.5192], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07595024732636868\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3528860469562801\n",
      "Valence RMSE: 0.3059117857770135\n",
      "Arousal RMSE: 0.39430331420820997\n",
      "Test R^2 score: tensor([-1.4888e-05, -1.4662e-01], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3190, -0.5124], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07331868950261711\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35238395002190154\n",
      "Valence RMSE: 0.30590093328910206\n",
      "Arousal RMSE: 0.3934126529217538\n",
      "Test R^2 score: tensor([ 5.6064e-05, -1.4145e-01], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3189, -0.5055], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07069612198485464\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35188205140911094\n",
      "Valence RMSE: 0.30589255496227374\n",
      "Arousal RMSE: 0.39251968234270845\n",
      "Test R^2 score: tensor([ 1.1084e-04, -1.3627e-01], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3188, -0.4987], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06808080852137183\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35138022601342955\n",
      "Valence RMSE: 0.30588602325262015\n",
      "Arousal RMSE: 0.39162465096721794\n",
      "Test R^2 score: tensor([ 0.0002, -0.1311], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3188, -0.4919], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06547146053389474\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.350878339091725\n",
      "Valence RMSE: 0.30588071945357936\n",
      "Arousal RMSE: 0.3907277865139562\n",
      "Test R^2 score: tensor([ 0.0002, -0.1259], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3187, -0.4851], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06286675231215777\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3503763286117606\n",
      "Valence RMSE: 0.3058762027646873\n",
      "Arousal RMSE: 0.38982931121860087\n",
      "Test R^2 score: tensor([ 0.0002, -0.1207], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3187, -0.4782], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06026591814836424\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3498741501231649\n",
      "Valence RMSE: 0.30587197305777863\n",
      "Arousal RMSE: 0.38892952825229843\n",
      "Test R^2 score: tensor([ 0.0002, -0.1156], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3186, -0.4714], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.05766822485730638\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3493717215741798\n",
      "Valence RMSE: 0.3058676783534076\n",
      "Arousal RMSE: 0.388028559528946\n",
      "Test R^2 score: tensor([ 0.0003, -0.1104], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3186, -0.4646], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.05507289686972333\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3488692387120745\n",
      "Valence RMSE: 0.3058630574732932\n",
      "Arousal RMSE: 0.38712695787321993\n",
      "Test R^2 score: tensor([ 0.0003, -0.1053], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3186, -0.4578], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.05248068252264948\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3483661192391597\n",
      "Valence RMSE: 0.30585584863404375\n",
      "Arousal RMSE: 0.3862254599632886\n",
      "Test R^2 score: tensor([ 0.0004, -0.1001], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3185, -0.4510], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.04988630047394932\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3478603005612302\n",
      "Valence RMSE: 0.30584263411079393\n",
      "Arousal RMSE: 0.385323060007145\n",
      "Test R^2 score: tensor([ 0.0004, -0.0950], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3184, -0.4443], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.04727572108813882\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3473562280416179\n",
      "Valence RMSE: 0.30583481544436963\n",
      "Arousal RMSE: 0.3844187352103294\n",
      "Test R^2 score: tensor([ 0.0005, -0.0899], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4375], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.04468332638335626\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3468775910785801\n",
      "Valence RMSE: 0.3058296291961336\n",
      "Arousal RMSE: 0.3835575110602225\n",
      "Test R^2 score: tensor([ 0.0005, -0.0850], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4310], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.042227479380698374\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3464060322449378\n",
      "Valence RMSE: 0.3058370920590606\n",
      "Arousal RMSE: 0.38269825120088996\n",
      "Test R^2 score: tensor([ 0.0005, -0.0801], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4246], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03982398524190145\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3459449881354004\n",
      "Valence RMSE: 0.3058323906297381\n",
      "Arousal RMSE: 0.3818670167397336\n",
      "Test R^2 score: tensor([ 0.0005, -0.0754], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4185], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0374651054491309\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3454902345164049\n",
      "Valence RMSE: 0.3058326487648921\n",
      "Arousal RMSE: 0.3810425110691077\n",
      "Test R^2 score: tensor([ 0.0005, -0.0708], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4123], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.035146438839833094\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34503798740906033\n",
      "Valence RMSE: 0.30582933689324315\n",
      "Arousal RMSE: 0.3802247259266931\n",
      "Test R^2 score: tensor([ 0.0005, -0.0662], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.4063], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03283996379633042\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3445864486045869\n",
      "Valence RMSE: 0.30582046895549536\n",
      "Arousal RMSE: 0.37941202127988005\n",
      "Test R^2 score: tensor([ 0.0006, -0.0617], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3182, -0.4003], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.030534479595353048\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.344137684273111\n",
      "Valence RMSE: 0.30580971423914804\n",
      "Arousal RMSE: 0.3786052167504952\n",
      "Test R^2 score: tensor([ 0.0007, -0.0571], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3181, -0.3943], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.028244175705804675\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34369109435275175\n",
      "Valence RMSE: 0.3057984185470862\n",
      "Arousal RMSE: 0.3778021491321685\n",
      "Test R^2 score: tensor([ 0.0007, -0.0527], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3180, -0.3884], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.02596731789504908\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.343246687470888\n",
      "Valence RMSE: 0.305792304977128\n",
      "Arousal RMSE: 0.37699820044166\n",
      "Test R^2 score: tensor([ 0.0008, -0.0482], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3180, -0.3825], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.023709701415305373\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34280641038726195\n",
      "Valence RMSE: 0.3057900767225835\n",
      "Arousal RMSE: 0.37619795185938504\n",
      "Test R^2 score: tensor([ 0.0008, -0.0437], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3179, -0.3767], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.02147981353790268\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34237098576925096\n",
      "Valence RMSE: 0.30579019410303887\n",
      "Arousal RMSE: 0.37540397038875745\n",
      "Test R^2 score: tensor([ 0.0008, -0.0393], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3179, -0.3709], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01927966425275024\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34194147455847634\n",
      "Valence RMSE: 0.30579273455370964\n",
      "Arousal RMSE: 0.3746181356269538\n",
      "Test R^2 score: tensor([ 0.0008, -0.0350], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3180, -0.3651], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.017114589435924032\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34151867037910244\n",
      "Valence RMSE: 0.3057979837373719\n",
      "Arousal RMSE: 0.3738416744788869\n",
      "Test R^2 score: tensor([ 0.0007, -0.0307], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3180, -0.3595], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.014988763769246793\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34110406128283993\n",
      "Valence RMSE: 0.30580645502898\n",
      "Arousal RMSE: 0.37307689999503224\n",
      "Test R^2 score: tensor([ 0.0007, -0.0265], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3181, -0.3539], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.012910068472989478\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34069894870863454\n",
      "Valence RMSE: 0.30581778869127424\n",
      "Arousal RMSE: 0.37232650647290316\n",
      "Test R^2 score: tensor([ 0.0006, -0.0224], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3182, -0.3485], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.010884529294687806\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3403042580995189\n",
      "Valence RMSE: 0.3058322125307276\n",
      "Arousal RMSE: 0.3715920262060098\n",
      "Test R^2 score: tensor([ 0.0005, -0.0183], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3183, -0.3432], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.008916852218534499\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33992185047951917\n",
      "Valence RMSE: 0.30584962540552496\n",
      "Arousal RMSE: 0.3708769816342627\n",
      "Test R^2 score: tensor([ 0.0004, -0.0144], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3184, -0.3380], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.007016084057190508\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33955401466808466\n",
      "Valence RMSE: 0.3058701457101461\n",
      "Arousal RMSE: 0.3701855098699593\n",
      "Test R^2 score: tensor([ 0.0003, -0.0106], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3186, -0.3330], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00519360065501373\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3392050050597231\n",
      "Valence RMSE: 0.3058981946173243\n",
      "Arousal RMSE: 0.36952180645395627\n",
      "Test R^2 score: tensor([ 7.3968e-05, -7.0238e-03], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3189, -0.3282], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0034749289912234804\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33887499263751575\n",
      "Valence RMSE: 0.30593450187387716\n",
      "Arousal RMSE: 0.3688856216137128\n",
      "Test R^2 score: tensor([-0.0002, -0.0036], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3192, -0.3237], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0018613742556731205\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3385603546766976\n",
      "Valence RMSE: 0.3059764402698833\n",
      "Arousal RMSE: 0.36827251528914434\n",
      "Test R^2 score: tensor([-0.0004, -0.0002], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3195, -0.3193], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0003319093614103652\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382586472629775\n",
      "Valence RMSE: 0.306019545399192\n",
      "Arousal RMSE: 0.3676817410887175\n",
      "Test R^2 score: tensor([-0.0007,  0.0030], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3199, -0.3150], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0011303943583141352\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3379705559879011\n",
      "Valence RMSE: 0.30605437067880736\n",
      "Arousal RMSE: 0.36712248040393974\n",
      "Test R^2 score: tensor([-0.0009,  0.0060], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3202, -0.3110], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0025318641807047237\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33770298924264675\n",
      "Valence RMSE: 0.3060887416799836\n",
      "Arousal RMSE: 0.36660100941432594\n",
      "Test R^2 score: tensor([-0.0012,  0.0088], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3205, -0.3073], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0038303347397448673\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3374589551591157\n",
      "Valence RMSE: 0.3061356533479086\n",
      "Arousal RMSE: 0.3661120792645514\n",
      "Test R^2 score: tensor([-0.0015,  0.0115], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3209, -0.3038], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.004997904663369135\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3372360945266441\n",
      "Valence RMSE: 0.30618447453810166\n",
      "Arousal RMSE: 0.3656602719123228\n",
      "Test R^2 score: tensor([-0.0018,  0.0139], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3213, -0.3006], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0060573353005460295\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33701651874568717\n",
      "Valence RMSE: 0.30622167460396693\n",
      "Arousal RMSE: 0.36522397760506686\n",
      "Test R^2 score: tensor([-0.0020,  0.0163], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3217, -0.2975], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.007111479587686376\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33679231058961323\n",
      "Valence RMSE: 0.3062542525747899\n",
      "Arousal RMSE: 0.36478274866615856\n",
      "Test R^2 score: tensor([-0.0023,  0.0186], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3219, -0.2944], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.008192607260037232\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33655537178266925\n",
      "Valence RMSE: 0.3062934559356471\n",
      "Arousal RMSE: 0.3643121675192022\n",
      "Test R^2 score: tensor([-0.0025,  0.0212], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3223, -0.2910], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0093294691726451\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33628407772348573\n",
      "Valence RMSE: 0.3063182670154312\n",
      "Arousal RMSE: 0.3637898859964826\n",
      "Test R^2 score: tensor([-0.0027,  0.0240], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3225, -0.2873], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.010650511612269764\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33598015948636684\n",
      "Valence RMSE: 0.30634358884149976\n",
      "Arousal RMSE: 0.36320647118777927\n",
      "Test R^2 score: tensor([-0.0028,  0.0271], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3227, -0.2832], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.012131631185166913\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3356386002715633\n",
      "Valence RMSE: 0.3063729628236987\n",
      "Arousal RMSE: 0.36254951059839674\n",
      "Test R^2 score: tensor([-0.0030,  0.0306], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3230, -0.2786], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01379363318354837\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3352564901803991\n",
      "Valence RMSE: 0.3064050154077892\n",
      "Arousal RMSE: 0.36181458642392045\n",
      "Test R^2 score: tensor([-0.0032,  0.0345], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3232, -0.2734], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01565173053974217\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3348147766671287\n",
      "Valence RMSE: 0.3064284415799014\n",
      "Arousal RMSE: 0.3609757326195647\n",
      "Test R^2 score: tensor([-0.0034,  0.0390], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3234, -0.2675], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.017810800397928428\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3343088464508614\n",
      "Valence RMSE: 0.30644655670568866\n",
      "Arousal RMSE: 0.3600212736962059\n",
      "Test R^2 score: tensor([-0.0035,  0.0441], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3236, -0.2608], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.02028906423481469\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33374447663379425\n",
      "Valence RMSE: 0.30645594898570083\n",
      "Arousal RMSE: 0.3589644866813485\n",
      "Test R^2 score: tensor([-0.0036,  0.0497], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3237, -0.2534], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.023060108556083758\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33311098131531675\n",
      "Valence RMSE: 0.3064518667450755\n",
      "Arousal RMSE: 0.3577891908850309\n",
      "Test R^2 score: tensor([-0.0035,  0.0559], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3236, -0.2452], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.026179800665887376\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33241879863779833\n",
      "Valence RMSE: 0.30641815945447437\n",
      "Arousal RMSE: 0.35652829751946274\n",
      "Test R^2 score: tensor([-0.0033,  0.0626], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3234, -0.2365], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.029611409344379058\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33166533402248694\n",
      "Valence RMSE: 0.30635039911251577\n",
      "Arousal RMSE: 0.3551805464099884\n",
      "Test R^2 score: tensor([-0.0029,  0.0696], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3228, -0.2271], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03337031145442221\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33086128170882284\n",
      "Valence RMSE: 0.30624100652316083\n",
      "Arousal RMSE: 0.3537722733506562\n",
      "Test R^2 score: tensor([-0.0022,  0.0770], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3218, -0.2174], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0374099371218472\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3299905631017301\n",
      "Valence RMSE: 0.30608557073103493\n",
      "Arousal RMSE: 0.35227711657536087\n",
      "Test R^2 score: tensor([-0.0012,  0.0848], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3205, -0.2072], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.041811173367026166\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3290522674457441\n",
      "Valence RMSE: 0.305883125575396\n",
      "Arousal RMSE: 0.35069403033213653\n",
      "Test R^2 score: tensor([0.0002, 0.0930], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3187, -0.1963], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.046576779375432864\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3280410028091715\n",
      "Valence RMSE: 0.3056091587871168\n",
      "Arousal RMSE: 0.34903415465183996\n",
      "Test R^2 score: tensor([0.0020, 0.1015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3164, -0.1850], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.05175474872924746\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3269489853739776\n",
      "Valence RMSE: 0.3052380074688434\n",
      "Arousal RMSE: 0.3473053942434485\n",
      "Test R^2 score: tensor([0.0044, 0.1104], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3132, -0.1733], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0574050964692544\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32577575321222463\n",
      "Valence RMSE: 0.3047649488762014\n",
      "Arousal RMSE: 0.34551122803531414\n",
      "Test R^2 score: tensor([0.0075, 0.1196], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3091, -0.1612], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.06353054752221804\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32452179295675476\n",
      "Valence RMSE: 0.30418235519495007\n",
      "Arousal RMSE: 0.3436595451835827\n",
      "Test R^2 score: tensor([0.0113, 0.1290], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3041, -0.1488], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.07013175986842946\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32320459084160474\n",
      "Valence RMSE: 0.30349530768376043\n",
      "Arousal RMSE: 0.3417791879212627\n",
      "Test R^2 score: tensor([0.0157, 0.1385], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2982, -0.1363], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.07711515540538039\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3218422809064984\n",
      "Valence RMSE: 0.3027152018182801\n",
      "Arousal RMSE: 0.339894710382955\n",
      "Test R^2 score: tensor([0.0208, 0.1480], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2916, -0.1238], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08437883125101231\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32046238469833777\n",
      "Valence RMSE: 0.301875801934259\n",
      "Arousal RMSE: 0.3380285198317427\n",
      "Test R^2 score: tensor([0.0262, 0.1573], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2844, -0.1115], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09175551796916792\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3190697237523714\n",
      "Valence RMSE: 0.30097227898576273\n",
      "Arousal RMSE: 0.33619438501103105\n",
      "Test R^2 score: tensor([0.0320, 0.1664], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2767, -0.0994], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09922576846565478\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3176698797562265\n",
      "Valence RMSE: 0.3000173369644416\n",
      "Arousal RMSE: 0.3343918398068882\n",
      "Test R^2 score: tensor([0.0382, 0.1753], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2686, -0.0877], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.10674944788767848\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3162636201242379\n",
      "Valence RMSE: 0.29899228064156086\n",
      "Arousal RMSE: 0.33263940077046594\n",
      "Test R^2 score: tensor([0.0447, 0.1840], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2600, -0.0763], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.11434055193215537\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3148610977070609\n",
      "Valence RMSE: 0.2979018967041168\n",
      "Arousal RMSE: 0.33095238575765007\n",
      "Test R^2 score: tensor([0.0517, 0.1922], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2508, -0.0654], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1219460952738805\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3134829315055465\n",
      "Valence RMSE: 0.29678478446902096\n",
      "Arousal RMSE: 0.32933552556367063\n",
      "Test R^2 score: tensor([0.0588, 0.2001], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2415, -0.0550], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.12943233884030675\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31214213146085\n",
      "Valence RMSE: 0.29566804943979685\n",
      "Arousal RMSE: 0.3277892997129574\n",
      "Test R^2 score: tensor([0.0658, 0.2076], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2321, -0.0452], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13671403991993142\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31082641740353006\n",
      "Valence RMSE: 0.2945367053046927\n",
      "Arousal RMSE: 0.32630392694554505\n",
      "Test R^2 score: tensor([0.0730, 0.2148], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2227, -0.0357], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14386433644556462\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3095313047581721\n",
      "Valence RMSE: 0.29336748140367985\n",
      "Arousal RMSE: 0.32489194835430185\n",
      "Test R^2 score: tensor([0.0803, 0.2215], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2130, -0.0268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15092760420901363\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30828782988104053\n",
      "Valence RMSE: 0.29222609135369354\n",
      "Arousal RMSE: 0.3235532160830202\n",
      "Test R^2 score: tensor([0.0875, 0.2279], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2036, -0.0183], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15769988685459224\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30704184829668985\n",
      "Valence RMSE: 0.29107189599926336\n",
      "Arousal RMSE: 0.32222126647738353\n",
      "Test R^2 score: tensor([0.0947, 0.2343], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1941, -0.0100], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1644687430661425\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30580829867781295\n",
      "Valence RMSE: 0.2899013181049555\n",
      "Arousal RMSE: 0.32092780627649936\n",
      "Test R^2 score: tensor([0.1019, 0.2404], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1845, -0.0019], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1711699400845405\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3045889500296313\n",
      "Valence RMSE: 0.2887522248868486\n",
      "Arousal RMSE: 0.3196420022199856\n",
      "Test R^2 score: tensor([0.1090, 0.2465], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1752,  0.0062], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17775981744160552\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30342207832758955\n",
      "Valence RMSE: 0.2876484127696085\n",
      "Arousal RMSE: 0.31841530406749097\n",
      "Test R^2 score: tensor([0.1158, 0.2523], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1662,  0.0138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.184045432190592\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30230327343470764\n",
      "Valence RMSE: 0.2865424853351615\n",
      "Arousal RMSE: 0.317282117927607\n",
      "Test R^2 score: tensor([0.1226, 0.2576], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1572,  0.0208], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19009462598617483\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30119171444340903\n",
      "Valence RMSE: 0.2853457663786933\n",
      "Arousal RMSE: 0.3162446700080119\n",
      "Test R^2 score: tensor([0.1299, 0.2624], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1476,  0.0272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1961749094118001\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30006732464317737\n",
      "Valence RMSE: 0.28402510543172177\n",
      "Arousal RMSE: 0.31529436741167344\n",
      "Test R^2 score: tensor([0.1380, 0.2669], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1370,  0.0330], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20240559836305783\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2990121153861007\n",
      "Valence RMSE: 0.2827641757209868\n",
      "Arousal RMSE: 0.31442155019046086\n",
      "Test R^2 score: tensor([0.1456, 0.2709], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1269,  0.0383], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20825086932403636\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29797446245008535\n",
      "Valence RMSE: 0.2815780775404516\n",
      "Arousal RMSE: 0.31351450810681486\n",
      "Test R^2 score: tensor([0.1527, 0.2751], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1175,  0.0439], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2139275361015872\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29694977626968255\n",
      "Valence RMSE: 0.2805144367784679\n",
      "Arousal RMSE: 0.3125219832461198\n",
      "Test R^2 score: tensor([0.1591, 0.2797], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1091,  0.0499], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2194131582687091\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2959934687415044\n",
      "Valence RMSE: 0.2795773411960283\n",
      "Arousal RMSE: 0.31154578694794677\n",
      "Test R^2 score: tensor([0.1647, 0.2842], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1017,  0.0559], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22446393459427322\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2951140752946225\n",
      "Valence RMSE: 0.2787153722618278\n",
      "Arousal RMSE: 0.31064831584760116\n",
      "Test R^2 score: tensor([0.1699, 0.2883], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0949,  0.0613], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22909423807977403\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2942962433392941\n",
      "Valence RMSE: 0.27786691874831043\n",
      "Arousal RMSE: 0.30985566503222406\n",
      "Test R^2 score: tensor([0.1749, 0.2919], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0882,  0.0661], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23343103978229196\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2935815904495503\n",
      "Valence RMSE: 0.2770044382367608\n",
      "Arousal RMSE: 0.30927146926108706\n",
      "Test R^2 score: tensor([0.1800, 0.2946], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0815,  0.0696], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23732173967075243\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929228448727869\n",
      "Valence RMSE: 0.2761953494972992\n",
      "Arousal RMSE: 0.30874538865025714\n",
      "Test R^2 score: tensor([0.1848, 0.2970], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0752,  0.0728], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2409120926362569\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2923762893943542\n",
      "Valence RMSE: 0.27556969781694374\n",
      "Arousal RMSE: 0.3082679529972271\n",
      "Test R^2 score: tensor([0.1885, 0.2992], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0703,  0.0756], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24384283253646055\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2918866666722439\n",
      "Valence RMSE: 0.27512134742670796\n",
      "Arousal RMSE: 0.3077399820500741\n",
      "Test R^2 score: tensor([0.1912, 0.3016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0668,  0.0788], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24636132418771883\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.291465004748596\n",
      "Valence RMSE: 0.27467889726972694\n",
      "Arousal RMSE: 0.3073356493817223\n",
      "Test R^2 score: tensor([0.1938, 0.3034], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0634,  0.0812], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2485781130850076\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2910398992685829\n",
      "Valence RMSE: 0.27401719291906396\n",
      "Arousal RMSE: 0.3071205364629501\n",
      "Test R^2 score: tensor([0.1976, 0.3044], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0583,  0.0825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2510054172822282\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2906816906484499\n",
      "Valence RMSE: 0.2734172485244689\n",
      "Arousal RMSE: 0.30697670720396864\n",
      "Test R^2 score: tensor([0.2011, 0.3050], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0537,  0.0833], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25308591261441976\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29036645900382235\n",
      "Valence RMSE: 0.273087534715602\n",
      "Arousal RMSE: 0.30667337577916953\n",
      "Test R^2 score: tensor([0.2031, 0.3064], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0511,  0.0852], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25473505126140444\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29017366051475363\n",
      "Valence RMSE: 0.2728517224130523\n",
      "Arousal RMSE: 0.30651826061312176\n",
      "Test R^2 score: tensor([0.2045, 0.3071], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0493,  0.0861], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25577363878758563\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2899133424450111\n",
      "Valence RMSE: 0.2723218658280332\n",
      "Arousal RMSE: 0.30649680854327865\n",
      "Test R^2 score: tensor([0.2075, 0.3072], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0452,  0.0862], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25736552614955505\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.289786277588123\n",
      "Valence RMSE: 0.2720196144975567\n",
      "Arousal RMSE: 0.30652488102169123\n",
      "Test R^2 score: tensor([0.2093, 0.3071], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0429,  0.0860], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2581811386684918\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2897934492076325\n",
      "Valence RMSE: 0.27202856462350317\n",
      "Arousal RMSE: 0.3065304983785299\n",
      "Test R^2 score: tensor([0.2092, 0.3070], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0430,  0.0860], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2581424233070001\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28968243917069036\n",
      "Valence RMSE: 0.2716841773575972\n",
      "Arousal RMSE: 0.30662605711401486\n",
      "Test R^2 score: tensor([0.2112, 0.3066], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0403,  0.0854], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2589268269107706\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28942574018841283\n",
      "Valence RMSE: 0.2710221706133357\n",
      "Arousal RMSE: 0.30672707934458154\n",
      "Test R^2 score: tensor([0.2151, 0.3062], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0353,  0.0848], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2606179449621944\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28936739148047635\n",
      "Valence RMSE: 0.2708412078660862\n",
      "Arousal RMSE: 0.30677681565931614\n",
      "Test R^2 score: tensor([0.2161, 0.3059], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0339,  0.0845], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2610293449973396\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2892892456800021\n",
      "Valence RMSE: 0.27060355359010835\n",
      "Arousal RMSE: 0.306839130680255\n",
      "Test R^2 score: tensor([0.2175, 0.3056], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0321,  0.0842], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2615758626694881\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2891738333884623\n",
      "Valence RMSE: 0.27029471992419524\n",
      "Arousal RMSE: 0.3068937539511959\n",
      "Test R^2 score: tensor([0.2193, 0.3054], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0297,  0.0838], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2623447762509698\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2891979015091391\n",
      "Valence RMSE: 0.27025699700029604\n",
      "Arousal RMSE: 0.3069723245619406\n",
      "Test R^2 score: tensor([0.2195, 0.3050], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0294,  0.0834], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26227587221359544\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28898458300818586\n",
      "Valence RMSE: 0.26971112218304605\n",
      "Arousal RMSE: 0.30705062938152755\n",
      "Test R^2 score: tensor([0.2227, 0.3047], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0253,  0.0829], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2636734465602951\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2889547755093251\n",
      "Valence RMSE: 0.26954151137629334\n",
      "Arousal RMSE: 0.3071434489359531\n",
      "Test R^2 score: tensor([0.2236, 0.3043], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0240,  0.0824], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.263951912174217\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28885425300516776\n",
      "Valence RMSE: 0.2692493680357019\n",
      "Arousal RMSE: 0.30721057398911594\n",
      "Test R^2 score: tensor([0.2253, 0.3040], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0218,  0.0820], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2646408541748762\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2885091658771479\n",
      "Valence RMSE: 0.26852889034299826\n",
      "Arousal RMSE: 0.3071926311639218\n",
      "Test R^2 score: tensor([0.2295, 0.3040], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0163,  0.0821], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2667516840999153\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28881491946467736\n",
      "Valence RMSE: 0.26908796199149365\n",
      "Arousal RMSE: 0.30727802414432537\n",
      "Test R^2 score: tensor([0.2262, 0.3037], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0206,  0.0815], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2649522747922399\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.288483383302199\n",
      "Valence RMSE: 0.2684672359157372\n",
      "Arousal RMSE: 0.30719809264175274\n",
      "Test R^2 score: tensor([0.2298, 0.3040], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0159,  0.0820], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2669162074260073\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882625191236871\n",
      "Valence RMSE: 0.26803329068484427\n",
      "Arousal RMSE: 0.3071623592625273\n",
      "Test R^2 score: tensor([0.2323, 0.3042], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0126,  0.0822], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.268241069778684\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28848171781145204\n",
      "Valence RMSE: 0.2683906403358392\n",
      "Arousal RMSE: 0.3072618870003417\n",
      "Test R^2 score: tensor([0.2303, 0.3037], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0153,  0.0816], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26699137024031677\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28806324832395136\n",
      "Valence RMSE: 0.26752717049378\n",
      "Arousal RMSE: 0.3072296911391946\n",
      "Test R^2 score: tensor([0.2352, 0.3039], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0087,  0.0818], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2695367844557956\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879491673764928\n",
      "Valence RMSE: 0.2671932447204277\n",
      "Arousal RMSE: 0.30730638776537095\n",
      "Test R^2 score: tensor([0.2371, 0.3035], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0062,  0.0814], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2703170102645289\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2880842955144799\n",
      "Valence RMSE: 0.2673424739261453\n",
      "Arousal RMSE: 0.3074298688793863\n",
      "Test R^2 score: tensor([0.2363, 0.3030], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0074,  0.0806], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26961089876178995\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28796574769223743\n",
      "Valence RMSE: 0.26700827584181736\n",
      "Arousal RMSE: 0.3074981696203713\n",
      "Test R^2 score: tensor([0.2382, 0.3027], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0048,  0.0802], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2704101695898884\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882790914756246\n",
      "Valence RMSE: 0.2674157481138524\n",
      "Arousal RMSE: 0.30773119247929953\n",
      "Test R^2 score: tensor([0.2358, 0.3016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0079,  0.0788], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2687180187552694\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2877289050531618\n",
      "Valence RMSE: 0.2662344348977469\n",
      "Arousal RMSE: 0.30772564287179605\n",
      "Test R^2 score: tensor([0.2426, 0.3016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0010, 0.0789], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27209887481409534\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28828849924592215\n",
      "Valence RMSE: 0.26709833316531034\n",
      "Arousal RMSE: 0.3080243464650886\n",
      "Test R^2 score: tensor([0.2376, 0.3003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0055,  0.0771], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2689588932353259\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2874519221075196\n",
      "Valence RMSE: 0.2656431961755618\n",
      "Arousal RMSE: 0.30771887717890656\n",
      "Test R^2 score: tensor([0.2459, 0.3017], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0054, 0.0789], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27379442011388916\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882433872503134\n",
      "Valence RMSE: 0.2671342709036991\n",
      "Arousal RMSE: 0.3079087233188066\n",
      "Test R^2 score: tensor([0.2374, 0.3008], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0058,  0.0778], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2691189206225586\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28794264082085785\n",
      "Valence RMSE: 0.2666075550801094\n",
      "Arousal RMSE: 0.3078024372547845\n",
      "Test R^2 score: tensor([0.2404, 0.3013], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0018,  0.0784], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27086230999094413\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28762851108571863\n",
      "Valence RMSE: 0.26595846785252275\n",
      "Arousal RMSE: 0.3077765653137245\n",
      "Test R^2 score: tensor([0.2441, 0.3014], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0030, 0.0786], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27276801071350226\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2883333533038241\n",
      "Valence RMSE: 0.267116082172252\n",
      "Arousal RMSE: 0.3080929143940224\n",
      "Test R^2 score: tensor([0.2375, 0.3000], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0057,  0.0767], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26875245141040904\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2877481710964235\n",
      "Valence RMSE: 0.26600153099308516\n",
      "Arousal RMSE: 0.3079629936339884\n",
      "Test R^2 score: tensor([0.2439, 0.3006], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0027, 0.0774], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2722223237637943\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2876716867302983\n",
      "Valence RMSE: 0.26588809623449866\n",
      "Arousal RMSE: 0.3079180393762336\n",
      "Test R^2 score: tensor([0.2445, 0.3008], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0036, 0.0777], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27264678492014904\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882908373464193\n",
      "Valence RMSE: 0.267087521404394\n",
      "Arousal RMSE: 0.3080380978156055\n",
      "Test R^2 score: tensor([0.2377, 0.3002], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0054,  0.0770], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26895851241242985\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28738913710351366\n",
      "Valence RMSE: 0.2655273074371726\n",
      "Arousal RMSE: 0.30770161074549085\n",
      "Test R^2 score: tensor([0.2466, 0.3017], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0063, 0.0790], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2741625001897337\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28833551529368023\n",
      "Valence RMSE: 0.2672569339359563\n",
      "Arousal RMSE: 0.3079747879656647\n",
      "Test R^2 score: tensor([0.2367, 0.3005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0067,  0.0774], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2686186510043719\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28760161143486346\n",
      "Valence RMSE: 0.2659180850960976\n",
      "Arousal RMSE: 0.30776118309280803\n",
      "Test R^2 score: tensor([0.2444, 0.3015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0033, 0.0787], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27291768509741304\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28763455048495273\n",
      "Valence RMSE: 0.2659876989151405\n",
      "Arousal RMSE: 0.30776259241693127\n",
      "Test R^2 score: tensor([0.2440, 0.3015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0028, 0.0786], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2727166462552721\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2883237014003148\n",
      "Valence RMSE: 0.2672848027533163\n",
      "Arousal RMSE: 0.30792847837684095\n",
      "Test R^2 score: tensor([0.2366, 0.3007], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0069,  0.0777], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2686442314184886\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2872241661552604\n",
      "Valence RMSE: 0.26544312825273764\n",
      "Arousal RMSE: 0.3074660776582841\n",
      "Test R^2 score: tensor([0.2471, 0.3028], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0069, 0.0804], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27493560135762096\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879213151762973\n",
      "Valence RMSE: 0.2668557683109012\n",
      "Arousal RMSE: 0.30754734007124185\n",
      "Test R^2 score: tensor([0.2390, 0.3024], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0037,  0.0799], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27073366938137494\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2873567913112259\n",
      "Valence RMSE: 0.2659560585160426\n",
      "Arousal RMSE: 0.3072706070615577\n",
      "Test R^2 score: tensor([0.2442, 0.3037], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0031, 0.0816], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2739223581332\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28677080907696983\n",
      "Valence RMSE: 0.2650444169216497\n",
      "Arousal RMSE: 0.30696327294316905\n",
      "Test R^2 score: tensor([0.2493, 0.3051], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0099, 0.0834], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2772049062459296\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2872441236879416\n",
      "Valence RMSE: 0.26609551075806853\n",
      "Arousal RMSE: 0.3069390042678017\n",
      "Test R^2 score: tensor([0.2434, 0.3052], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0020, 0.0836], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27427697337752105\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2864720839367878\n",
      "Valence RMSE: 0.26493760368737\n",
      "Arousal RMSE: 0.30649726899691293\n",
      "Test R^2 score: tensor([0.2499, 0.3072], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0107, 0.0862], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27856152615570806\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28660606412940837\n",
      "Valence RMSE: 0.26534758335842523\n",
      "Arousal RMSE: 0.3063931004401775\n",
      "Test R^2 score: tensor([0.2476, 0.3077], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0076, 0.0868], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.277635351204554\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2863033303213568\n",
      "Valence RMSE: 0.26489293658120444\n",
      "Arousal RMSE: 0.30622038804688684\n",
      "Test R^2 score: tensor([0.2502, 0.3084], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0110, 0.0879], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2793135535709678\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28610852019391947\n",
      "Valence RMSE: 0.2645743461914968\n",
      "Arousal RMSE: 0.3061316481392456\n",
      "Test R^2 score: tensor([0.2520, 0.3088], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0134, 0.0884], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2804152033790392\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28590689867135144\n",
      "Valence RMSE: 0.2642907804604941\n",
      "Arousal RMSE: 0.3059998248027665\n",
      "Test R^2 score: tensor([0.2536, 0.3094], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0155, 0.0892], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2815140350349548\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28586480666105324\n",
      "Valence RMSE: 0.2643385836527484\n",
      "Arousal RMSE: 0.3058798596952432\n",
      "Test R^2 score: tensor([0.2533, 0.3100], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0152, 0.0899], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2816496933816798\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2857859483090722\n",
      "Valence RMSE: 0.26429313827103396\n",
      "Arousal RMSE: 0.30577173441094824\n",
      "Test R^2 score: tensor([0.2536, 0.3105], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0155, 0.0905], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2820219240503472\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28579564035946464\n",
      "Valence RMSE: 0.2643242616034539\n",
      "Arousal RMSE: 0.30576294874418936\n",
      "Test R^2 score: tensor([0.2534, 0.3105], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0153, 0.0906], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28195383131935176\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28569754426429095\n",
      "Valence RMSE: 0.26410605787541264\n",
      "Arousal RMSE: 0.305768153656986\n",
      "Test R^2 score: tensor([0.2546, 0.3105], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0169, 0.0906], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2825581697081708\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28583488484067315\n",
      "Valence RMSE: 0.26433425590241233\n",
      "Arousal RMSE: 0.30582767033131547\n",
      "Test R^2 score: tensor([0.2533, 0.3102], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0152, 0.0902], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2817796393469013\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28563537863889155\n",
      "Valence RMSE: 0.2640021466924524\n",
      "Arousal RMSE: 0.305741730226907\n",
      "Test R^2 score: tensor([0.2552, 0.3106], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0177, 0.0907], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28291095626721036\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28622353607979156\n",
      "Valence RMSE: 0.2650699890844947\n",
      "Arousal RMSE: 0.3059178420733014\n",
      "Test R^2 score: tensor([0.2492, 0.3098], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0097, 0.0897], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27949513546097565\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28526476382485605\n",
      "Valence RMSE: 0.26320876311062785\n",
      "Arousal RMSE: 0.30573373706850737\n",
      "Test R^2 score: tensor([0.2597, 0.3106], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0236, 0.0908], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28516384609347406\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2874096978466854\n",
      "Valence RMSE: 0.266928905289133\n",
      "Arousal RMSE: 0.30652508601057604\n",
      "Test R^2 score: tensor([0.2386, 0.3071], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0042,  0.0860], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2728398534908987\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2848615667519758\n",
      "Valence RMSE: 0.2623395437817343\n",
      "Arousal RMSE: 0.30572894562532393\n",
      "Test R^2 score: tensor([0.2646, 0.3107], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0300, 0.0908], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2876154151963989\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28662020363288876\n",
      "Valence RMSE: 0.26556459391609466\n",
      "Arousal RMSE: 0.306231495309181\n",
      "Test R^2 score: tensor([0.2464, 0.3084], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0060, 0.0878], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27738483936160657\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28668990280247114\n",
      "Valence RMSE: 0.26576655807603317\n",
      "Arousal RMSE: 0.3061867687314382\n",
      "Test R^2 score: tensor([0.2452, 0.3086], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0045, 0.0881], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2769124894412829\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2850349377472079\n",
      "Valence RMSE: 0.26280127323975366\n",
      "Arousal RMSE: 0.3056555614685813\n",
      "Test R^2 score: tensor([0.2620, 0.3110], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0266, 0.0912], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28648532760537365\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2860782357193477\n",
      "Valence RMSE: 0.2646717883767682\n",
      "Arousal RMSE: 0.30599078146577\n",
      "Test R^2 score: tensor([0.2514, 0.3095], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0127, 0.0892], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2804576234600042\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28711395424595293\n",
      "Valence RMSE: 0.2663412500793238\n",
      "Arousal RMSE: 0.30648194718723226\n",
      "Test R^2 score: tensor([0.2420, 0.3073], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0002, 0.0863], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2746117462680894\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2855981597691848\n",
      "Valence RMSE: 0.26359138827389633\n",
      "Arousal RMSE: 0.3060264657753144\n",
      "Test R^2 score: tensor([0.2575, 0.3093], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0207, 0.0890], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28342652345044794\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2861094069996141\n",
      "Valence RMSE: 0.26435637367627235\n",
      "Arousal RMSE: 0.3063215520398014\n",
      "Test R^2 score: tensor([0.2532, 0.3080], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0150, 0.0873], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2806023282654174\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28796367748827734\n",
      "Valence RMSE: 0.26741862572169217\n",
      "Arousal RMSE: 0.3071374899328747\n",
      "Test R^2 score: tensor([0.2358, 0.3043], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0079,  0.0824], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27005590672115354\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28712031428746426\n",
      "Valence RMSE: 0.26588940218030455\n",
      "Arousal RMSE: 0.3068859324916422\n",
      "Test R^2 score: tensor([0.2445, 0.3054], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0036, 0.0839], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2749829401389368\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2870239537912849\n",
      "Valence RMSE: 0.26555605891819994\n",
      "Arousal RMSE: 0.3069942665129054\n",
      "Test R^2 score: tensor([0.2464, 0.3049], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0061, 0.0832], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27568423771180345\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28855272003363397\n",
      "Valence RMSE: 0.2680787107741652\n",
      "Arousal RMSE: 0.3076672704518292\n",
      "Test R^2 score: tensor([0.2320, 0.3019], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0129,  0.0792], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2669662463888712\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28860946113827496\n",
      "Valence RMSE: 0.2679571593304896\n",
      "Arousal RMSE: 0.30787952656937995\n",
      "Test R^2 score: tensor([0.2327, 0.3009], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0120,  0.0779], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26683259128019254\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2876940102886438\n",
      "Valence RMSE: 0.26613070649791937\n",
      "Arousal RMSE: 0.3077501164432645\n",
      "Test R^2 score: tensor([0.2432, 0.3015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0018, 0.0787], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27233837781045006\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28894656296887283\n",
      "Valence RMSE: 0.2679979675031424\n",
      "Arousal RMSE: 0.30847580442756645\n",
      "Test R^2 score: tensor([0.2325, 0.2982], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0123,  0.0744], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2653605140767192\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2901874227434622\n",
      "Valence RMSE: 0.2698481478012816\n",
      "Arousal RMSE: 0.3091916198172315\n",
      "Test R^2 score: tensor([0.2219, 0.2950], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0263,  0.0701], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2584132760353546\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2894094198150833\n",
      "Valence RMSE: 0.26837591228773217\n",
      "Arousal RMSE: 0.3090145534746442\n",
      "Test R^2 score: tensor([0.2303, 0.2958], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0152,  0.0711], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2630506630477553\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29016438682732876\n",
      "Valence RMSE: 0.26952779976429775\n",
      "Arousal RMSE: 0.3094277103298539\n",
      "Test R^2 score: tensor([0.2237, 0.2939], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0239,  0.0687], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25879792313085326\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2914773191699309\n",
      "Valence RMSE: 0.27163869544461\n",
      "Arousal RMSE: 0.31004914822997004\n",
      "Test R^2 score: tensor([0.2115, 0.2910], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0400,  0.0649], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2512748284678444\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2909681002416308\n",
      "Valence RMSE: 0.2705245867682062\n",
      "Arousal RMSE: 0.31006663585484107\n",
      "Test R^2 score: tensor([0.2180, 0.2910], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0315,  0.0648], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25446215776684566\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29079568303774345\n",
      "Valence RMSE: 0.2699401533617037\n",
      "Arousal RMSE: 0.3102524329474877\n",
      "Test R^2 score: tensor([0.2213, 0.2901], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0270,  0.0637], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2557248287331118\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29178087034368894\n",
      "Valence RMSE: 0.27132689247054687\n",
      "Arousal RMSE: 0.310892055252812\n",
      "Test R^2 score: tensor([0.2133, 0.2872], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0376,  0.0598], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25024937598748026\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29214314470058333\n",
      "Valence RMSE: 0.2717507056193824\n",
      "Arousal RMSE: 0.3112021657804773\n",
      "Test R^2 score: tensor([0.2109, 0.2858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0409,  0.0579], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24830823494207865\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2920026166188645\n",
      "Valence RMSE: 0.27149974968933593\n",
      "Arousal RMSE: 0.31115742341000807\n",
      "Test R^2 score: tensor([0.2123, 0.2860], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0389,  0.0582], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24913933549922412\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29266253699773204\n",
      "Valence RMSE: 0.27260788191084806\n",
      "Arousal RMSE: 0.31142842491329986\n",
      "Test R^2 score: tensor([0.2059, 0.2847], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0474,  0.0566], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24529566051722418\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929596065975097\n",
      "Valence RMSE: 0.273096306275568\n",
      "Arousal RMSE: 0.3115590950272138\n",
      "Test R^2 score: tensor([0.2030, 0.2841], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0512,  0.0558], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24357138411291823\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29230021134925765\n",
      "Valence RMSE: 0.27191283467016863\n",
      "Arousal RMSE: 0.3113554840552434\n",
      "Test R^2 score: tensor([0.2099, 0.2851], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0421,  0.0570], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24748531668001938\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29232013186064687\n",
      "Valence RMSE: 0.27192798574796256\n",
      "Arousal RMSE: 0.31137965500190895\n",
      "Test R^2 score: tensor([0.2098, 0.2849], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0422,  0.0569], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24738578737719907\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29259305164045507\n",
      "Valence RMSE: 0.27240777986705167\n",
      "Arousal RMSE: 0.3114729349469259\n",
      "Test R^2 score: tensor([0.2070, 0.2845], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0459,  0.0563], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24577612320415915\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2922100220089234\n",
      "Valence RMSE: 0.271855489211026\n",
      "Arousal RMSE: 0.3112362236481235\n",
      "Test R^2 score: tensor([0.2102, 0.2856], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0417,  0.0577], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24792572245778016\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2924213510007262\n",
      "Valence RMSE: 0.27235034458938145\n",
      "Arousal RMSE: 0.31120055084177634\n",
      "Test R^2 score: tensor([0.2074, 0.2858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0455,  0.0580], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24656871630501603\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929472847571226\n",
      "Valence RMSE: 0.2733274295184263\n",
      "Arousal RMSE: 0.3113331648989408\n",
      "Test R^2 score: tensor([0.2017, 0.2852], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0530,  0.0571], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24341555129010567\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2925795214459847\n",
      "Valence RMSE: 0.27258999420464214\n",
      "Arousal RMSE: 0.31128804634716706\n",
      "Test R^2 score: tensor([0.2060, 0.2854], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0473,  0.0574], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2456701120007349\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2927948851891282\n",
      "Valence RMSE: 0.2728114328718597\n",
      "Arousal RMSE: 0.3114989754079976\n",
      "Test R^2 score: tensor([0.2047, 0.2844], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0490,  0.0561], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24454042210574117\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2931738380078511\n",
      "Valence RMSE: 0.27326668941999793\n",
      "Arousal RMSE: 0.3118126280924841\n",
      "Test R^2 score: tensor([0.2020, 0.2830], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0525,  0.0542], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2424912109908292\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29294827531144446\n",
      "Valence RMSE: 0.2725661665686345\n",
      "Arousal RMSE: 0.312001712908726\n",
      "Test R^2 score: tensor([0.2061, 0.2821], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0471,  0.0531], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24409924772535918\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2933609465426808\n",
      "Valence RMSE: 0.27305803062685247\n",
      "Arousal RMSE: 0.3123469254259119\n",
      "Test R^2 score: tensor([0.2032, 0.2805], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0509,  0.0510], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2418705614831373\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29387038414525646\n",
      "Valence RMSE: 0.2737533725931609\n",
      "Arousal RMSE: 0.31269585278546824\n",
      "Test R^2 score: tensor([0.1992, 0.2789], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0562,  0.0489], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2390348273009556\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2939504434155804\n",
      "Valence RMSE: 0.2736295449710385\n",
      "Arousal RMSE: 0.312954626882836\n",
      "Test R^2 score: tensor([0.1999, 0.2777], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0553,  0.0473], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23879997011269072\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.294357873848638\n",
      "Valence RMSE: 0.27414641869969736\n",
      "Arousal RMSE: 0.31326802726660696\n",
      "Test R^2 score: tensor([0.1969, 0.2762], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0593,  0.0454], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23656350617848143\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.294656218074882\n",
      "Valence RMSE: 0.2744598138831839\n",
      "Arousal RMSE: 0.3135544358856893\n",
      "Test R^2 score: tensor([0.1950, 0.2749], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0617,  0.0436], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2349828801879058\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2946797056933362\n",
      "Valence RMSE: 0.2742344242292588\n",
      "Arousal RMSE: 0.31379569541768626\n",
      "Test R^2 score: tensor([0.1964, 0.2738], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0600,  0.0422], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23508553181841518\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2953879679811758\n",
      "Valence RMSE: 0.2752831383375385\n",
      "Arousal RMSE: 0.3142090021039052\n",
      "Test R^2 score: tensor([0.1902, 0.2719], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0681,  0.0396], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23104932251831722\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2954464292893343\n",
      "Valence RMSE: 0.2751750933024728\n",
      "Arousal RMSE: 0.3144135066844466\n",
      "Test R^2 score: tensor([0.1908, 0.2709], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0672,  0.0384], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2308930444913319\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2956898263271874\n",
      "Valence RMSE: 0.2754705895208991\n",
      "Arousal RMSE: 0.31461230283606634\n",
      "Test R^2 score: tensor([0.1891, 0.2700], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0695,  0.0372], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22956255575304352\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2958861156959847\n",
      "Valence RMSE: 0.2757875629617923\n",
      "Arousal RMSE: 0.31470368132405246\n",
      "Test R^2 score: tensor([0.1872, 0.2696], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0720,  0.0366], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2284169013891702\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29561492953160445\n",
      "Valence RMSE: 0.2752360718848242\n",
      "Arousal RMSE: 0.3146767831559279\n",
      "Test R^2 score: tensor([0.1905, 0.2697], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0677,  0.0368], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2301029787133756\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29611965856243805\n",
      "Valence RMSE: 0.27610046969035285\n",
      "Arousal RMSE: 0.3148685995951904\n",
      "Test R^2 score: tensor([0.1854, 0.2688], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0744,  0.0356], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2271113603781058\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29602959701727766\n",
      "Valence RMSE: 0.27575678347163857\n",
      "Arousal RMSE: 0.31500038252325896\n",
      "Test R^2 score: tensor([0.1874, 0.2682], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0718,  0.0348], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22781865892880981\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2965817326045836\n",
      "Valence RMSE: 0.2765820848868966\n",
      "Arousal RMSE: 0.3153153953569203\n",
      "Test R^2 score: tensor([0.1825, 0.2668], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0782,  0.0329], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22465090286676415\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29666078898214593\n",
      "Valence RMSE: 0.2764521885918132\n",
      "Arousal RMSE: 0.31557793785658583\n",
      "Test R^2 score: tensor([0.1833, 0.2655], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0772,  0.0313], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22442394702156454\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2971584418898111\n",
      "Valence RMSE: 0.2771077577837726\n",
      "Arousal RMSE: 0.3159391867888668\n",
      "Test R^2 score: tensor([0.1794, 0.2638], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0823,  0.0290], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2216437514799834\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29742843795439605\n",
      "Valence RMSE: 0.2772812361603109\n",
      "Arousal RMSE: 0.31629490587329084\n",
      "Test R^2 score: tensor([0.1784, 0.2622], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0836,  0.0269], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22030058667912883\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29785737265229895\n",
      "Valence RMSE: 0.27778141258057104\n",
      "Arousal RMSE: 0.3166630949627751\n",
      "Test R^2 score: tensor([0.1754, 0.2605], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0876,  0.0246], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2179578568850457\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29805645883343135\n",
      "Valence RMSE: 0.2778641752388468\n",
      "Arousal RMSE: 0.3169649908483354\n",
      "Test R^2 score: tensor([0.1750, 0.2591], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0882,  0.0227], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21700677367303273\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2984864581972415\n",
      "Valence RMSE: 0.2784163567979423\n",
      "Arousal RMSE: 0.3172895581667133\n",
      "Test R^2 score: tensor([0.1717, 0.2575], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0925,  0.0207], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21460648184648518\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29853124485162874\n",
      "Valence RMSE: 0.27818232021423545\n",
      "Arousal RMSE: 0.31757897447021083\n",
      "Test R^2 score: tensor([0.1731, 0.2562], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0907,  0.0189], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21462494074108107\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2994401237979002\n",
      "Valence RMSE: 0.27958339284347766\n",
      "Arousal RMSE: 0.318059588640766\n",
      "Test R^2 score: tensor([0.1647, 0.2539], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1017,  0.0160], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2093230423246275\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2989604539341679\n",
      "Valence RMSE: 0.2782787011537851\n",
      "Arousal RMSE: 0.31830122606928457\n",
      "Test R^2 score: tensor([0.1725, 0.2528], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0915,  0.0145], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2126448523934224\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3018235119805793\n",
      "Valence RMSE: 0.283232230126791\n",
      "Arousal RMSE: 0.319334258397038\n",
      "Test R^2 score: tensor([0.1428, 0.2479], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1307,  0.0081], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19535461089407835\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2996718557684815\n",
      "Valence RMSE: 0.27858633075189915\n",
      "Arousal RMSE: 0.3193682805127094\n",
      "Test R^2 score: tensor([0.1707, 0.2478], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0939,  0.0079], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20922049068195508\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30225940529050166\n",
      "Valence RMSE: 0.2833893640405964\n",
      "Arousal RMSE: 0.32001869401927424\n",
      "Test R^2 score: tensor([0.1418, 0.2447], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1319,  0.0038], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1932652696045668\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3018863371296633\n",
      "Valence RMSE: 0.28239113951626693\n",
      "Arousal RMSE: 0.3201967604674252\n",
      "Test R^2 score: tensor([0.1478, 0.2439], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1240,  0.0027], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19586248388902133\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30088845061472635\n",
      "Valence RMSE: 0.27985335038616393\n",
      "Arousal RMSE: 0.3205461303842244\n",
      "Test R^2 score: tensor([0.1631, 0.2422], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1038,  0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20266070076836024\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30273656420596384\n",
      "Valence RMSE: 0.2831022334557792\n",
      "Arousal RMSE: 0.32117281956419164\n",
      "Test R^2 score: tensor([0.1436, 0.2393], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1296, -0.0034], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1914055521000863\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3035185348388068\n",
      "Valence RMSE: 0.28410071621499433\n",
      "Arousal RMSE: 0.321766662392982\n",
      "Test R^2 score: tensor([0.1375, 0.2364], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1376, -0.0071], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18697169457548263\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3026113805706449\n",
      "Valence RMSE: 0.2816979691914178\n",
      "Arousal RMSE: 0.3221700629406376\n",
      "Test R^2 score: tensor([0.1520, 0.2345], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1184, -0.0096], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19327745394163692\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3036896427791308\n",
      "Valence RMSE: 0.2834382866595848\n",
      "Arousal RMSE: 0.3226724901786814\n",
      "Test R^2 score: tensor([0.1415, 0.2321], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1323, -0.0128], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18682784546187037\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3050640275091231\n",
      "Valence RMSE: 0.28554239129159886\n",
      "Arousal RMSE: 0.3234094379198751\n",
      "Test R^2 score: tensor([0.1287, 0.2286], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1492, -0.0174], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1786755352734597\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3037868108102016\n",
      "Valence RMSE: 0.2827242015959313\n",
      "Arousal RMSE: 0.3234808783783202\n",
      "Test R^2 score: tensor([0.1458, 0.2283], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1266, -0.0179], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1870618234303501\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3043562769159149\n",
      "Valence RMSE: 0.2834453694223164\n",
      "Arousal RMSE: 0.32392006598739864\n",
      "Test R^2 score: tensor([0.1415, 0.2262], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1324, -0.0206], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18383180188226927\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3060596695888041\n",
      "Valence RMSE: 0.2863509711661301\n",
      "Arousal RMSE: 0.32457381904557553\n",
      "Test R^2 score: tensor([0.1238, 0.2231], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1557, -0.0248], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1734226224951545\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3054252006099941\n",
      "Valence RMSE: 0.28485623430214563\n",
      "Arousal RMSE: 0.32469375127111916\n",
      "Test R^2 score: tensor([0.1329, 0.2225], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1437, -0.0255], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17769735093513506\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3052643854576452\n",
      "Valence RMSE: 0.28415365510723484\n",
      "Arousal RMSE: 0.3250067543096085\n",
      "Test R^2 score: tensor([0.1372, 0.2210], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1380, -0.0275], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17908346132238917\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3063054071482679\n",
      "Valence RMSE: 0.28598864034432525\n",
      "Arousal RMSE: 0.32535596274009876\n",
      "Test R^2 score: tensor([0.1260, 0.2193], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1528, -0.0297], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17265614341573576\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3065295240911935\n",
      "Valence RMSE: 0.28623376535819417\n",
      "Arousal RMSE: 0.3255624822488264\n",
      "Test R^2 score: tensor([0.1245, 0.2183], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1548, -0.0310], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1714110058659919\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.306372500991966\n",
      "Valence RMSE: 0.28556017026095837\n",
      "Arousal RMSE: 0.32585826349607416\n",
      "Test R^2 score: tensor([0.1286, 0.2169], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1493, -0.0329], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17275840124761138\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3070644624850925\n",
      "Valence RMSE: 0.28676206899369494\n",
      "Arousal RMSE: 0.32610532658785707\n",
      "Test R^2 score: tensor([0.1213, 0.2157], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1590, -0.0344], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16848913835799778\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3073448006864258\n",
      "Valence RMSE: 0.28706185075096735\n",
      "Arousal RMSE: 0.3263696475798369\n",
      "Test R^2 score: tensor([0.1194, 0.2144], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1614, -0.0361], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1669340743463455\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30671149279090754\n",
      "Valence RMSE: 0.2855239984432391\n",
      "Arousal RMSE: 0.3265270676881765\n",
      "Test R^2 score: tensor([0.1288, 0.2137], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1490, -0.0371], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17125985831642726\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30753829004010147\n",
      "Valence RMSE: 0.28667323342230516\n",
      "Arousal RMSE: 0.3270750019808602\n",
      "Test R^2 score: tensor([0.1218, 0.2110], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1583, -0.0406], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.166425768329392\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3089645005902127\n",
      "Valence RMSE: 0.28890126710052527\n",
      "Arousal RMSE: 0.32780204867820767\n",
      "Test R^2 score: tensor([0.1081, 0.2075], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1764, -0.0452], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1578182206461024\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3085337898515243\n",
      "Valence RMSE: 0.2876623745667701\n",
      "Arousal RMSE: 0.3280801079292928\n",
      "Test R^2 score: tensor([0.1157, 0.2062], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1663, -0.0470], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16096221702222885\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30839294464769734\n",
      "Valence RMSE: 0.28716188306105606\n",
      "Arousal RMSE: 0.3282536664437791\n",
      "Test R^2 score: tensor([0.1188, 0.2053], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1623, -0.0481], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1620793162898792\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3092212413302072\n",
      "Valence RMSE: 0.28865915062658304\n",
      "Arousal RMSE: 0.3284987776828525\n",
      "Test R^2 score: tensor([0.1096, 0.2042], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1744, -0.0497], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15687921629399154\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30927603518577096\n",
      "Valence RMSE: 0.288665237521329\n",
      "Arousal RMSE: 0.32859658021229493\n",
      "Test R^2 score: tensor([0.1096, 0.2037], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1745, -0.0503], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1566234624041039\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30877442544103223\n",
      "Valence RMSE: 0.2876990441224216\n",
      "Arousal RMSE: 0.32850045909240977\n",
      "Test R^2 score: tensor([0.1155, 0.2041], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1666, -0.0497], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15983176568692636\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30939681839361755\n",
      "Valence RMSE: 0.28869636062409904\n",
      "Arousal RMSE: 0.32879658426842956\n",
      "Test R^2 score: tensor([0.1094, 0.2027], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1747, -0.0516], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.156042618179775\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31024760411631974\n",
      "Valence RMSE: 0.28987604831417835\n",
      "Arousal RMSE: 0.32936154653145144\n",
      "Test R^2 score: tensor([0.1021, 0.2000], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1843, -0.0552], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1510247123511983\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3102126420263119\n",
      "Valence RMSE: 0.2893983228815722\n",
      "Arousal RMSE: 0.32971560056998017\n",
      "Test R^2 score: tensor([0.1050, 0.1983], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1804, -0.0575], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1516428294606717\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3104137286788401\n",
      "Valence RMSE: 0.2894613401734126\n",
      "Arousal RMSE: 0.33003863175333437\n",
      "Test R^2 score: tensor([0.1046, 0.1967], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1809, -0.0596], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15066204710655878\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3107938061666158\n",
      "Valence RMSE: 0.28998006439576923\n",
      "Arousal RMSE: 0.33029856517413064\n",
      "Test R^2 score: tensor([0.1014, 0.1954], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1852, -0.0612], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14842317155203688\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3109913834261665\n",
      "Valence RMSE: 0.2902246155240112\n",
      "Arousal RMSE: 0.33045567580930074\n",
      "Test R^2 score: tensor([0.0999, 0.1946], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1872, -0.0622], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14728225395772082\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31138884691918345\n",
      "Valence RMSE: 0.2907932465391937\n",
      "Arousal RMSE: 0.3307042723318117\n",
      "Test R^2 score: tensor([0.0964, 0.1934], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1918, -0.0638], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1449109309300191\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31144143780327255\n",
      "Valence RMSE: 0.29086393075706374\n",
      "Arousal RMSE: 0.33074115580993146\n",
      "Test R^2 score: tensor([0.0959, 0.1933], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1924, -0.0641], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14460129823617313\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.311384296972082\n",
      "Valence RMSE: 0.2906306114399884\n",
      "Arousal RMSE: 0.3308386441992742\n",
      "Test R^2 score: tensor([0.0974, 0.1928], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1905, -0.0647], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14508837310006162\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31144889217556637\n",
      "Valence RMSE: 0.29060110167386083\n",
      "Arousal RMSE: 0.3309861395598214\n",
      "Test R^2 score: tensor([0.0976, 0.1921], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1903, -0.0656], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14482005875746684\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31176313490222174\n",
      "Valence RMSE: 0.29090813555982437\n",
      "Arousal RMSE: 0.3313079552821343\n",
      "Test R^2 score: tensor([0.0957, 0.1905], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1928, -0.0677], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1430801687355474\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3120534070455397\n",
      "Valence RMSE: 0.29120931381364856\n",
      "Arousal RMSE: 0.3315897966549069\n",
      "Test R^2 score: tensor([0.0938, 0.1891], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1952, -0.0695], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14145449122772846\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31226483120746606\n",
      "Valence RMSE: 0.2914859622985973\n",
      "Arousal RMSE: 0.3317447564030444\n",
      "Test R^2 score: tensor([0.0921, 0.1884], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1975, -0.0705], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14021415583954144\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3124345892352993\n",
      "Valence RMSE: 0.29169973439592933\n",
      "Arousal RMSE: 0.3318764981956461\n",
      "Test R^2 score: tensor([0.0907, 0.1877], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1993, -0.0714], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1392256680187431\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31211980154338453\n",
      "Valence RMSE: 0.29107086213066596\n",
      "Arousal RMSE: 0.33183624613576246\n",
      "Test R^2 score: tensor([0.0947, 0.1879], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1941, -0.0711], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14128232373912436\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3121869928434783\n",
      "Valence RMSE: 0.2910908136327024\n",
      "Arousal RMSE: 0.33194513887080684\n",
      "Test R^2 score: tensor([0.0945, 0.1874], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1943, -0.0718], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14095373036381448\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3126932297767064\n",
      "Valence RMSE: 0.29181692743369053\n",
      "Arousal RMSE: 0.3322604291207973\n",
      "Test R^2 score: tensor([0.0900, 0.1858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2002, -0.0739], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13792004929124352\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3125508317797283\n",
      "Valence RMSE: 0.29149437825037433\n",
      "Arousal RMSE: 0.3322755969685823\n",
      "Test R^2 score: tensor([0.0920, 0.1858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1976, -0.0740], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.138888145648912\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3122571956386113\n",
      "Valence RMSE: 0.29088021863493113\n",
      "Arousal RMSE: 0.3322616602364387\n",
      "Test R^2 score: tensor([0.0958, 0.1858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1925, -0.0739], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14083332814830324\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31260142196441426\n",
      "Valence RMSE: 0.29144912763382314\n",
      "Arousal RMSE: 0.3324104451274524\n",
      "Test R^2 score: tensor([0.0923, 0.1851], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1972, -0.0748], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13869857088449133\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3131192215282502\n",
      "Valence RMSE: 0.2922150911519453\n",
      "Arousal RMSE: 0.3327125400161154\n",
      "Test R^2 score: tensor([0.0875, 0.1836], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2035, -0.0768], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13556897992349576\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.313164650872715\n",
      "Valence RMSE: 0.29214478894669504\n",
      "Arousal RMSE: 0.33285975936383483\n",
      "Test R^2 score: tensor([0.0880, 0.1829], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2029, -0.0777], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13542716123742554\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Epoch 299, Loss: 0.45046328763266535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31307362849076503\n",
      "Valence RMSE: 0.2917747159990193\n",
      "Arousal RMSE: 0.333013076044228\n",
      "Test R^2 score: tensor([0.0903, 0.1821], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1999, -0.0787], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13620528954046685\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Epoch 299, Loss: 0.45046328763266535\n",
      "Epoch 300, Loss: 0.4503020948098693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31340519017676255\n",
      "Valence RMSE: 0.2921102081000748\n",
      "Arousal RMSE: 0.3333425457141593\n",
      "Test R^2 score: tensor([0.0882, 0.1805], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2027, -0.0809], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13434909463452988\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the graph to visualise the relationship between the evalutation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnwklEQVR4nO3de1yO9/8H8Nfd+aCjKIVShnIoK5o5Zcpxm8NMDpvkNKMxmW0YYVkOG7Yhv9mcGWY5bIhEzOQ8MySHIUbl3IlK9/X74/Pt5lZxR3Xd3ffr+Xjcj/u6r+u6r/t9fbrT2+eokCRJAhEREZEeMZA7ACIiIqKKxgSIiIiI9A4TICIiItI7TICIiIhI7zABIiIiIr3DBIiIiIj0DhMgIiIi0jtMgIiIiEjvMAEiIiIivcMEiIi0ipubG9588025wyAiHccEiIioHAUEBEChUDz3MWXKlDL5vIULF2LZsmUan/90HNbW1mjbti22bt1a5Nxly5apztu/f3+R45IkoVatWlAoFEWS2KysLERERKBRo0awtLRE1apV4ePjg9GjR+P69euq86ZMmfLMckpNTdW8MIiewUjuAIiIdNnEiRMxZMgQ1esjR47gu+++w4QJE+Dp6ana36RJkzL5vIULF8LBwQEDBw7U+D1BQUEYMGAAJEnClStXEB0djbfeegvbt29Hx44di5xvZmaGNWvWoFWrVmr79+7di2vXrsHU1FRtf35+Ptq0aYOzZ88iJCQEH330EbKysnD69GmsWbMGPXr0gLOzs9p7oqOjUaVKlSKfbWtrq/F9ET0LEyAionIUFBSk9trMzAzfffcdgoKCEBAQIE9QT6lXrx7ee+891et33nkHXl5e+Pbbb4tNgLp06YJffvkF3333HYyMHv8ZWbNmDXx9fXHr1i218zdt2oS//voLq1evRr9+/dSOPXz4EHl5eUU+o1evXnBwcHjZWyMqEZvAiF5QYVX9hQsXMHDgQNja2sLGxgahoaHIyclRnXf58mUoFIpimyWebvoovOa5c+fw3nvvwcbGBtWqVcOkSZMgSRKuXr2Kbt26wdraGk5OTvjmm29eKPbt27ejdevWsLS0hJWVFbp27YrTp0+rnTNw4EBUqVIF//77Lzp27AhLS0s4Oztj2rRpkCRJ7dzs7GyMHTsWtWrVgqmpKerXr4+vv/66yHkAsGrVKjRv3hwWFhaws7NDmzZtsHPnziLn7d+/H82bN4eZmRnc3d2xYsUKteP5+fmYOnUqXnnlFZiZmaFq1apo1aoV4uLiSrzvo0ePQqFQYPny5UWO7dixAwqFAr///jsAIDMzEx9//DHc3NxgamqK6tWrIygoCMePHy+5YF+CJj+T1NRUhIaGombNmjA1NUWNGjXQrVs3XL58GYDoP3X69Gns3btX1WT0IkmWp6cnHBwccPHixWKP9+3bF7dv31Yr67y8PGzYsKFIggNAdZ2WLVsWOWZmZgZra+tSx0j0spgAEb2k3r17IzMzE1FRUejduzeWLVuGqVOnvtQ1g4ODoVQqMWPGDPj7+yMyMhLz5s1DUFAQXFxcMHPmTNStWxeffPIJ9u3bV6prr1y5El27dkWVKlUwc+ZMTJo0CWfOnEGrVq1Uf0gLFRQUoFOnTnB0dMSsWbPg6+uLiIgIREREqM6RJAlvv/025s6di06dOmHOnDmoX78+xo0bh/DwcLXrTZ06Fe+//z6MjY0xbdo0TJ06FbVq1cLu3bvVzrtw4QJ69eqFoKAgfPPNN7Czs8PAgQPVEoIpU6Zg6tSpaNeuHebPn4+JEyeidu3az0xQ/Pz84O7ujvXr1xc5tm7dOtjZ2alqPIYPH47o6Gi88847WLhwIT755BOYm5sjKSlJ47LWlKY/k3feeQcbN25EaGgoFi5ciFGjRiEzMxMpKSkAgHnz5qFmzZpo0KABVq5ciZUrV2LixImljuf+/fu4e/cu7Ozsij3u5uaGFi1a4Oeff1bt2759O+7fv48+ffoUOd/V1RUAsGLFimKT4uLcuXMHt27dUnvcu3ev1PdCVCKJiF5IRESEBEAaNGiQ2v4ePXpIVatWVb2+dOmSBEBaunRpkWsAkCIiIopcc9iwYap9jx49kmrWrCkpFAppxowZqv13796VzM3NpZCQEI1jzszMlGxtbaWhQ4eq7U9NTZVsbGzU9oeEhEgApI8++ki1T6lUSl27dpVMTEykmzdvSpIkSZs2bZIASJGRkWrX7NWrl6RQKKQLFy5IkiRJ58+flwwMDKQePXpIBQUFaucqlUrVtqurqwRA2rdvn2pfenq6ZGpqKo0dO1a1z9vbW+ratavG915o/PjxkrGxsXTnzh3VvtzcXMnW1lbtZ2ljYyONHDmy1Nd/nl9++UUCIO3Zs0eSJM1/Jnfv3pUASLNnz37m9Rs2bCi1bdtW43gASIMHD5Zu3rwppaenS0ePHpU6depU7GctXbpUAiAdOXJEmj9/vmRlZSXl5ORIkiRJ7777rtSuXTtJksTP8MmfTU5OjlS/fn0JgOTq6ioNHDhQ+umnn6S0tLQi8RT+DhT3qF+/vsb3RfQ8rAEieknDhw9Xe926dWvcvn0bGRkZL3zNJzvNGhoaws/PD5IkYfDgwar9tra2qF+/Pv7991+NrxsXF4d79+6hb9++av+zNjQ0hL+/P/bs2VPkPWFhYapthUKBsLAw5OXlYdeuXQCAbdu2wdDQEKNGjVJ739ixYyFJErZv3w5A9ANRKpWYPHkyDAzU/+lRKBRqr728vNC6dWvV62rVqhW5V1tbW5w+fRrnz5/X+P4BUbuWn5+PmJgY1b6dO3fi3r17CA4OVrv+oUOH1EYolQdNfybm5uYwMTFBQkIC7t69W6Yx/PTTT6hWrRqqV68OPz8/xMfH49NPPy1Sg/ek3r1748GDB/j999+RmZmJ33//vdjmr8LYDx06hHHjxgEQo8kGDx6MGjVq4KOPPkJubm6R9/z666+Ii4tTeyxdurRsbpgI7ARN9NJq166t9rqw2eDu3bsv3Lfh6Wva2NjAzMysSKdQGxsb3L59W+PrFiYLb7zxRrHHn47XwMAA7u7uavvq1asHAKqmmStXrsDZ2RlWVlZq5xWOcLpy5QoA0Q/EwMAAXl5ez43z6fsHRLk++Yd/2rRp6NatG+rVq4dGjRqhU6dOeP/99587msrb2xsNGjTAunXrVAnlunXr4ODgoFYus2bNQkhICGrVqgVfX1906dIFAwYMKFIeL0vTn4mpqSlmzpyJsWPHwtHREa+99hrefPNNDBgwAE5OTi8VQ7du3VSJ7ZEjR/DVV18hJyenSKL6pGrVqiEwMBBr1qxBTk4OCgoK0KtXrxLPt7GxwaxZszBr1ixcuXIF8fHx+PrrrzF//nzY2NggMjJS7fw2bdqwEzSVKyZARC/J0NCw2P3S//o6PF27UaigoKBU13ze52hCqVQCEH1Oivuj+eSIHjlpcq9t2rTBxYsXsXnzZuzcuRM//vgj5s6di0WLFqnVoBUnODgY06dPx61bt2BlZYUtW7agb9++avffu3dvtG7dGhs3bsTOnTsxe/ZszJw5EzExMejcuXPZ3ChK9zP5+OOP8dZbb2HTpk3YsWMHJk2ahKioKOzevRtNmzZ94Rhq1qyJwMBAAGKEl4ODA8LCwtCuXTv07NmzxPf169cPQ4cORWpqKjp37qzxEHVXV1cMGjQIPXr0gLu7O1avXl0kASIqb2wCIypnhTVCT3fgLKwZqUgeHh4AgOrVqyMwMLDI4+kRQ0qlskgT27lz5wCIjrCA+GN2/fp1ZGZmqp139uxZ1fHCz1YqlThz5kyZ3Y+9vT1CQ0Px888/4+rVq2jSpIlGEwoGBwfj0aNH+PXXX7F9+3ZkZGQU23m3Ro0aGDFiBDZt2oRLly6hatWqmD59epnFD5T+Z+Lh4YGxY8di586dOHXqFPLy8tRGA5aUcJfGBx98AA8PD3zxxRfPTLB79OgBAwMDHDx4sMTmr2exs7ODh4cHbty48TLhEr0QJkBE5cza2hoODg5FRmstXLiwwmPp2LEjrK2t8dVXXyE/P7/I8Zs3bxbZN3/+fNW2JEmYP38+jI2N0b59ewCixqCgoEDtPACYO3cuFAqFqrake/fuMDAwwLRp01S1Hk9et7SebvqrUqUK6tatW2x/kqd5enqicePGWLduHdatW4caNWqgTZs2quMFBQW4f/++2nuqV68OZ2dntevfunULZ8+eVZv2oLQ0/Znk5OTg4cOHasc8PDxgZWWlFpOlpeVLj5YyMjLC2LFjkZSUhM2bN5d4XpUqVRAdHY0pU6bgrbfeKvG8v//+u8jcQID4T8CZM2dQv379l4qX6EVoR303kY4bMmQIZsyYgSFDhsDPzw/79u1T1aRUJGtra0RHR+P999/Hq6++ij59+qBatWpISUnB1q1b0bJlS7VExszMDLGxsQgJCYG/vz+2b9+OrVu3YsKECahWrRoA4K233kK7du0wceJEXL58Gd7e3ti5cyc2b96Mjz/+WFXDUbduXUycOBFffvklWrdujZ49e8LU1BRHjhyBs7MzoqKiSnUvXl5eCAgIgK+vL+zt7XH06FFs2LBBrdP2swQHB2Py5MkwMzPD4MGD1fq7ZGZmombNmujVqxe8vb1RpUoV7Nq1C0eOHFGrbZk/fz6mTp2KPXv2vPCkhpr+TM6dO4f27dujd+/e8PLygpGRETZu3Ii0tDS12itfX19ER0cjMjISdevWRfXq1UvsX/QsAwcOxOTJkzFz5kx07969xPNCQkKee624uDhERETg7bffxmuvvaaaX2rJkiXIzc0tttZuw4YNxc4EHRQUBEdHx9LcClHx5BuARlS5FQ7XLRwOXqhwqPClS5dU+3JycqTBgwdLNjY2kpWVldS7d28pPT29xGHwT18zJCREsrS0LBJD27ZtpYYNG5Y69j179kgdO3aUbGxsJDMzM8nDw0MaOHCgdPTo0SKfefHiRalDhw6ShYWF5OjoKEVERBQZxp6ZmSmNGTNGcnZ2loyNjaVXXnlFmj17ttrw9kJLliyRmjZtKpmamkp2dnZS27Ztpbi4ONXxp4dQP3mvTw7vjoyMlJo3by7Z2tpK5ubmUoMGDaTp06dLeXl5GpXB+fPnVcOr9+/fr3YsNzdXGjdunOTt7S1ZWVlJlpaWkre3t7Rw4UK18wp/XoVD2jXx9DD4Qs/7mdy6dUsaOXKk1KBBA8nS0lKysbGR/P39pfXr16tdJzU1VeratatkZWUlAXjukHgAJQ73nzJlilqsTw6Df5anf4b//vuvNHnyZOm1116TqlevLhkZGUnVqlWTunbtKu3evVvtvc8aBl/asiZ6FoUkvUDdMxHpvIEDB2LDhg3IysqSOxQiojLHPkBERESkd9gHiEhH3Lx585lD601MTGBvb1+BERERaS8mQEQ6olmzZs8cWt+2bVskJCRUXEBERFqMfYCIdMSff/6JBw8elHjczs4Ovr6+FRgREZH2YgJEREREeoedoImIiEjvsA9QMZRKJa5fvw4rK6symVaeiIiIyp8kScjMzISzs/MzF/MFmAAV6/r166hVq5bcYRAREdELuHr1KmrWrPnMc5gAFcPKygqAKEBra+syuWZ+fj527tyJDh06wNjYuEyuqatYVqXD8tIcy6p0WF6aY1lprjzLKiMjA7Vq1VL9HX8WJkDFKGz2sra2LtMEyMLCAtbW1vzleA6WVemwvDTHsiodlpfmWFaaq4iy0qT7CjtBExERkd5hAkRERER6hwkQERER6R0mQERERKR3mAARERGR3mECRERERHqHCRARERHpHSZAREREpHeYABEREZHeYQJEREREeocJEBEREekdJkBERESkd5gAVaC8PANs26bA6dNyR0JERKTfmABVoJ9+aoTu3Y2waJHckRAREek3JkAVqFmzNADApk2AJMkbCxERkT5jAlSBmjS5iSpVJFy7Bhw9Knc0RERE+osJUAUyMVGiUydR9RMTI3MwREREeowJUAXr1k0JANi4UeZAiIiI9BgToArWubMEExMgORlISpI7GiIiIv3EBKiCWVsD7duLbdYCERERyUP2BGjBggVwc3ODmZkZ/P39cfjw4RLPjYmJgZ+fH2xtbWFpaQkfHx+sXLlS7ZysrCyEhYWhZs2aMDc3h5eXFxZp2bjzHj3EMxMgIiIieciaAK1btw7h4eGIiIjA8ePH4e3tjY4dOyI9Pb3Y8+3t7TFx4kQkJibi5MmTCA0NRWhoKHbs2KE6Jzw8HLGxsVi1ahWSkpLw8ccfIywsDFu2bKmo23qut98GFAoxEuzqVbmjISIi0j+yJkBz5szB0KFDERoaqqqpsbCwwJIlS4o9PyAgAD169ICnpyc8PDwwevRoNGnSBPv371edc+DAAYSEhCAgIABubm4YNmwYvL29n1mzVNEcHYFWrcT2pk2yhkJERKSXjOT64Ly8PBw7dgzjx49X7TMwMEBgYCASExOf+35JkrB7924kJydj5syZqv2vv/46tmzZgkGDBsHZ2RkJCQk4d+4c5s6dW+K1cnNzkZubq3qdkZEBAMjPz0d+fv6L3F4RhdcpfH77bQP88YchYmKUGD68oEw+Q1c8XVb0bCwvzbGsSoflpTmWlebKs6xKc03ZEqBbt26hoKAAjo6OavsdHR1x9uzZEt93//59uLi4IDc3F4aGhli4cCGCgoJUx7///nsMGzYMNWvWhJGREQwMDLB48WK0adOmxGtGRUVh6tSpRfbv3LkTFhYWL3B3JYuLiwMAWFubA+iAffuAtWvjYG3NX5qnFZYVaYblpTmWVemwvDTHstJceZRVTk6OxufKlgC9KCsrK5w4cQJZWVmIj49HeHg43N3dERAQAEAkQAcPHsSWLVvg6uqKffv2YeTIkXB2dkZgYGCx1xw/fjzCw8NVrzMyMlCrVi106NAB1tbWZRJ3fn4+4uLiEBQUBGNjYwDAwoUS/v7bAHl5HdClC9fGKFRcWVHJWF6aY1mVDstLcywrzZVnWRW24GhCtgTIwcEBhoaGSEtLU9uflpYGJyenEt9nYGCAunXrAgB8fHyQlJSEqKgoBAQE4MGDB5gwYQI2btyIrl27AgCaNGmCEydO4Ouvvy4xATI1NYWpqWmR/cbGxmX+w3nymj17An//DWzebITBg8v0Y3RCeZS/LmN5aY5lVTosL82xrDRXXn9jNSVbJ2gTExP4+voiPj5etU+pVCI+Ph4tWrTQ+DpKpVLVf6ewz46BgfptGRoaQqlUlk3gZahwOPzOnUBWlryxEBER6RNZm8DCw8MREhICPz8/NG/eHPPmzUN2djZCQ0MBAAMGDICLiwuioqIAiL46fn5+8PDwQG5uLrZt24aVK1ciOjoaAGBtbY22bdti3LhxMDc3h6urK/bu3YsVK1Zgzpw5st1nSRo1Al55BTh/Hvj9d6BPH7kjIiIi0g+yJkDBwcG4efMmJk+ejNTUVPj4+CA2NlbVMTolJUWtNic7OxsjRozAtWvXYG5ujgYNGmDVqlUIDg5WnbN27VqMHz8e/fv3x507d+Dq6orp06dj+PDhFX5/z6NQAMHBQGQksHYtEyAiIqKKInsn6LCwMISFhRV7LCEhQe11ZGQkIiMjn3k9JycnLF26tKzCK3eFCdD27cD9+4CNjdwRERER6T7Zl8LQd40aAV5eQF4eJ0UkIiKqKEyAtEBh09e6dfLGQUREpC+YAGmBwi5McXHA7dvyxkJERKQPmABpgXr1gKZNgUePgJgYuaMhIiLSfUyAtERhLRCbwYiIiMofEyAt0bu3eN6zB3hqcmwiIiIqY0yAtESdOoC/P6BUAhs2yB0NERGRbmMCpEUKm8HWrpU3DiIiIl3HBEiL9O4tZofevx+4dk3uaIiIiHQXEyAt4uICtGolttevlzcWIiIiXcYESMsUToq4Zo28cRAREekyJkBapndvwNgYOHYMOH1a7miIiIh0ExMgLePgAHTtKrZXrJA3FiIiIl3FBEgLDRggnletAgoK5I2FiIhIFzEB0kJdugD29sD168Du3XJHQ0REpHuYAGkhU9PHnaHZDEZERFT2mABpqcJmsJgYIDNT3liIiIh0DRMgLdW8uVglPieHK8QTERGVNSZAWkqhAEJCxPby5fLGQkREpGuYAGmx994Tz3v2AFeuyBsLERGRLmECpMVq1wbatRPbq1fLGwsREZEuYQKk5Qo7Q69YAUiSvLEQERHpCiZAWu6ddwALCyA5GTh0SO5oiIiIdAMTIC1nZSWSIABYskTeWIiIiHQFE6BKYPBg8bx2LZCdLW8sREREuoAJUCXQpg3g4SEmRPz1V7mjISIiqvyYAFUCCgUwaJDY/vZbdoYmIiJ6WUyAKolhwwBLS+D4ceC33+SOhoiIqHJjAlRJODgAH30ktiMiAKVS3niIiIgqMyZAlcgnnwBVqgAnTgDffy93NERERJUXE6BKpGpVICpKbI8bBxw7Jm88RERElRUToEpm5EigWzcgPx/o00eMDCMiIqLSYQJUySgUYkLEWrWACxeADz/kqDAiIqLSYgJUCdnbA2vWAAYGYpHUFSvkjoiIiKhyYQJUSbVqBUydKrZHjhRrhREREZFmmABVYuPHA+3aieUx+vQBHj6UOyIiIqLKgQlQJWZoCKxaJeYIOnEC+PRTuSMiIiKqHJgAVXLOzsDy5WL7+++BzZvljYeIiKgyYAKkA7p0AcaOFduhocDVq/LGQ0REpO2YAOmIr74C/PyAu3eBfv2AR4/kjoiIiEh7MQHSESYmwNq1gJUVsH8/8OWXckdERESkvZgA6RAPD+CHH8T2l18Ce/bIGw8REZG2YgKkY/r0AQYPFrNDv/cecPOm3BERERFpHyZAOujbbwFPT+D6dSAkBFAq5Y6IiIhIuzAB0kGWlsC6dYCZGbB9u+ggTURERI8xAdJRjRsD0dFie/JkYNcueeMhIiLSJkyAdNjAgcCQIaI/UN++wLVrckdERESkHZgA6bjvvgN8fIBbt4DevYG8PLkjIiIikh8TIB1nbg78+itgYwMkJnK9MCIiIoAJkF5wdwdWrBDb334LrF8vbzxERERyYwKkJ95+G/jsM7E9eDBw9qy88RAREcmJCZAeiYwE2rYFsrKAnj2BzEy5IyIiIpIHEyA9YmQk1gtzdgaSkoABAzhJIhER6ScmQHrGyUl0ijYxATZtAqZPlzsiIiKiiscESA+99hqwcKHYjogAfv9d3niIiIgqGhMgPTV4MPDhh2KSxP79geRkuSMiIiKqOEyA9Ni8eUCrVkBGBtC9u3gmIiLSB7InQAsWLICbmxvMzMzg7++Pw4cPl3huTEwM/Pz8YGtrC0tLS/j4+GDlypVFzktKSsLbb78NGxsbWFpaolmzZkhJSSnP26iUTEyADRsAFxcxLP7999kpmoiI9IOsCdC6desQHh6OiIgIHD9+HN7e3ujYsSPS09OLPd/e3h4TJ05EYmIiTp48idDQUISGhmLHjh2qcy5evIhWrVqhQYMGSEhIwMmTJzFp0iSYmZlV1G1VKo6OwMaNgKkpsGULMHWq3BERERGVP1kToDlz5mDo0KEIDQ2Fl5cXFi1aBAsLCyxZsqTY8wMCAtCjRw94enrCw8MDo0ePRpMmTbB//37VORMnTkSXLl0wa9YsNG3aFB4eHnj77bdRvXr1irqtSqdZM2DRIrE9bRpniiYiIt1nJNcH5+Xl4dixYxg/frxqn4GBAQIDA5GYmPjc90uShN27dyM5ORkzZ84EACiVSmzduhWffvopOnbsiL/++gt16tTB+PHj0b179xKvlZubi9zcXNXrjP91hsnPz0d+fv4L3qG6wuuU1fXKWv/+wIkTBvj2W0OEhEioXbsAvr6SLLFoe1lpG5aX5lhWpcPy0hzLSnPlWValuaZCkiRZ/spdv34dLi4uOHDgAFq0aKHa/+mnn2Lv3r04dOhQse+7f/8+XFxckJubC0NDQyxcuBCDBg0CAKSmpqJGjRqwsLBAZGQk2rVrh9jYWEyYMAF79uxB27Zti73mlClTMLWYtp81a9bAwsKiDO62cigoAL76yh/HjjnB3v4BZs/eh6pVH8odFhERkUZycnLQr18/3L9/H9bW1s88V7YaoBdlZWWFEydOICsrC/Hx8QgPD4e7uzsCAgKg/F8P3m7dumHMmDEAAB8fHxw4cACLFi0qMQEaP348wsPDVa8zMjJQq1YtdOjQ4bkFqKn8/HzExcUhKCgIxsbGZXLN8tC2LdC6tYSkJHMsWBCE3bsLUNE5YGUpK23B8tIcy6p0WF6aY1lprjzLKqMUw5llS4AcHBxgaGiItLQ0tf1paWlwcnIq8X0GBgaoW7cuAJHcJCUlISoqCgEBAXBwcICRkRG8vLzU3uPp6anWT+hppqamMDU1LbLf2Ni4zH845XHNslS1qpgYsXlz4PhxAwwdaoC1awEDGXqLaXtZaRuWl+ZYVqXD8tIcy0pz5fU3VlOydYI2MTGBr68v4uPjVfuUSiXi4+PVmsSeR6lUqvrvmJiYoFmzZkh+ala/c+fOwdXVtWwC1wPu7kBMDGBsDPzyi+gYTUREpEtkbQILDw9HSEgI/Pz80Lx5c8ybNw/Z2dkIDQ0FAAwYMAAuLi6IiooCAERFRcHPzw8eHh7Izc3Ftm3bsHLlSkRHR6uuOW7cOAQHB6NNmzaqPkC//fYbEhIS5LjFSqtNGzEybPBgMTTe0xMIDpY7KiIiorIhawIUHByMmzdvYvLkyUhNTYWPjw9iY2Ph6OgIAEhJSYHBE20v2dnZGDFiBK5duwZzc3M0aNAAq1atQvATf5l79OiBRYsWISoqCqNGjUL9+vXx66+/olWrVhV+f5XdoEHAmTPAN98AAweKmqFmzeSOioiI6OXJ3gk6LCwMYWFhxR57utYmMjISkZGRz73moEGDVCPD6OXMnClmid66FejWDTh8GKhZU+6oiIiIXo7sS2GQdjM0BNasARo2BG7cAN58E8jMlDsqIiKil8MEiJ7L2lqMDHN0BP7+W/QFevRI7qiIiIheHBMg0oibG/Dbb4C5ObB9OzBqFCDPFJpEREQvjwkQaaxZM2D1akChAKKjgTlz5I6IiIjoxTABolLp0QP4+muxPW6cmC+IiIiosmECRKU2ZgwwYoRoAnvvPTEyjIiIqDJhAkSlplAA334LdOkCPHgAvPUWcOmS3FERERFpjgkQvRAjI2DdOsDHB0hPB7p2Be7elTsqIiIizTABohdWpYoYHu/iAiQlAe+8A+TlyR0VERHR8zEBopfi4iJmia5SBdizB/jgAw6PJyIi7ccEiF6atzewfr2YNXrZMmD6dLkjIiIiejYmQFQmOncG5s8X25MmieUziIiItBUTICozw4cDn3witkNDgb175Y2HiIioJEyAqEzNnAn06iU6Q3fvDpw5I3dERERERTEBojJlYACsXAm0bAncuyeaxq5flzsqIiIidUyAqMyZmQGbNwP16gEpKWKOoMxMuaMiIiJ6jAkQlYuqVcWq8dWrAydOAO++C+Tnyx0VERGRwASIyo27u5go0cIC2LFDdJLmHEFERKQNmABRuWrWTCyZYWAALFkCREbKHRERERETIKoAb74JLFggtidPBpYvlzceIiIiJkBUIYYPBz77TGwPGQLExckbDxER6TcmQFRhvvoK6NsXePRILJz6999yR0RERPqKCRBVGAMDYOlSICBADIvv0gW4elXuqIiISB8xAaIKZWoKbNwIeHmJCRK7dBETJhIREVUkJkBU4WxtxRxBNWoAp04BPXuKpTOIiIgqChMgkkXt2sDWrUCVKsCePcDgwZwjiIiIKg4TIJJN06bAhg2AoSGwahUwaZLcERERkb5gAkSy6tgR+OEHsT19+uNtIiKi8mQkdwBEgwYBV64A06YBI0YATk4KuUMiIiIdxxog0gpTpgADBwIFBUC/foa4cMFG7pCIiEiHMQEiraBQiOavoCAgO1uByMjXcOmS3FEREZGuYgJEWsPYWHSKbtxYwr17ZnjrLSPcuSN3VEREpIuYAJFWsbYGtmx5hKpVH+DcOQW6dQMePpQ7KiIi0jVMgEjruLgAkyYlwtpawv79QEgIoFTKHRUREekSJkCkldzcMvHLLwUwNgbWr3+8kjwREVFZYAJEWqtdOwlLlojtr78G5s+XNx4iItIdTIBIq733HhAZKbZHjQI2bZI1HCIi0hFMgEjrTZgADB0q1grr2xc4cEDuiIiIqLJjAkRaT6EAFi4EunQRI8K6dAGOH5c7KiIiqsyYAFGlYGQkOkO3agXcvw906ACcPi13VEREVFkxAaJKw9IS2LoV8PMDbt8GAgOB8+fljoqIiCojJkBUqVhbAzt2AI0bA6mpQPv2YiFVIiKi0mACRJWOvT0QFwfUrw9cvSpqgm7ckDsqIiKqTJgAUaXk6Ajs2gXUqQNcuCCSoJs35Y6KiIgqCyZAVGnVrAnEx4ulM86cEUnQ7dtyR0VERJUBEyCq1OrUEUmQkxNw8qRIgriCPBERPQ8TIKr06tcHdu8WzWInTjAJIiKi52MCRDrB01MkQdWrA3/9JeYJuntX7qiIiEhbMQEineHlJZrDqlUDjh0TSdC9e3JHRURE2ogJEOmURo1EEuTgABw9CnTsKGaOJiIiepLGCVCXLl1w/4m/JDNmzMC9J/57ffv2bXh5eZVpcEQvonFjMUS+alXg8GGgUycgI0PuqIiISJtonADt2LEDubm5qtdfffUV7jzR0/TRo0dITk4u2+iIXpC3t0iC7O2BgwdFEpSZKXdURESkLTROgCRJeuZrIm3j4yNmjLazAxITgc6dmQQREZHAPkCk0159VSRBtrbAn3+yTxAREQkaJ0AKhQIKhaLIPiJt5+urXhPUvj1njCYi0ndGmp4oSRIGDhwIU1NTAMDDhw8xfPhwWFpaAoBa/yAibePnB+zZAwQFiSHyAQGij5Cjo9yRERGRHDROgEJCQtRev/fee0XOGTBgwMtHRFROvL2BvXvFTNGnTgFt2ogh8zVryh0ZERFVNI0ToKVLl5ZnHEQVwtMT2LcPeOMN4Nw5oHVrYOdO4JVX5I6MiIgq0kt3gr5y5QrOnDkDpVL5wtdYsGAB3NzcYGZmBn9/fxw+fLjEc2NiYuDn5wdbW1tYWlrCx8cHK1euLPH84cOHQ6FQYN68eS8cH+kWDw/gjz+AunWBy5eB118X8wUREZH+0DgBWrJkCebMmaO2b9iwYXB3d0fjxo3RqFEjXL16tdQBrFu3DuHh4YiIiMDx48fh7e2Njh07Ij09vdjz7e3tMXHiRCQmJuLkyZMIDQ1FaGgoduzYUeTcjRs34uDBg3B2di51XKTbatcWo8L8/IBbt4B27YDt2+WOioiIKorGCdAPP/wAOzs71evY2FgsXboUK1aswJEjR2Bra4upU6eWOoA5c+Zg6NChCA0NhZeXFxYtWgQLCwssWbKk2PMDAgLQo0cPeHp6wsPDA6NHj0aTJk2wf/9+tfP+++8/fPTRR1i9ejWMjY1LHRfpvurVRcfojh2BnBzgrbeAZcvkjoqIiCqCxn2Azp8/Dz8/P9XrzZs3o1u3bujfvz8AMTN0aGhoqT48Ly8Px44dw/jx41X7DAwMEBgYiMTExOe+X5Ik7N69G8nJyZg5c6Zqv1KpxPvvv49x48ahYcOGz71Obm6u2ii2jP+tm5Cfn4/8/PzS3FKJCq9TVtfTZRVZVqamQEwM8MEHhli1ygChocDFiwX44gslDCrJLFn8bmmOZVU6LC/Nsaw0V55lVZprapwAPXjwANbW1qrXBw4cwODBg1Wv3d3dkZqaqvEHA8CtW7dQUFAAx6fGIjs6OuLs2bMlvu/+/ftwcXFBbm4uDA0NsXDhQgQFBamOz5w5E0ZGRhg1apRGcURFRRVbe7Vz505YWFhoeDeaiYuLK9Pr6bKKLKt33gEePPDEr7/WQ2SkIXbtSsPo0cdhbl5QYTG8LH63NMeyKh2Wl+ZYVporj7LKycnR+FyNEyBXV1ccO3YMrq6uuHXrFk6fPo2WLVuqjqempsLGxqZ0kb4gKysrnDhxAllZWYiPj0d4eDjc3d0REBCAY8eO4dtvv8Xx48c1nqhx/PjxCA8PV73OyMhArVq10KFDB7Wk72Xk5+cjLi4OQUFBbJJ7DrnKqmtXYPnyRxg50hAHDzojK6sGfvnlETw8KiyEF8LvluZYVqXD8tIcy0pz5VlWGaVY+bpU8wCNHDkSp0+fxu7du9GgQQP4+vqqjh84cACNGjUqVaAODg4wNDREWlqa2v60tDQ4OTmV+D4DAwPUrVsXAODj44OkpCRERUUhICAAf/zxB9LT01G7dm3V+QUFBRg7dizmzZuHy5cvF7meqampaoLHJxkbG5f5D6c8rqmr5CirIUOAhg2Bnj2BU6cU8Pc3xg8/AMHBFRrGC+F3S3Msq9JheWmOZaW58vobqymNezl8+umnGDp0KGJiYmBmZoZffvlF7fiff/6Jvn37ah4lABMTE/j6+iI+Pl61T6lUIj4+Hi1atND4OkqlUtWH5/3338fJkydx4sQJ1cPZ2Rnjxo0rdqQY0dNatACOHgVatgQyMoA+fURilJ0td2RERFRWNK4BMjAwwLRp0zBt2rRijz+dEGkqPDwcISEh8PPzQ/PmzTFv3jxkZ2erOlQPGDAALi4uiIqKAiD66/j5+cHDwwO5ubnYtm0bVq5ciejoaABA1apVUbVqVbXPMDY2hpOTE+rXr/9CMZL+cXEBEhKAqVOB6dOBn34SEyguXSoSIyIiqtw0ToDKS3BwMG7evInJkycjNTUVPj4+iI2NVXWMTklJgcETw3Gys7MxYsQIXLt2Debm5mjQoAFWrVqF4MrQRkGVipER8OWXYo6gAQOA8+fFzNFjxgCRkYC5udwREhHRi9I4AXJ3d9fovH///bfUQYSFhSEsLKzYYwkJCWqvIyMjERkZWarrF9fvh0hTb7wh1g4bM0bMEzRnjhg6//XXoq+Qhn3tiYhIi2icAF2+fBmurq7o168fqlevXp4xEWkdW1vR/NWrFzB8uFhCo1cvUTs0Zw7g4yNzgEREVCoaJ0Dr1q1TLYfRuXNnDBo0CF26dFFrniLSdV27AmfPAjNnArNni5mkmzYVydDUqYCXl9wREhGRJjTOXt59911s374dFy5cgK+vL8aMGYNatWrh888/x/nz58szRiKtYmkJTJsGJCUBffuKJrANG4BGjYD33hN9hYiISLuVuvrGxcUFEydOxPnz57FmzRocOnQIDRo0wN27d8sjPiKt5eYGrFkDnDwp+gJJErB6NeDpCQwaBKSkyB0hERGV5IXarx4+fIhVq1Zh6tSpOHToEN59990yXzKCqLJo1Aj49Vfg+HHgzTeBggLRX6hhQ2DBAkCplDtCIiJ6WqkSoEOHDmHYsGFwcnLCnDlz0LNnT/z3339Yu3ZtsTMpE+mTpk2B334DEhPFXEFZWUBYGBAQAFy4IHd0RET0JI0ToIYNG+LNN9+Eubk59u7di+PHjyMsLAx2dnblGR9RpfPaa2LSxO+/F/2F/vgDaNYM2LlT7siIiKiQxglQUlISHj58iBUrVqBdu3awt7cv9kFEgIGBqP05dQp4/XXg3j2gc2dg3jzRV4iIiOSl8TD4pUuXlmccRDrJzQ3YvRv48EPRL2jMGCA5WfQN4gwSRETyKdVq8ERUeqamYi0xb28gPBxYtEh0lF60iEkQEZFcyuyf3xs3bpS4nAWRvlMogNGjgRUrRNKzeDEwcaLcURER6a9SJUCnT5/G/Pnz8cMPP+DevXsAgFu3bmHMmDFwd3fHnj17yiNGIp3Rv7+oDQKAGTNEIkRERBVP4wRoy5YtaNq0KUaNGoXhw4fDz88Pe/bsgaenJ5KSkrBx40acPn26PGMl0gkDBwJTpojtkSPFsHkiIqpYGidAkZGRGDlyJDIyMjBnzhz8+++/GDVqFLZt24bY2Fh06tSpPOMk0imTJ4v1w/LzxXN6utwRERHpF40ToOTkZIwcORJVqlTBRx99BAMDA8ydOxfNmjUrz/iIdJJCASxZAjRoAFy/DgwbxuHxREQVSeMEKDMzE9bW1gAAQ0NDmJubw93dvdwCI9J1VlbAzz8DxsbA5s1imDwREVUMjYfBA8COHTtgY2MDAFAqlYiPj8epU6fUznn77bfLLjoiHefjA3z5JfD552KOoE6dAGdnuaMiItJ9pUqAnp4L6IMPPlB7rVAoUFBQ8PJREemRTz4BYmKAw4eBjz4SC6sSEVH50rgJTKlUPvfB5Ieo9AwNxXB4IyORCP3+u9wRERHpPs5DS6QFmjQRs0QDwNixYnQYERGVHyZARFpi4kSgenXg3DkgOlruaIiIdBsTICItYW0tOkQDYqLEO3dkDYeISKcxASLSIoMGAY0aAXfvAtOmyR0NEZHuYgJEpEWMjIA5c8T2ggWiOYyIiMpeqROgq1ev4tq1a6rXhw8fxscff4wffvihTAMj0ldBQUCXLsCjR8D06XJHQ0Skm0qdAPXr10+16ntqaiqCgoJw+PBhTJw4EdNYZ09UJgoXS129Gvj3X1lDISLSSaVOgE6dOoXmzZsDANavX49GjRrhwIEDWL16NZYtW1bW8RHppWbNgI4dgYICYMYMuaMhItI9pU6A8vPzYWpqCgDYtWuXaumLBg0a4MaNG2UbHZEemzRJPC9bBqSkyBoKEZHOKXUC1LBhQyxatAh//PEH4uLi0KlTJwDA9evXUbVq1TIPkEhftWwJBASISRFnz5Y7GiIi3VLqBGjmzJn4v//7PwQEBKBv377w9vYGAGzZskXVNEZEZaOwFmjxYoAVrEREZadUi6ECQEBAAG7duoWMjAzY2dmp9g8bNgyWlpZlGhyRvmvXDnj9deDAAWDhwscTJRIR0cspdQ3QG2+8gczMTLXkBwDs7e0RHBxcZoEREaBQAB9/LLYXLwby8mQNh4hIZ5Q6AUpISEBeMf8KP3z4EH/88UeZBEVEj3XvDjg7A2lpwK+/yh0NEZFu0LgJ7OTJk6rtM2fOIDU1VfW6oKAAsbGxcHFxKdvoiAjGxsCwYWJuoAULgL595Y6IiKjy0zgB8vHxgUKhgEKhwBtvvFHkuLm5Ob7//vsyDY6IhGHDgMhI4M8/gb//Bv439oCIiF6QxgnQpUuXIEkS3N3dcfjwYVSrVk11zMTEBNWrV4ehoWG5BEmk72rUAHr2BNavF7VAXHmGiOjlaJwAubq6AgCUSmW5BUNEJRs5UiRAq1cDs2YBtrZyR0REVHm90GrwK1euRMuWLeHs7IwrV64AAObOnYvNmzeXaXBE9Fjr1kDjxkBODrBqldzREBFVbqVOgKKjoxEeHo4uXbrg3r17KCgoAADY2dlh3rx5ZR0fEf2PQgEMHiy2V66UNxYiosqu1AnQ999/j8WLF2PixIlqfX78/Pzwzz//lGlwRKSub1/A0BA4fBhITpY7GiKiyqvUCdClS5fQtGnTIvtNTU2RnZ1dJkERUfGqVxerxANsBiMiehmlToDq1KmDEydOFNkfGxsLT0/PsoiJiJ7hvffE86pVAMckEBG9GI0ToGnTpiEnJwfh4eEYOXIk1q1bB0mScPjwYUyfPh3jx4/Hp59+Wp6xEhGAbt0AKyvg8mUxLxAREZWexgnQ1KlTkZWVhSFDhmDmzJn44osvkJOTg379+iE6Ohrffvst+vTpU56xEhEACwvgnXfENpvBiIhejMYJkCRJqu3+/fvj/PnzyMrKQmpqKq5du4bBhcNTiKjcvf++eF6/Hnj4UN5YiIgqo1L1AVIoFGqvLSwsUL169TINiIieLyAAqFkTuHcP2L5d8bzTiYjoKaVKgOrVqwd7e/tnPoio/BkYPF4U9ddfX2g+UyIivabxUhiA6AdkY2NTXrEQUSn06gXMng1s3arAO+8wCSIiKo1SJUB9+vRhkxeRlmjWDKhVC7h6VYG//qqOHj3kjoiIqPLQ+L+NT/f/ISJ5KRSiFggAEhOd5Q2GiKiSeaFRYESkHQoToMOHnZCbK28sRESVicYJkFKpZPMXkZZ57TXAxUXCgwfG2LWLtbRERJpiz0miSszAAOjRQ6yHERPDX2ciIk3xX0yiSq5nT9E8/dtvCuTlyRwMEVElwQSIqJJr0UKCnd1D3LunQHy83NEQEVUOTICIKjlDQ8Df/wYAICZG5mCIiCoJJkBEOqBFC5EAbdoEFBTIGwsRUWWgFQnQggUL4ObmBjMzM/j7++Pw4cMlnhsTEwM/Pz/Y2trC0tISPj4+WLlypep4fn4+PvvsMzRu3BiWlpZwdnbGgAEDcP369Yq4FSJZNGx4C3Z2Em7dAvbvlzsaIiLtJ3sCtG7dOoSHhyMiIgLHjx+Ht7c3OnbsiPT09GLPt7e3x8SJE5GYmIiTJ08iNDQUoaGh2LFjBwAgJycHx48fx6RJk3D8+HHExMQgOTkZb7/9dkXeFlGFMjKS8OabojP0xo0yB0NEVAnIngDNmTMHQ4cORWhoKLy8vLBo0SJYWFhgyZIlxZ4fEBCAHj16wNPTEx4eHhg9ejSaNGmC/f/7b6+NjQ3i4uLQu3dv1K9fH6+99hrmz5+PY8eOISUlpSJvjahCde9eOBwe4LylRETPVqq1wMpaXl4ejh07hvHjx6v2GRgYIDAwEImJic99vyRJ2L17N5KTkzFz5swSz7t//z4UCgVsbW2LPZ6bm4vcJ6bRzcjIACCa0/Lz8zW8m2crvE5ZXU+XsaxKp7Cc2rbNg6WlIa5eVeDQoUfw9WUW9DR+t0qH5aU5lpXmyrOsSnNNhSTjGhfXr1+Hi4sLDhw4gBYtWqj2f/rpp9i7dy8OHTpU7Pvu378PFxcX5ObmwtDQEAsXLsSgQYOKPffhw4do2bIlGjRogNWrVxd7zpQpUzB16tQi+9esWQMLC4sXuDMiecya5YcDB1zwzjvn8P77SXKHQ0RUoXJyctCvXz/cv38f1tbWzzxX1hqgF2VlZYUTJ04gKysL8fHxCA8Ph7u7OwICAtTOy8/PR+/evSFJEqKjo0u83vjx4xEeHq56nZGRgVq1aqFDhw7PLUBN5efnIy4uDkFBQTA2Ni6Ta+oqllXpPFleGRkmOHAAOHnyFXTuXAdcw1gdv1ulw/LSHMtKc+VZVoUtOJqQNQFycHCAoaEh0tLS1PanpaXBycmpxPcZGBigbt26AAAfHx8kJSUhKipKLQEqTH6uXLmC3bt3PzORMTU1hampaZH9xsbGZf7DKY9r6iqWVekYGxujWzcjmJgA588rcOGCMby85I5KO/G7VTosL82xrDRXXn9jNSVrJ2gTExP4+voi/onpa5VKJeLj49WaxJ5HqVSq9eEpTH7Onz+PXbt2oWrVqmUaN5G2srYGAgPFNkeDERGVTPZRYOHh4Vi8eDGWL1+OpKQkfPjhh8jOzkZoaCgAYMCAAWqdpKOiohAXF4d///0XSUlJ+Oabb7By5Uq89957AETy06tXLxw9ehSrV69GQUEBUlNTkZqaijwulER6oGdP8cxZoYmISiZ7H6Dg4GDcvHkTkydPRmpqKnx8fBAbGwtHR0cAQEpKCgwMHudp2dnZGDFiBK5duwZzc3M0aNAAq1atQnBwMADgv//+w5YtWwCI5rEn7dmzp0g/ISJd8/bbYpX448eBy5cBNze5IyIi0j6yJ0AAEBYWhrCwsGKPJSQkqL2OjIxEZGRkiddyc3ODjAPbiGRXrRrQpg2QkCCawcaMkTsiIiLtI3sTGBGVPTaDERE9GxMgIh3Uvbt4/vNP4KlBlkREBCZARDqpVi2gWTOxJMbmzXJHQ0SkfZgAEekoNoMREZWMCRCRjipMgOLjgXv3ZA2FiEjrMAEi0lH16gENGwKPHgG//y53NERE2oUJEJEOYzMYEVHxmAAR6bDCBCg2FsjJkTcWIiJtwgSISId5e4uZoB88AHbskDsaIiLtwQSISIcpFGwGIyIqDhMgIh1XmAD99hvA9YCJiAQmQEQ6rkULwMkJuH8f2LNH7miIiLQDEyAiHWdg8HhpjI0bZQ2FiEhrMAEi0gOFzWCbNgEFBbKGQkSkFZgAEemBgADA1lYsjJqYKHc0RKTvzpyxhyTJGwMTICI9YGwMvPWW2OZoMCKqSPn5wMWLwLFjwK1bwKhRBpgwoTWio+VNQZgAEemJJ4fDy/0/LyLSD1evAp6eQN26gJ8fUK0asGiRIQCRDMmJCRCRnujQAbCwAK5cAf76S+5oiEiXKJXA3LlAVBSQlCT2ZWYCb74pan/MzIDq1cX+qlUlTJqUiMmTlfIFDCZARHrDwgLo3FlsczQYEb2otDRg1iygd2/gyBGx74svgPBwYMIEwMsLeP99oE0b4ORJwNERSE4W70tPB1JSHsHXN13emwATICK9wlmhiehF/fcf8MEHQO3awGefAb/8Arz+uphrLCpKnNO6tZiBftUq4MQJ0eT1++/iPYB4bWws2y2oYQJEpEe6dhX/+Jw5A5w9K3c0RFQZSBKwZg3QqBHwww9iRnl/f9G89egRcPCgOG/KFGDfPmD/fqBpU6B9e+D4cdH3RxsZyR0AEVUcGxvxj1JsrGgGGz9e7oiISJvcvQucPi2ea9YEsrKA778XtT2ASGbmzBE1PZIExMeLpi0fH6BhQ3HO66+LxEfbMQEi0jM9e4oEKCaGCRARPbZ5M9CvH5CTU/SYkREwaZLo42P0v8xBoQACAys2xrLEJjAiPdOtm/iH6+hRICVF7miISG6pqSKx6dlTJD81awKvvipGbbm5iaauxERg8uTHyY8u0KFbISJNVK8uqq/37RO1QB9/LHdERCSXzZuB/v2B7GzxevBgYNEi3Up0SsIaICI91KuXeF67Vt44iKhiPXwohq7//TfwySdAjx4i+fHzA379FVi8WD+SH4A1QER6qXdvUfNz6BDw77+Au7vcERFRRejTR9T6POmDD4D58/Un8SnEGiAiPeToKEaDAcDPP8sbCxFVjD//FMmPQiFGhLZtC/z2GxAdrX/JD8AEiEhv9e0rnpkAEek+SQImThTbQ4YA9+4BCQmig7NCIWdk8mECRKSnevQATEzEnB///CN3NERUljZuFDU87u7A2LGif8/eveJ3ftIkuaPTDkyAiPSUra2YGRoQs7wSUeV0545Y7HjmTFHTc+cOEBoqRnpeuiQmLuzXT5z7ySdArVryxqstmAAR6bEnm8EkSd5YiOjFrFgBxMUBn38uanumTgXu3xdLV0yfLs7Jzwd8fYGICHlj1SZ62O2JiAq9+SZQpQpw5YqY6Oz11+WOiIhK68lRXXPnPt6eMUPU8pqZAVu2iCHuJiYVH5+2Yg0QkR4zNxd9gQB2hiaqjO7cAf74Q2xPnw688orYDgoCunQR2+HhosNz4TESmAAR6bnCZrD168XKzkRUeWzdChQUAI0bi+UskpNFje6WLfo7uktTTICI9FxgIFC1KpCeDuzeLXc0RPQ8d+4AZ8+K7ZgY8dytm3hWKIDatUWzFz0bEyAiPWdsLGaGBtgMRqTNJAn46SfAwwPw8gIiIx/3/3n3XXljq4yYABGRqhksJgZ48EDeWIioeEuXPp7EUJLEfD6SJNb2a9JE7ugqHyZARISWLUW1eUaGmBqfiLTD7dti9NaGDWKIOyDm8vHxEduGhqImiEqPCRARwcAAeO89sb1ypbyxEJGwfDlQpw4wbJho4rp3T8zlExUFrF4N1K8PfPGFeKbSYwJERACA998Xz9u3iw7RRFRxUlIAf3/gxx/F67t3gREjgMxM0d/HwUHM2bV4sVi41MtLdISeMkXWsCs1ToRIRACABg2AZs2AI0eAtWuBUaPkjohIfyxZAhw+DBw/Drz6qpi3JydHDG8/cQJQKoG8PMDCQu5IdQdrgIhIpbAWiM1gRBWrcAqKR4+APn2Ab78Vr8PCRBO1kRGTn7LGBIiIVPr0Ef/QHj0KJCXJHQ2R7jp5Eti2TWxnZwMHD4ptOzvg/HnRJGZrC/TvL1uIOo8JEBGpVKsGdO4stlkLRFQ+Hj0COnYU63QdOgTs3y8WK61dWzSBjRkj+vhERQGWlnJHq7vYB4iI1Lz/vhgKv2qVGF5rwP8mEZWp/fuB1FSxvWbN41mb33gDcHMD5syRLTS9wn/aiEjNW28BNjbA1avA3r1yR0OkewqXrwDEGnyFc2+1by9PPPqKCRARqTEze7w0BpvBiMqWUqmeAKWmiv521taiWYwqDhMgIipiwADx/MsvYiguEb2c1FSR/Bw9Cvz3n5jTp3DUJSAmPaxWTb749BETICIqomVLMQNtVhawaZPc0RBVbmvXAjVqiKUsli8X+958Exg3DnBxAaZPB7p3lzVEvcQEiIiKUCge/+90yRJ5YyGqzLKygPBwsb1gAbBihdgeMkRMcnjtGjBhgnzx6TMmQERUrEGDRCIUHw9cuCB3NESV0+zZwI0bYjs/XyRE9eqJEV8kLyZARFQsV1egUyexvXixvLEQVUYFBY9ndB4x4vH+Dz4Q/7kgeTEBIqISffCBeF66VKxDREQlu3rVCm3aGOL//k+8PnkSuH9fjPD67jsgOBho1AgIDZU3ThI4ESIRlahrV9F588YNYPNm4N135Y6ISLsolWLhUj8/ICGhJg4eNMDBg2Jou7u7OOf11wFDQ9EZmrQHa4CIqERGRsDgwWL7hx/kjYVIGy1dKiYwnDrVAFeuWKv2f/stMHeu2G7dWqbg6JmYABHRMw0eLPor7NrFztBETztwQDzv3m2Aq1etAACenmLf5cvimQmQdmICRETP5Ob2eIbaH3+UNRQirZOcLJ5PnQLS0sTKpU/+npiYAM2ayRAYPZdWJEALFiyAm5sbzMzM4O/vj8OHD5d4bkxMDPz8/GBrawtLS0v4+Phg5VPz9UuShMmTJ6NGjRowNzdHYGAgzp8/X963QaSz2BmaqHhnz4rnR4/EsC5HRwmvvy76zwFA8+aPFzsl7SJ7ArRu3TqEh4cjIiICx48fh7e3Nzp27Ij09PRiz7e3t8fEiRORmJiIkydPIjQ0FKGhodixY4fqnFmzZuG7777DokWLcOjQIVhaWqJjx454+PBhRd0WkU4p7Aydng5s2SJ3NETa4fZt8XhSw4YSAOCrr4BXX308CSJpH9kToDlz5mDo0KEIDQ2Fl5cXFi1aBAsLCywpYfrZgIAA9OjRA56envDw8MDo0aPRpEkT7N+/H4Co/Zk3bx6++OILdOvWDU2aNMGKFStw/fp1bOKc/kQvxNhYTIwIQDXEl0jfFTZ/PcnLSyRATZoAx44BPXpUcFCkMVmHwefl5eHYsWMYP368ap+BgQECAwORmJj43PdLkoTdu3cjOTkZM2fOBABcunQJqampCAwMVJ1nY2MDf39/JCYmok+fPkWuk5ubi9zcXNXrjIwMAEB+fj7y8/Nf+P6eVHidsrqeLmNZlU5FldeAAcBXXxlh1y4Fzp7Nh4dHuX5cueB3q3RYXs92+rQCgBFMTSXk5oomsAYNHiE/XylvYFquPL9XpbmmrAnQrVu3UFBQAEdHR7X9jo6OOFvYsFqM+/fvw8XFBbm5uTA0NMTChQsRFBQEAEhNTVVd4+lrFh57WlRUFKZOnVpk/86dO2FhYVGqe3qeuLi4Mr2eLmNZlU5FlJePz2v46y9HfPHFJbz/flK5f1554XerdFhe6iQJSEuzQGysG4BX0LTpDRw86AwAyMo6hG3b7soaX2VRHt+rnJwcjc+tlBMhWllZ4cSJE8jKykJ8fDzCw8Ph7u6OgICAF7re+PHjEf5EQ21GRgZq1aqFDh06wNra+hnv1Fx+fj7i4uIQFBQEY2PjMrmmrmJZlU5FlldengK9ewN//PEKli+vAxOTcv24MsfvVumwvIr3ww8GCAszVL3u08cRlpYFSE7OwZAhfrC2Zlk9S3l+rwpbcDQhawLk4OAAQ0NDpKWlqe1PS0uDk5NTie8zMDBA3bp1AQA+Pj5ISkpCVFQUAgICVO9LS0tDjRo11K7p4+NT7PVMTU1hampaZL+xsXGZ/3DK45q6imVVOhVRXt27A05OQGqqAtu3G6NXr3L9uHLD71bpsLwee/gQmD5dfZ+XlyFGjMjHtm27YW3dhWWlofL6G6spWTtBm5iYwNfXF/Hx8ap9SqUS8fHxaNGihcbXUSqVqj48derUgZOTk9o1MzIycOjQoVJdk4iKerIz9MKF8sZCJIeffhJLw9jaitcKBdCwoawh0QuSvQksPDwcISEh8PPzQ/PmzTFv3jxkZ2cj9H+rxQ0YMAAuLi6IiooCIPrr+Pn5wcPDA7m5udi2bRtWrlyJ6OhoAIBCocDHH3+MyMhIvPLKK6hTpw4mTZoEZ2dndO/eXa7bJNIZH3wAzJwJ7NkjJn9r1EjuiIjK39q1wOjRYioIAIiMBOrXB7KyAGdngP3EKx/ZE6Dg4GDcvHkTkydPRmpqKnx8fBAbG6vqxJySkgIDg8cVVdnZ2RgxYgSuXbsGc3NzNGjQAKtWrUJwcLDqnE8//RTZ2dkYNmwY7t27h1atWiE2NhZmnI2K6KXVri2awn79FZg/H1i0SO6IiMrPo0di6oePPhKdnwExv8+gQYC5ubyx0cuRPQECgLCwMISFhRV7LCEhQe11ZGQkIiMjn3k9hUKBadOmYdq0aWUVIhE94aOPRAK0ciUQFQXY2ckdEVHZ++MPICQEuHRJvP7gA2DWLKCMxsaQzGSfCJGIKp82bYDGjYGcHKCEOUuJKrVdu8QaeJcuAVWrAjNmANHRTH50CRMgIio1hULUAgHAggVAQYG88RCVlaws8d3u2BF48ADo0gW4cgX47DPxvSfdwQSIiF5I//6i6evSJWDbNrmjIXp5eXmif9v8+YBSKWY/j4kBLC3ljozKAxMgInohFhbAkCFie/ZseWMhehnZ2WJahy5dgPh4kfDs2AEsXw4UM0Uc6QgmQET0wkaPBkxMRGfRP/6QOxqi0nv0COjWDRg5UiQ/hobAhg1Ahw5yR0bljQkQEb0wFxdg4ECx/fTsuETaqHAoe2ws8NprQNu2j2t9Jk0CDh4EOnWSN0aqGEyAiOilfPaZ+F/zjh3AsWNyR0NUvL17gTffFE23AwcCPXsChw4BBw6I40uXAtOmAX5+soZJFUgr5gEiosrL3R3o2xdYtQr46isxPxCRNjh/Xgxn/+03YPv2x/uXLxfPgYFAu3ZA3brAu+/KEyPJhwkQEb208eNFAhQTA5w5A3h5yR0R6buTJ4HmzYH/LRMJIyPRab9dOzGnj52d+L5aWckbJ8mHCRARvTQvL9GkEBMjmhHWrpU7ItJnSqWYtTk3F2jSRHRoHjYMeOUVcbx3b3njI+3APkBEVCYmTxYTxa1bx75AJI9Ll4B33hFNWgcPitqdbdvENA2FyQ9RISZARFQmvL3F5IgA8Pnnj0fbEFWE9evF8iwxMY/X7poxQ4xUJCoOEyAiKjNffinmBdq1S/whIipvZ8+KBXn79BETGrZuLYa1JycDI0bIHR1pMyZARFRm3NzEsHgACAsD7t6VNRzSYZIk+vV4egITJojXI0cCe/YAb7wB1Ksnd4Sk7ZgAEVGZmjgRaNAASE0FPv1U7mhIV335JbB4MWBgALRoAXz7LfD992JOKiJNcBQYEZUpU1Pxh6l1a+DHH4F+/cTQY6KXce+eqF20txffsalTxf6FC8WIL6LSYg0QEZW5Vq2ADz8U20OHApmZ8sZD2iMtTUw66Or67NGCM2aIkVsxMUBSEvD668APP4j9hclPRASTH3pxTICIqFzMmAHUrAlcvAgMGCDmZiH9dvGiGKm1YQOQkiK+F7m5wLlzYnvuXNGXZ/lyMbnmhQtiWHujRiIJcnYGfH1FM9fXXwNTpsh9R1SZMQEionJhbS3+0JmYAJs2iXmCSD9dvSqasEJDgZs3gYYNgerVxazh3t4iKVq5EggPB5o1AwYPFu8rXJdLqQTeflus3XXkiLjW2LFy3Q3pCiZARFRu/P2BRYvE9vTpQHS0vPFQxZIkURPo6gpUrQr88YdYdf233x5/L5KTgbw8sTK7oaFoFisoEMnSoUNAQgLwzz/A5s2iRlGhAKpUkfW2SEewEzQRlavQUODKFdFvY+RIwNhYrMlEuu+zz8QszMDjiTFnzwbq1BGPP/8UNULOzqK2Z88eUVs4YMDj2p+2bWUJnfQAEyAiKncREcDt28D8+aJT9IMHwEcfyR0VlaXr14EuXYA33xRD1FNTgXnzxLH588XcPOnpQJs2j9/z+uvq13jjDfEgqghMgIio3CkUwHffif5Ac+YAo0YBOTliniCFQu7oqCz89BPw99/iYWcnJsHMzxdJzsiR4hxPT3ljJHoSEyAiqhAKhRi5Y2EBREaK9cIuXhS1AyYmckdHL2v9+sfbn3zyeEJCdlYmbcVO0ERUYRQK0TzyzTdie/FioGVL4Px5uSOjl5GUBJw6Jfp3jRolkp+CAsDDA+jWTe7oiIrHBIiIKlx4OLB1q2gqOXpUzPMyaJCYD4Yqh1OngFWrROfmX34R+zp0EEtSXLsmktutW7k0BWkvNoERkSw6dwZOnhSjxHbtApYuFX9QR4wQMwVXqybmEnJykjtSelrhvDyXLgF37oj+P4D4uQHiZ8aRfqTtWANERLKpWROIiwMOHAA6dhSdZr/9ViylUb8+UKOGWA5h9mwxYzBph8REkfwAwOjRYlbnWrUeJ0BElQETICKSXYsWwPbtwO+/A717i1mCbWzESt8XLojRYnXqAF27ik61S5cC//4r5pDhOmMVb82aovtmzxYd3IkqCzaBEZFWUChEgtO16+N99++L5TS++AK4cUM8tm1Tf5+RERASAgQFiQn1Xn1VzDZM5SM///GIr9GjxfQG7duLxJWoMmENEBFpLRsbsS7UxYtiluBFi4CPPxZzyxR2rn30SPRB6dNHTLJnawsMGyaa1d58E4iKEiOS6MVJkkhGJUk0Wd66JWrpvv5a1NBt2cL5nKjyYQ0QEWk9CwsgIEA8CuXliSTo0CGxxlhKikiU/vtPjEBavFict3UrsHOn+GN9+LAC27c3gr//y3WuVipF518Hh+KPZ2aKmilz88f7srMrZ83U6tUN0K+fEXJyRAf1+/fF/j59xD26u8sbH9GLYg0QEVVKJiYiAXr9dbGS+N69YtXxuLjHiUn79iLpSEgQa0uNGGGE337zQGCgEVJSRHNOYqJYkDM2FvDyEkt1nD8vOl+HhIhk5t13xfw2GRniumFhogZk+XLxOjVVrHDeo4dopnN3F6uaP3ggjn/5pVjA89dfK7yYXkpODrBpU13k5Ijqnf/7PyAmRhzr10/GwIjKAGuAiEhnKBRAYKCYmO/8ebHC+IULog/R+vVAjRoSHj7MxenTZnB1FTVLOTnq10hKEvPa3L8v3puaKmqQALGKeVjY41Xthw8HmjQBJkwAzpwRj/R00UR06xYwa5YYLj51qjh/5kwxWmr2bLHPy6viykYTKSmiL09YGPDee0BCggL5+YZwdZVQv74CO3eKpM7DA2jeXO5oiV4Oa4CISOc4OIiRZQqFqMlZt07U3ly48AhfffUH/P2VAETyU7Xq46U4OnQQz4XNPMDj5MfBAbh8WSzzAIganYcPRafr2NjH5x848Hg7Kgro3v1xH6QjR0Qn7w0bRGL05Odog2XLRJPiyJFi8drYWFHz06mTEl988fi8fv3Y54cqPyZARKQXrKzEUg01auTgjz8KkJ4uFu5MSxMLd96+DezYAUyaJJqwfvnlcR+eNm1E/6KBA8VrZ2fg+HFRw1QoIgIwNRXbnp6iJio3V9SqODiIUWqAqBkCxPVCQ0XH4v37xYzYZSk0VPSZKs38SX/8IZ4zMkRtVWys+BPRqZOE1q1FMmdjI65NVNmxCYyI9FK1auIBiKawwjlspk0TD0A0f82fD3z/vZiVeulS0RfIyUlM0piYKGpx7twR8xTl5Ylan2nTgHbtRE2PpaVIfs6cEf2TCj8jMhLYuBHo1Us8GxuLczw8Xv7ebt8WtTkA8Ndf6onak4YOFbNx79olkr3ExMfH5swBCgoUMDYuQECABEDcT0EBF68l3cAEiIioBGFh4vGkpk3VX9vYiAcATJ8u1jkr7IT9wQePz6teXdQuWVuLZrSqVUVTU2Gn4rw8MeHj3LniHFvbF4/7ydqkU6cANzeRfF2/LpqvatUStV8//ijO+f13oG5dMVLNzk7UHG3cKI75+KTD0lLckKEh1/Yi3cEEiIiojCgUJQ+NVyge1ywBwIcfiv5Cq1cDnTqJvkYxMeJhbi6aoEaOFLNhl9aTCdA//4haqX//Fa+PHBE1OfPnPz5n27bHiV2rVuL4qVPAyZOPUFDwN4D2pQ+CSMuxDxARkQwUCjF8/9QpkYCMHPn42IMHoqnNy0sMPS8cTv8sjx4BffuKjtyFfXkAUZNTmPwAornr5k2ReBWKjRXTCAAiATIwEKPbgoMl2NpyETbSTawBIiKSiUIh5g8CRJ+bd98Vr3/+GZg4UcxPNHy4GGbfqJFoyvL0FBMQVq0K1KsH/PmnaJa6dg1Yu7boZ1y9Kp7btBH9fe7dExMaPngANG4sOmnfuiWG+ANA27YVcedE8mMCRESkBYyMgNatxfbIkcCAAWKJj3nzgCtXgH37xENTBgZihJkk+i+jfXvRPBcTI5q4ALGW144dYsSbJIkEjPP7kL5gAkREpIWsrMS6Z2FhYm6eq1fF5I7nzonjV6+KSRt9fUWT1tGjYlTXL7+IWh5PTzEE/sIFcX67dmLUW2Gnazs70WRmayve89prYmZrzu9D+oIJEBGRFjMyAlq2fPY5kiSG7Ds5idFm06eLJq8bN0QCZGYmanaeXP9syBAx9L9nT+DwYdHnp3AeIyJ9wE7QRESVnEIh5iVSKIApU8SyH9Oni35DgFgvzdRUDHV/9VUxzH7EiMfvbdaMyQ/pH9YAERHpECMj0ZcHAIYNEzNWf/aZeK1QiIVhHzwQNUVE+owJEBGRjqpVC9i6VX2flZV4EOk7NoERERGR3mECRERERHqHCRARERHpHSZAREREpHeYABEREZHeYQJEREREeocJEBEREekdJkBERESkd5gAERERkd5hAkRERER6R/YEaMGCBXBzc4OZmRn8/f1x+PDhEs9dvHgxWrduDTs7O9jZ2SEwMLDI+VlZWQgLC0PNmjVhbm4OLy8vLFq0qLxvg4iIiCoRWROgdevWITw8HBERETh+/Di8vb3RsWNHpKenF3t+QkIC+vbtiz179iAxMRG1atVChw4d8N9//6nOCQ8PR2xsLFatWoWkpCR8/PHHCAsLw5YtWyrqtoiIiEjLyZoAzZkzB0OHDkVoaKiqpsbCwgJLliwp9vzVq1djxIgR8PHxQYMGDfDjjz9CqVQiPj5edc6BAwcQEhKCgIAAuLm5YdiwYfD29n5mzRIRERHpF9lWg8/Ly8OxY8cwfvx41T4DAwMEBgYiMTFRo2vk5OQgPz8f9vb2qn2vv/46tmzZgkGDBsHZ2RkJCQk4d+4c5s6dW+J1cnNzkZubq3qdkZEBAMjPz0d+fn5pb61Yhdcpq+vpMpZV6bC8NMeyKh2Wl+ZYVporz7IqzTUVkiRJZR6BBq5fvw4XFxccOHAALVq0UO3/9NNPsXfvXhw6dOi51xgxYgR27NiB06dPw8zMDIBIZoYNG4YVK1bAyMgIBgYGWLx4MQYMGFDidaZMmYKpU6cW2f/jjz/CwsLiBe6OiIiIKlpOTg6GDBmCe/fuwcbG5pnnylYD9LJmzJiBtWvXIiEhQZX8AMD333+PgwcPYsuWLXB1dcW+ffswcuRIODs7IzAwsNhrjR8/HuHh4arX//33H7y8vDBkyJByvw8iIiIqW5mZmdqbADk4OMDQ0BBpaWlq+9PS0uDk5PTM93799deYMWMGdu3ahSZNmqj2P3jwABMmTMDGjRvRtWtXAECTJk1w4sQJfP311yUmQKampjA1NVW9rlKlCq5evQorKysoFIoXvUU1GRkZqFWrFq5evQpra+syuaauYlmVDstLcyyr0mF5aY5lpbnyLCtJkpCZmQlnZ+fnnitbAmRiYgJfX1/Ex8eje/fuAKDq0BwWFlbi+2bNmoXp06djx44d8PPzUztW2GfHwEC9b7ehoSGUSqXGsRkYGKBmzZqa30wpWFtb85dDQyyr0mF5aY5lVTosL82xrDRXXmX1vJqfQrI2gYWHhyMkJAR+fn5o3rw55s2bh+zsbISGhgIABgwYABcXF0RFRQEAZs6cicmTJ2PNmjVwc3NDamoqAFFjU6VKFVhbW6Nt27YYN24czM3N4erqir1792LFihWYM2eObPdJRERE2kXWBCg4OBg3b97E5MmTkZqaCh8fH8TGxsLR0REAkJKSolabEx0djby8PPTq1UvtOhEREZgyZQoAYO3atRg/fjz69++PO3fuwNXVFdOnT8fw4cMr7L6IiIhIu8neCTosLKzEJq+EhAS115cvX37u9ZycnLB06dIyiKxsmZqaIiIiQq2vERWPZVU6LC/NsaxKh+WlOZaV5rSlrGQbBk9EREQkF9nXAiMiIiKqaEyAiIiISO8wASIiIiK9wwSIiIiI9A4ToAqyYMECuLm5wczMDP7+/lydHmINNoVCofZo0KCB6vjDhw8xcuRIVK1aFVWqVME777xTZOZwXbVv3z689dZbcHZ2hkKhwKZNm9SOS5KEyZMno0aNGjA3N0dgYCDOnz+vds6dO3fQv39/WFtbw9bWFoMHD0ZWVlYF3kXFeV55DRw4sMh3rVOnTmrn6Et5RUVFoVmzZrCyskL16tXRvXt3JCcnq52jye9eSkoKunbtCgsLC1SvXh3jxo3Do0ePKvJWyp0mZRUQEFDku/X0tCv6UFbR0dFo0qSJanLDFi1aYPv27arj2vidYgJUAdatW4fw8HBERETg+PHj8Pb2RseOHZGeni53aLJr2LAhbty4oXrs379fdWzMmDH47bff8Msvv2Dv3r24fv06evbsKWO0FSc7Oxve3t5YsGBBscdnzZqF7777DosWLcKhQ4dgaWmJjh074uHDh6pz+vfvj9OnTyMuLg6///479u3bh2HDhlXULVSo55UXAHTq1Entu/bzzz+rHdeX8tq7dy9GjhyJgwcPIi4uDvn5+ejQoQOys7NV5zzvd6+goABdu3ZFXl4eDhw4gOXLl2PZsmWYPHmyHLdUbjQpKwAYOnSo2ndr1qxZqmP6UlY1a9bEjBkzcOzYMRw9ehRvvPEGunXrhtOnTwPQ0u+UROWuefPm0siRI1WvCwoKJGdnZykqKkrGqOQXEREheXt7F3vs3r17krGxsfTLL7+o9iUlJUkApMTExAqKUDsAkDZu3Kh6rVQqJScnJ2n27Nmqfffu3ZNMTU2ln3/+WZIkSTpz5owEQDpy5IjqnO3bt0sKhUL677//Kix2OTxdXpIkSSEhIVK3bt1KfI8+l1d6eroEQNq7d68kSZr97m3btk0yMDCQUlNTVedER0dL1tbWUm5ubsXeQAV6uqwkSZLatm0rjR49usT36GtZSZIk2dnZST/++KPWfqdYA1TO8vLycOzYMbWFWA0MDBAYGIjExEQZI9MO58+fh7OzM9zd3dG/f3+kpKQAAI4dO4b8/Hy1cmvQoAFq166t9+V26dIlpKamqpWNjY0N/P39VWWTmJgIW1tbtfXyAgMDYWBggEOHDlV4zNogISEB1atXR/369fHhhx/i9u3bqmP6XF73798HANjb2wPQ7HcvMTERjRs3Vs3aDwAdO3ZERkaG6n/8uujpsiq0evVqODg4oFGjRhg/fjxycnJUx/SxrAoKCrB27VpkZ2ejRYsWWvudkn0maF1369YtFBQUqP1QAcDR0RFnz56VKSrt4O/vj2XLlqF+/fq4ceMGpk6ditatW+PUqVNITU2FiYkJbG1t1d7j6OioWgNOXxXef3HfqcJjqampqF69utpxIyMj2Nvb62X5derUCT179kSdOnVw8eJFTJgwAZ07d0ZiYiIMDQ31tryUSiU+/vhjtGzZEo0aNQIAjX73UlNTi/3+FR7TRcWVFQD069cPrq6ucHZ2xsmTJ/HZZ58hOTkZMTExAPSrrP755x+0aNECDx8+RJUqVbBx40Z4eXnhxIkTWvmdYgJEsuncubNqu0mTJvD394erqyvWr18Pc3NzGSMjXdOnTx/VduPGjdGkSRN4eHggISEB7du3lzEyeY0cORKnTp1S63tHxSuprJ7sJ9a4cWPUqFED7du3x8WLF+Hh4VHRYcqqfv36OHHiBO7fv48NGzYgJCQEe/fulTusErEJrJw5ODjA0NCwSG/3tLQ0ODk5yRSVdrK1tUW9evVw4cIFODk5IS8vD/fu3VM7h+UG1f0/6zvl5ORUpJP9o0ePcOfOHb0vPwBwd3eHg4MDLly4AEA/yyssLAy///479uzZg5o1a6r2a/K75+TkVOz3r/CYrimprIrj7+8PAGrfLX0pKxMTE9StWxe+vr6IioqCt7c3vv32W639TjEBKmcmJibw9fVFfHy8ap9SqUR8fDxatGghY2TaJysrCxcvXkSNGjXg6+sLY2NjtXJLTk5GSkqK3pdbnTp14OTkpFY2GRkZOHTokKpsWrRogXv37uHYsWOqc3bv3g2lUqn6B1qfXbt2Dbdv30aNGjUA6Fd5SZKEsLAwbNy4Ebt370adOnXUjmvyu9eiRQv8888/akljXFwcrK2t4eXlVTE3UgGeV1bFOXHiBACofbf0oayKo1QqkZubq73fqXLpWk1q1q5dK5mamkrLli2Tzpw5Iw0bNkyytbVV6+2uj8aOHSslJCRIly5dkv78808pMDBQcnBwkNLT0yVJkqThw4dLtWvXlnbv3i0dPXpUatGihdSiRQuZo64YmZmZ0l9//SX99ddfEgBpzpw50l9//SVduXJFkiRJmjFjhmRraytt3rxZOnnypNStWzepTp060oMHD1TX6NSpk9S0aVPp0KFD0v79+6VXXnlF6tu3r1y3VK6eVV6ZmZnSJ598IiUmJkqXLl2Sdu3aJb366qvSK6+8Ij18+FB1DX0prw8//FCysbGREhISpBs3bqgeOTk5qnOe97v36NEjqVGjRlKHDh2kEydOSLGxsVK1atWk8ePHy3FL5eZ5ZXXhwgVp2rRp0tGjR6VLly5Jmzdvltzd3aU2bdqorqEvZfX5559Le/fulS5duiSdPHlS+vzzzyWFQiHt3LlTkiTt/E4xAaog33//vVS7dm3JxMREat68uXTw4EG5Q5JdcHCwVKNGDcnExERycXGRgoODpQsXLqiOP3jwQBoxYoRkZ2cnWVhYSD169JBu3LghY8QVZ8+ePRKAIo+QkBBJksRQ+EmTJkmOjo6Sqamp1L59eyk5OVntGrdv35b69u0rValSRbK2tpZCQ0OlzMxMGe6m/D2rvHJycqQOHTpI1apVk4yNjSVXV1dp6NChRf4Doi/lVVw5AZCWLl2qOkeT373Lly9LnTt3lszNzSUHBwdp7NixUn5+fgXfTfl6XlmlpKRIbdq0kezt7SVTU1Opbt260rhx46T79++rXUcfymrQoEGSq6urZGJiIlWrVk1q3769KvmRJO38TikkSZLKp26JiIiISDuxDxARERHpHSZAREREpHeYABEREZHeYQJEREREeocJEBEREekdJkBERESkd5gAERERkd5hAkREVM4SEhKgUCiKrIVERPJhAkRERER6hwkQERER6R0mQERUZgICAjBq1Ch8+umnsLe3h5OTE6ZMmQIAuHz5MhQKhWq1bAC4d+8eFAoFEhISADxuKtqxYweaNm0Kc3NzvPHGG0hPT8f27dvh6ekJa2tr9OvXDzk5ORrFpFQqERUVhTp16sDc3Bze3t7YsGGD6njhZ27duhVNmjSBmZkZXnvtNZw6dUrtOr/++isaNmwIU1NTuLm54ZtvvlE7npubi88++wy1atWCqakp6tati59++kntnGPHjsHPzw8WFhZ4/fXXkZycrDr2999/o127drCysoK1tTV8fX1x9OhRje6RiEqPCRARlanly5fD0tIShw4dwqxZszBt2jTExcWV6hpTpkzB/PnzceDAAVy9ehW9e/fGvHnzsGbNGmzduhU7d+7E999/r9G1oqKisGLFCixatAinT5/GmDFj8N5772Hv3r1q540bNw7ffPMNjhw5gmrVquGtt95Cfn4+AJG49O7dG3369ME///yDKVOmYNKkSVi2bJnq/QMGDMDPP/+M7777DklJSfi///s/VKlSRe0zJk6ciG+++QZHjx6FkZERBg0apDrWv39/1KxZE0eOHMGxY8fw+eefw9jYuFTlRkSlUG7LrBKR3mnbtq3UqlUrtX3NmjWTPvvsM+nSpUsSAOmvv/5SHbt7964EQNqzZ48kSY9Xdd+1a5fqnKioKAmAdPHiRdW+Dz74QOrYseNz43n48KFkYWEhHThwQG3/4MGDpb59+6p95tq1a1XHb9++LZmbm0vr1q2TJEmS+vXrJwUFBaldY9y4cZKXl5ckSZKUnJwsAZDi4uKKjaO4+9q6dasEQHrw4IEkSZJkZWUlLVu27Ln3RERlgzVARFSmmjRpova6Ro0aSE9Pf+FrODo6wsLCAu7u7mr7NLnmhQsXkJOTg6CgIFSpUkX1WLFiBS5evKh2bosWLVTb9vb2qF+/PpKSkgAASUlJaNmypdr5LVu2xPnz51FQUIATJ07A0NAQbdu21fi+atSoAQCq+wgPD8eQIUMQGBiIGTNmFImPiMqWkdwBEJFuebrZRqFQQKlUwsBA/H9LkiTVscImpmddQ6FQlHjN58nKygIAbN26FS4uLmrHTE1Nn/t+TZmbm2t03tP3BUB1H1OmTEG/fv2wdetWbN++HREREVi7di169OhRZnES0WOsASKiClGtWjUAwI0bN1T7nuwQXR68vLxgamqKlJQU1K1bV+1Rq1YttXMPHjyo2r579y7OnTsHT09PAICnpyf+/PNPtfP//PNP1KtXD4aGhmjcuDGUSmWRfkWlVa9ePYwZMwY7d+5Ez549sXTp0pe6HhGVjDVARFQhzM3N8dprr2HGjBmoU6cO0tPT8cUXX5TrZ1pZWeGTTz7BmDFjoFQq0apVK9y/fx9//vknrK2tERISojp32rRpqFq1KhwdHTFx4kQ4ODige/fuAICxY8eiWbNm+PLLLxEcHIzExETMnz8fCxcuBAC4ubkhJCQEgwYNwnfffQdvb29cuXIF6enp6N2793PjfPDgAcaNG4devXqhTp06uHbtGo4cOYJ33nmnXMqFiJgAEVEFWrJkCQYPHgxfX1/Ur18fs2bNQocOHcr1M7/88ktUq1YNUVFR+Pfff2Fra4tXX30VEyZMUDtvxowZGD16NM6fPw8fHx/89ttvMDExAQC8+uqrWL9+PSZPnowvv/wSNWrUwLRp0zBw4EDV+6OjozFhwgSMGDECt2/fRu3atYt8RkkMDQ1x+/ZtDBgwAGlpaXBwcEDPnj0xderUMisHIlKnkJ5skCci0jMJCQlo164d7t69C1tbW7nDIaIKwj5AREREpHeYABFRpZWSkqI2vP3pR0pKitwhEpGWYhMYEVVajx49wuXLl0s87ubmBiMjdnUkoqKYABEREZHeYRMYERER6R0mQERERKR3mAARERGR3mECRERERHqHCRARERHpHSZAREREpHeYABEREZHeYQJEREREeuf/AUPyIYqelqa2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2848615667519758\n",
      "Corresponding R^2 SCore: 0.2876154151963989\n",
      "Corresponding num_epochs: 190\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnCklEQVR4nO3dd3xN9/8H8NfNkCFLmsggROwYQZCqEioiqK1mi1C+ilZFtUaLUI1dVcWX1mprtNQmxC5NjZCvWcReSRBZUln3/P74/HLT26x7uTfn3tzX8/HI4551z33fT26Sdz5TIUmSBCIiIiITZCZ3AERERERyYSJEREREJouJEBEREZksJkJERERkspgIERERkcliIkREREQmi4kQERERmSwmQkRERGSymAgRERGRyWIiREQ64e3tjbffflvuMIiItMJEiIioEG3atIFCoSjxa/r06Tp5vaVLl2LNmjUaX//vOBwcHBAYGIjdu3eX+Ny9e/fC0tISNjY2OH78eJHXHTx4EEOHDkWtWrVga2sLHx8fvP/++3j06JHGce7cuROBgYGoWLGi6h59+vRBZGSkxvcg0icF1xojIl3w9vZG/fr1sWvXLrlD0YmoqCgkJCSo9k+fPo3Fixdj8uTJqFu3rup4w4YN0bBhw1d+vfr168PFxQVHjhzR6HqFQoH27dtj0KBBkCQJd+7cwbJly/Do0SPs3bsXHTp0KPR5MTExaNOmDapWrYq///4bycnJOHHiBOrUqVPg2qZNmyIpKQnvvPMOatasiZs3b2LJkiWwtbVFbGws3N3di41x/vz5mDBhAgIDA9GtWzfY2toiLi4OBw4cgJ+fn1aJH5HeSEREOlC1alWpc+fOcoehN7/++qsEQDp8+LBe7l+vXj0pMDBQ4+sBSKNHj1Y7dvnyZQmA1LFjx0Kfc+vWLcnd3V2qX7++lJiYKN25c0fy8fGRvL29pfj4+ALXHz16VMrNzS1wDIA0ZcqUYuPLzs6WHBwcpPbt2xd6PiEhodjn61Jubq70999/l9rrkXFh0xiZnOnTp0OhUCAuLg5DhgyBk5MTHB0dERoaioyMDNV1t2/fhkKhKPS/1n83ieTd89q1a3j33Xfh6OgIV1dXfPHFF5AkCffu3UO3bt3g4OAAd3d3LFiw4KVi37t3L1q1aoXy5cvD3t4enTt3xqVLl9SuGTJkCOzs7HDz5k106NAB5cuXh6enJ2bMmAHpXxXAz58/x/jx4+Hl5QUrKyvUrl0b8+fPL3AdAPz0009o3rw5bG1tUaFCBbRu3Rr79+8vcN3x48fRvHlzWFtbw8fHB+vWrVM7n52djfDwcNSsWRPW1tZ47bXX8OabbyIqKqrI933mzBkoFAqsXbu2wLl9+/ZBoVCoaqLS0tLw8ccfw9vbG1ZWVqhYsSLat2+Ps2fPFl2wr0CT70l8fDxCQ0NRuXJlWFlZwcPDA926dcPt27cBiNq0S5cu4ejRo6qmrjZt2mgdS926deHi4oIbN24UOJeUlISOHTvC1dUVhw4dgqurK6pUqYIjR47AzMwMnTt3xvPnz9We07p1a5iZmRU45uzsjCtXrhQby5MnT5CamoqWLVsWer5ixYpq+y9evMD06dNRq1YtWFtbw8PDAz179lR7L5p+XhUKBcaMGYOff/4Z9erVg5WVlaop7sGDBxg6dCjc3NxgZWWFevXqYdWqVcW+FyrbmAiRyerTpw/S0tIQERGBPn36YM2aNQgPD3+le/bt2xdKpRKzZ89GQEAAvvzySyxatAjt27dHpUqVMGfOHNSoUQOffPIJjh07ptW9f/zxR3Tu3Bl2dnaYM2cOvvjiC1y+fBlvvvmm6g9qntzcXISEhMDNzQ1z586Fv78/pk2bhmnTpqmukSQJXbt2xddff42QkBAsXLgQtWvXxoQJExAWFqZ2v/DwcLz33nuwtLTEjBkzEB4eDi8vLxw6dEjturi4OPTu3Rvt27fHggULUKFCBQwZMkQtMZg+fTrCw8PRtm1bLFmyBFOmTEGVKlWKTVSaNm0KHx8f/PLLLwXObdq0CRUqVFA1BY0cORLLli1Dr169sHTpUnzyySewsbEp8Q/3y9D0e9KrVy9s3boVoaGhWLp0KT766COkpaXh7t27AIBFixahcuXKqFOnDn788Uf8+OOPmDJlitbxpKSk4NmzZ6hQoYLa8czMTHTr1g3lypVTJUF5vLy8cOTIESQnJ+Odd95BTk5Osa+Rnp6O9PR0uLi4FHtdxYoVYWNjg507dyIpKanYa3Nzc/H2228jPDwc/v7+WLBgAcaOHYuUlBRcvHgRgHafVwA4dOgQxo0bh759++Kbb76Bt7c3EhIS8Prrr+PAgQMYM2YMvvnmG9SoUQPDhg3DokWLio2RyjA5q6OI5DBt2jQJgDR06FC14z169JBee+011f6tW7ckANLq1asL3AOANG3atAL3HDFihOpYTk6OVLlyZUmhUEizZ89WHX/27JlkY2MjDR48WOOY09LSJCcnJ2n48OFqx+Pj4yVHR0e144MHD5YASB9++KHqmFKplDp37iyVK1dOevz4sSRJkrRt2zYJgPTll1+q3bN3796SQqGQ4uLiJEmSpOvXr0tmZmZSjx49CjSTKJVK1XbVqlUlANKxY8dUxxITEyUrKytp/PjxqmN+fn4v1YQ2adIkydLSUkpKSlIdy8zMlJycnNS+l46OjgWajHTh301jmn5Pnj17JgGQ5s2bV+z9X6ZpbNiwYdLjx4+lxMRE6cyZM1JISIhGr/UqZs6cKQGQDh48WOK1U6dOlQBI5cuXlzp27CjNmjVLiomJKXDdqlWrJADSwoULC5zL+4xp+nmVJFE2ZmZm0qVLl9SuHTZsmOTh4SE9efJE7Xi/fv0kR0dHKSMjo8T3RGUPa4TIZI0cOVJtv1WrVnj69ClSU1Nf+p7vv/++atvc3BxNmzaFJEkYNmyY6riTkxNq166NmzdvanzfqKgoJCcno3///njy5Inqy9zcHAEBATh8+HCB54wZM0a1nddUkJWVhQMHDgAA9uzZA3Nzc3z00Udqzxs/fjwkScLevXsBANu2bYNSqcTUqVMLNJMoFAq1fV9fX7Rq1Uq17+rqWuC9Ojk54dKlS7h+/brG7x8QtW3Z2dn47bffVMf279+P5ORk9O3bV+3+J0+exMOHD7W6v7Y0/Z7Y2NigXLlyOHLkCJ49e6bTGH744Qe4urqiYsWKaNq0KQ4ePIhPP/200BoSXTh27BjCw8PRp08fvPXWWyVeHx4ejvXr16Nx48bYt28fpkyZAn9/fzRp0kSthm7Lli1wcXHBhx9+WOAeeZ8xTT+veQIDA+Hr66valyQJW7ZsQZcuXSBJktr3rEOHDkhJSdFb8ykZNiZCZLKqVKmitp/XnPAqf6z+fU9HR0dYW1sXaEZwdHTU6nXykoa33noLrq6ual/79+9HYmKi2vVmZmbw8fFRO1arVi0AUDXZ3LlzB56enrC3t1e7Lm9E1J07dwAAN27cgJmZmdoflaL8+/0Dolz/+V5nzJiB5ORk1KpVCw0aNMCECRNw/vz5Eu/t5+eHOnXqYNOmTapjmzZtgouLi9of5blz5+LixYvw8vJC8+bNMX36dK2STk1p+j2xsrLCnDlzsHfvXri5uaF169aYO3cu4uPjXzmGbt26ISoqCrt371b1U8vIyCiQsOrCX3/9hR49eqB+/fr4/vvvNX5e//798fvvv+PZs2fYv38/BgwYgHPnzqFLly548eIFAPEZq127NiwsLIq8j6af1zzVqlVT23/8+DGSk5OxYsWKAt+v0NBQACjwc0SmoehPHVEZZ25uXuhx6f87Xv67tiNPbm6uVvcs6XU0oVQqAYg+KYUNWS7uD0hp0uS9tm7dGjdu3MD27duxf/9+fP/99/j666+xfPlytRq1wvTt2xezZs3CkydPYG9vjx07dqB///5q779Pnz5o1aoVtm7div3792PevHmYM2cOfvvtN3Ts2FE3bxTafU8+/vhjdOnSBdu2bcO+ffvwxRdfICIiAocOHULjxo1fOobKlSsjKCgIANCpUye4uLhgzJgxaNu2LXr27PnS9/23e/fuITg4GI6OjtizZ0+BZEQTDg4OaN++Pdq3bw9LS0usXbsWJ0+eRGBgoM7i/CcbGxu1/bzv17vvvovBgwcX+hxdTINAxscwfnsSGaC8GqLk5GS14//+z7M0VK9eHYDogJr3h684SqUSN2/eVNUCAcC1a9cAiBFKAFC1alUcOHAAaWlpan/Y/vrrL9X5vNdWKpW4fPkyGjVqpIu3A2dnZ4SGhiI0NBTp6elo3bo1pk+frlEiFB4eji1btsDNzQ2pqano169fges8PDwwatQojBo1ComJiWjSpAlmzZql00RI2+9J9erVMX78eIwfPx7Xr19Ho0aNsGDBAvz0008Aik68tfGf//wHX3/9NT7//HP06NFDJ/d8+vQpgoODkZmZiYMHD8LDw+OV79m0aVOsXbtWNTFj9erVcfLkSWRnZ8PS0rLQ52j6eS2Kq6sr7O3tkZubq9H3i0wHm8aIiuDg4AAXF5cCo7uWLl1a6rF06NABDg4O+Oqrr5CdnV3g/OPHjwscW7JkiWpbkiQsWbIElpaWaNeuHQBRg5Cbm6t2HQB8/fXXUCgUqqShe/fuMDMzw4wZM1T/Vf/zvtp6+vSp2r6dnR1q1KiBzMzMEp9bt25dNGjQAJs2bcKmTZvg4eGB1q1bq87n5uYiJSVF7TkVK1aEp6en2v2fPHmCv/76S226BG1p+j3JyMhQNQHlqV69Ouzt7dViKl++fIGkW1sWFhYYP348rly5gu3bt7/SvQAxXL1Tp0548OAB9uzZg5o1a2r83IyMDERHRxd6Lq8/T+3atQGIUXVPnjwp8FkE8j9jmn5ei2Jubo5evXphy5YtqpFo/1TYzxCZBtYIERXj/fffx+zZs/H++++jadOmOHbsmKpmpTQ5ODhg2bJleO+999CkSRP069cPrq6uuHv3Lnbv3o2WLVuq/YGwtrZGZGQkBg8ejICAAOzduxe7d+/G5MmTVUOnu3TpgrZt22LKlCm4ffs2/Pz8sH//fmzfvh0ff/yxqsajRo0amDJlCmbOnIlWrVqhZ8+esLKywunTp+Hp6YmIiAit3ouvry/atGkDf39/ODs748yZM9i8ebNa5+7i9O3bF1OnToW1tTWGDRum1h8mLS0NlStXRu/eveHn5wc7OzscOHAAp0+fVpu7acmSJQgPD8fhw4dfar4eQPPvybVr19CuXTv06dMHvr6+sLCwwNatW5GQkKBWm+Xv749ly5bhyy+/RI0aNVCxYkWNOiT/25AhQzB16lTMmTMH3bt3f6n3lmfgwIE4deoUhg4diitXrqh1cLazsyv2/hkZGXjjjTfw+uuvIyQkBF5eXkhOTsa2bdvw+++/o3v37qpmwUGDBmHdunUICwvDqVOn0KpVKzx//hwHDhzAqFGj0K1bN40/r8WZPXs2Dh8+jICAAAwfPhy+vr5ISkrC2bNnceDAgRKH+VMZJdNoNSLZ5A11zxtGnmf16tUSAOnWrVuqYxkZGdKwYcMkR0dHyd7eXurTp4+UmJhY5PD5f99z8ODBUvny5QvEEBgYKNWrV0/r2A8fPix16NBBcnR0lKytraXq1atLQ4YMkc6cOVPgNW/cuCEFBwdLtra2kpubmzRt2rQCw9/T0tKkcePGSZ6enpKlpaVUs2ZNad68eWrD4vOsWrVKaty4sWRlZSVVqFBBCgwMlKKiolTni5pZOjAwUG1Y+Jdffik1b95ccnJykmxsbKQ6depIs2bNkrKysjQqg+vXr0sAJADS8ePH1c5lZmZKEyZMkPz8/CR7e3upfPnykp+fn7R06VK16/K+X9rMEl3UzNIlfU+ePHkijR49WqpTp45Uvnx5ydHRUQoICJB++eUXtfvEx8dLnTt3luzt7SUAJQ6lRyEzS+eZPn26TmbBzpsSobCvqlWrFvvc7OxsaeXKlVL37t2lqlWrSlZWVpKtra3UuHFjad68eVJmZqba9RkZGdKUKVOkatWqSZaWlpK7u7vUu3dv6caNG6prNP28Flc2CQkJ0ujRoyUvLy/V67Rr105asWLFyxUSGT2uNUZUxgwZMgSbN29Genq63KEQERk89hEiIiIik8U+QkQye/z4cbFD8suVKwdnZ+dSjIiIyHQwESKSWbNmzYodkh8YGIgjR46UXkBERCaEfYSIZHbixAn8/fffRZ6vUKEC/P39SzEiIiLTwUSIiIiITBY7SxMREZHJYh+hEiiVSjx8+BD29vY6ma6eiIiI9E+SJKSlpcHT07PYhYiZCJXg4cOH8PLykjsMIiIiegn37t1D5cqVizzPRKgEeYv73bt3Dw4ODjq5Z3Z2Nvbv34/g4OAiFxgkgWWlHZaX5lhW2mF5aY5lpTl9llVqaiq8vLzUFuktDBOhEuQ1hzk4OOg0EbK1tYWDgwN/SErAstIOy0tzLCvtsLw0x7LSXGmUVUndWthZmoiIiEwWEyEiIiIyWUaXCH333Xfw9vaGtbU1AgICcOrUqSKv/e2339C0aVM4OTmhfPnyaNSoEX788cdSjJaIiIgMmVElQps2bUJYWBimTZuGs2fPws/PDx06dEBiYmKh1zs7O2PKlCmIjo7G+fPnERoaitDQUOzbt6+UIyciIiJDZFSJ0MKFCzF8+HCEhobC19cXy5cvh62tLVatWlXo9W3atEGPHj1Qt25dVK9eHWPHjkXDhg1x/PjxUo6ciIiIDJHRjBrLyspCTEwMJk2apDpmZmaGoKAgREdHl/h8SZJw6NAhXL16FXPmzCnyuszMTGRmZqr2U1NTAYie7dnZ2a/wDvLl3UdX9yvLWFbaYXlpjmWlHZaX5lhWmtNnWWl6T6NZa+zhw4eoVKkS/vjjD7Ro0UJ1/NNPP8XRo0dx8uTJQp+XkpKCSpUqITMzE+bm5li6dCmGDh1a5OtMnz4d4eHhBY6vX78etra2r/5GiIiISO8yMjIwYMAApKSkFDv9jdHUCL0se3t7xMbGIj09HQcPHkRYWBh8fHzQpk2bQq+fNGkSwsLCVPt5EzIFBwfrdB6hqKgotG/fnnNMlIBlpR2Wl+ZYVtpheWmOZaU5fZZVXotOSYwmEXJxcYG5uTkSEhLUjickJMDd3b3I55mZmaFGjRoAgEaNGuHKlSuIiIgoMhGysrKClZVVgeOWlpY6/ybp455lFctKOywvzbGstMPy0hzLSnP6+hurCaPpLF2uXDn4+/vj4MGDqmNKpRIHDx5UayoriVKpVOsDRERERKbLaGqEACAsLAyDBw9G06ZN0bx5cyxatAjPnz9HaGgoAGDQoEGoVKkSIiIiAAARERFo2rQpqlevjszMTOzZswc//vgjli1bJufbICIiIgNhVIlQ37598fjxY0ydOhXx8fFo1KgRIiMj4ebmBgC4e/cuzMzyK7meP3+OUaNG4f79+7CxsUGdOnXw008/oW/fvnK9BSIiIjIgRpUIAcCYMWMwZsyYQs8dOXJEbf/LL7/El19+WQpREREZttxcIC4OqFULKGENSiKTYjR9hIiI6OV9+y1Qpw7wz/lnd+0Cdu+WLyYiQ8BEiIiojLp0CRgyBLh/H4iKEsd++008JiQA3bsDXboAN2/KFSGR/JgIEREZOUkCMjLEdloa8Mcf4tgHHwBr1wJffw1cuSLOnzghmsmOHhWPkgSsXCnOPXgAfPIJcP26PO+DSA5G10eIiMhUXb8O7NkDJCUBt28D9+4B5uYiyXnwAGjcGLh1C0hOBrp2BX7/XTzv+HFxPQCkpIiaon92qfzhByA8HPjqK2DpUuDXX4GTJ4FipmgjKjOYCBERGbjoaGDePGDbNlGDU5Rz5/K3d+zI3z51Sv2648fzEyGFAnj8GNi6Fdi3Txy7exfo2VNcZ8Z2AyrjmAgRERmYK1eADRuAwEBRQ5PXrwcAgoKAGjWASpWAatVEYlSpkjh2+DDg7CySoLzmrsJs3ixeQ6EARo8GliwBpk0DbtwALCwAS0uRfF25AtSrp//3SyQnJkJERAbg0CGR/EyeLDoxX7sGzJwpzllYAO+9J/rv+PoWfY9Bg8Rj27aig3TFiqIZ7MwZcbxWLXHfw4fFvp8fMGGCSLauXhXH3nhDNLcdPiz6GjERorKOlZ5ERDK4dAl48kRsX70q+vR8/73o53PtGmBvD9jYAPXri6atVauKT4L+qXx50ZdozRrA3z//+JAhosYoT1AQUKUK0Llz/rHgYKBlS7F94oSYe2jKFKB7d3PExTm9wjsmMkysESIi0sDvv4sOxB9/LGpoSrJuHfDLLyKBqVgRyMnJf97Zs0Dz5kD16uKeffoAz5+Lcykp4nHePCA0VDznVfrpNG6cv920KXDxoqh9evgQeP99cXzkSGDnTrEdHJyfoB08KJrZnj0DADM8flwLH3308rEQGSImQkRERfjzT+B//wP69gW6dRMJga0tMGqUqHH59FOgXz/RZDVrlmhOys0FVqwAxowRQ9kXLAAyM4FvvgHs7ETSkZYmrrt2TSQnN26IZOm770Tzlq8vMGyYZglXSZo0yd/29QU8PICBA9Wv6dBB1ApJkrg+NVUcv39fPLq5iXmHLlxwRVaWBEtLYPVqIDFRlAFnqiZjxkSIiExaTo4YHfXkCdCihRh5lZMj+sa0bw+kpwOLF+fVioiE584dYO5csT91quhLs3dv/j3ffFMkO4Dof5NX25OeDsyfD1hb519744ZIJH76SbxemzaiaUsXSRAg+gE1agQ4OACenoVfY24uZpnOU6GCeP+XLon9JUuA0aMlJCZaIDo6B9WqiURNkoBWrUS/IiJjxUSIiExOaqpIRi5dcsbIkRaIjy94jZWVqMkBgMuX8489fJifBPn4iFmZ85KgGTPEXDx5NSqWliL5AUStkp2dmLPnxQvA21vUAp06BUyaJJIgAHBx0e17LVdOfVi9pt54QyRC9eqJofRbt0pYv16BqCgFMjPzh/Fv3MhEiIwbO0sTUZn24oWYgBAQ8+O8+y7g6AgEBFjgyy9fR3y8Aq+9BjRoIK7x8hK1J5mZ4ro+fcTxWrVEzQggOjJv2iQ6Ezs4iGMffAB88YWYmDDvPl99JbatrIA5c8RXhQri2NChol/O9u35o8MMydixQLt2opnPzAwIClICALZvN1Nbr+yXX0QNGpGxYo0QEZVZFy6Ivi+JicBHH4m5dZKTxbnYWAUASwQGKrF3rxlsbETtja2tSJgWLQJ69xadmtu2FU1WtWsDVauKxypVxH02bxZ9gz7/XOyPHy9qdZo2BWrWFDVGrVuL5wEicdi0ScTj6ChGixmievWAAwfy99u3F1VAV6+KDkF16ohyTUgQy3W0aydHlESvjokQEZUZSqXo4BwXJx5//jm/mWrePPHYvDkwezawd28uYmPvYePGSrCxEZXjdnbiGm9vkQjlGTkyfzuvCeuf+/88Zm4uanvyLF2qfn1QkPgyNm5uwMCBV/DXX7WRk2OG2bPFbNQrVoj5j5gIkbFiIkRERu3mTTEcPDtbLDB68qT6+VatgI4dRbNUnz7At9+Kpqo331Riz57/wd6+kjyBG6F33rmGTp1qwNJSJI5WViIR2rJFjKQbM0aMIuveXd44ibTBRIiIjMqVK2LoekyM+Lp2Tf18+fJi9FedOmJYeEiIGIE1cSKHeeta69ZiOP6jR6KcHz8WcxO1aQM4OckdHZFmmAgRkVHIyAAWLhSdkf/ZOdfcXMyE7OQk+uFMmiT+OP8bkyDdMzcXtWzffCOSIAB4+lRMMZDXFElk6JgIEZFBunpVNHUdPCgSn7/+EskQIPqjtG0rOiQ3a6a+bASVrn79RCIEAAEBomnym2+AsLDCE1IiQ8NEiIgMSkaG6M/z1VcFh2V7e4saoffeYw2PoQgIEF/37gHbtolRcKdPi+0PPhAd2C9dEk2VlpZyR0tUEBMhIjIImZliNNfixfnz/oSEiOUgnJ1F7UKjRkyADI1CIWbmzskRk1S+845IhH77TQzBHzsWiI0VI+l++EHuaIkKYiJERLK7elX8Ab1wQex7e+eP8iLDZ2GRvyRIjx5i5NjhwyJBevFCHF+3TtTmVa4sX5xEheHM0kQkq5gYsTbXhQuAq6uYkyYujkmQsapRQ8zSnZsrkqCgILEER05O/szcRIaEiRARyebIEdHp+ckT0fH5wgXR+dbcXO7I6FX06iUeK1USie2nn4r9FSuA27dFM2jeGmxEcmPTGBHJYudO0RyWmSmSoe3bxRpeZPzCwkQNUP/+YrmRt98G6tYVc0A1bizOmZmJ2b/r1hXPSU0FsrLENAgWFsCdO2KkYPv24loifeHHi4hK3c8/i74kmZlilNGePUyCyhJ7e7GQrK+v2Dc3B/btA5o0EWu9paeLxOc//xGjyg4eBF57TTSNurkB06YBfn6is3yzZsC5c7K+HSrjmAgRUalav14Mf8/NFY9btojRRlS2eXmJztPffy9q/8qXB37/XYwU/M9/8qdKSEoCZswAUlLE/tmzYobwjRsBHx+xXMqxY/K9Dyp7mAgRUanZtQsYNAiQJPHHb82a/NFGVPbZ2ADDholawJkzxbEpU4AbN0R/oqdPxWhBOzvRV+zuXVEz9PixaGa7dQuIjAQCA0W/o969gWrVgKNH5X1fZNyYCBFRqbh4Ufwxy80VydDSpez7YcrGjhWTZuYlwt98I+aL+vRTURu0YYOoRdq8GXBwENd07iwSKUAs4bFli+h8HRIC7N4ty9ugMoD/ixGR3qWlAd26ib4hbduK5hEmQabNzEysC9etm1i0tV079XN5atQQTWinTgGDB4vZqVu2FKvd168v+hXt3Ss63h89KvoUEWmDv4qISO8mTwZu3hSLov76K5daoHy+vupJUGEaNhSr2ud9bkJDRV+iU6eAHTuATp2Av/8WNUNdu4pZrYk0xUSIiPTqjz+A774T299/L0YHEb0qGxuxvIeFhehI3bChSI527hTzGC1cKHeEZCyYCBGR3mRmiv/kJUn8Fx8UJHdEVBbZ24tV73fuBEaOFMfGjxedqbdvF8t9SJK8MZLhYiJERHrz1VdiEj03N2D+fLmjobLM2lpM3Lh0KTB3rpi7aMsWoHt34K23gNWr5Y6QDBUTISLSi7g4ICJCbH/7rRgRRKRvCgUwYQJw5ozoO5Q3qeNnn4mmM6J/YyJERHrxxRdAdraYDK93b7mjIVPTqJEYUh8bC9SrJ9aza9lSTOFw4kT+dYmJoqM1mS4mQkSkczExogOrQiEmyFMo5I6ITJWlpVj1XqEQa5dt3Ai8+aaYoXrCBMDDI3/pDzJNnEeIiHRu0iTxOGCAmBmYSE5t2oiaoevXxZpnq1eLGaojI8X5v/4SkzW6uIhao08/lTNaKm2sESIinTpwAIiKEv+J5y2jQCS3hg3FsPoVK0TiM2gQUKuW6MdmYyOmedixQ/Ql2rBB7mipNLFGiIh0RpKAiRPF9gcfiHWgiAxN9erA2rX5+w0a5M92/ttvYsqHDRtEjWa/fvLFSaWDiRAR6czu3aJ/kJ2dWEyTyBh07iy+cnNF5/6DB8WcRDt3AkqlSIio7GLTGBHphCQBs2aJ7VGjgIoV5Y2HSFvm5mLdst27gaFDxbHQUGDRIiArS9bQSI+YCBGRThw9Cvz5J2BlBYwbJ3c0RC/H0lLMP7RypZj2IStLfJ4DAsRQeyp7mAgRkU7MnSsehw4F3N3ljYXoVZmZiaH2K1eK0WSxsUDbtiLh53IdZQsTISJ6ZdeuiSYFhQIIC5M7GiLdMDcXHadPnAAqVQIuXxZD8QMDgZQUuaMjXWEiRESvbMkS8di5M1CjhryxEOlarVpieP377wO2tsDvvwPt2gFTp4oFXcm4MREioleSmpq/oOXYsfLGQqQvVaqIZrI//hDr5sXEiHmyOnUSfYcePBDLeJDxYSJERK9k9WogPR2oW1f8l0xUlvn5iWRo/HhR+/niBTB6tNgOCGD/IWPERIiIXppSKVaWB4CPPuKaYmQaatcG5s8HvvpK7G/eLBKimzdF7VBmJpCbyx8GY8FEiIhe2t69wI0bgJMT8N57ckdDVLp69Cg4e/qlS0D9+hb45JPWrB0yEkyEiOilffedeHz/faB8eXljISptFhZissVGjcT8WQCwdStw544Ct245ISFBzuhIU0yEiOil3LmTv3r3yJHyxkIkl65dgXPngMGDxf62bfnnrl1j85gxMLpE6LvvvoO3tzesra0REBCAU6dOFXntypUr0apVK1SoUAEVKlRAUFBQsdcTkeZ++EF0DA0KEotYEpmyvGkj7t/PP3b9ujyxkHaMKhHatGkTwsLCMG3aNJw9exZ+fn7o0KEDEouY9/zIkSPo378/Dh8+jOjoaHh5eSE4OBgPHjwo5ciJypacHJEIAcCIEfLGQmQICvtnIK9GaPt2wM1NLOZKhseoEqGFCxdi+PDhCA0Nha+vL5YvXw5bW1usWrWq0Ot//vlnjBo1Co0aNUKdOnXw/fffQ6lU4iA/jUSvZPdu4OFDwNUV6NZN7miI5FfYRKJXr4pEaOZMMZosb74tMiwWcgegqaysLMTExGDSpEmqY2ZmZggKCkJ0dLRG98jIyEB2djacnZ2LvCYzMxOZmZmq/dTUVABAdnY2srOzXzJ6dXn30dX9yjKWlXZKq7z++19zAGYYNCgXCoUSxvjt4WdLOyyv4nl5AYCl2rHr14GLF7MREyOOnzwpITs7p/SDM2D6/Fxpek+jSYSePHmC3NxcuLm5qR13c3PDX3/9pdE9PvvsM3h6eiIoKKjIayIiIhAeHl7g+P79+2Fra6td0CWIiorS6f3KMpaVdvRZXo8f2yAysj0AoHr1w9iz57neXqs08LOlHZZX0SpU6IBnz6xhbZ2DFy8scPMmMH36LQC1AABxcQps3BgFBwcmk/+mj89VRkaGRtcZTSL0qmbPno2NGzfiyJEjsLa2LvK6SZMmIewfq0ampqaq+hY5ODjoJJbs7GxERUWhffv2sLS0LPkJJoxlpZ3SKK/wcDNIkgJt2yrx/vuBenmN0sDPlnZYXiXz9TXHiRNA27YKHDqUg8xMC2zZUkvtGienYISEcIKhPPr8XOW16JTEaBIhFxcXmJubI+FfEzMkJCTA3d292OfOnz8fs2fPxoEDB9CwYcNir7WysoJV3oQQ/2Bpaanzb5I+7llWsay0o6/yys0F1q4V2yNGmMHS0qi6GRaKny3tsLyKVqeOWKnezw+4cuU5bt92BAA4OIjlZ7ZuBc6etUCXLkBWlpiJnUUp6OtvrCaM5rdYuXLl4O/vr9bROa/jc4sWLYp83ty5czFz5kxERkaiadOmpREqUZm1b58YHuzsLGbVJaJ8n34KfPwx8MEHSjg7v1AdX78eeOstsX3ypFiOo2FDoH59sU3yMppECADCwsKwcuVKrF27FleuXMEHH3yA58+fIzQ0FAAwaNAgtc7Uc+bMwRdffIFVq1bB29sb8fHxiI+PR3p6ulxvgcioff+9eBw0KH8mXSISatUCvv4a8PAAOna8hWrVJGzcCHTuLBZkBUQitGULcPUqcO0asGOHvDGTETWNAUDfvn3x+PFjTJ06FfHx8WjUqBEiIyNVHajv3r0LM7P83G7ZsmXIyspC79691e4zbdo0TJ8+vTRDJzJ68fHAzp1i+/335Y2FyNA1a5aAadNyVM0zfn6Aiwvw5IlYrT7P2rVAnz4yBUkAjCwRAoAxY8ZgzJgxhZ47cuSI2v7t27f1HxCRiVi7Vkyk2KIFUK+e3NEQGZdy5YDPPxdNZykp+cf37QMePRITLt64IeYjUnBljlJlVE1jRCQPScpvFmNtENHLGTkS8PYW24GB4p+K3FzRh2jFCtG0VsT/+aRHTISIqERHjwJxcYC9PavxiV6WlRWwciXQoAEwYwYwYIA4vn27SIYAYOlSYNMm+WI0RUyEiKhEebVB/fsDdnbyxkJkzIKCgPPngdatgbffFsf++EN85RkxAnhu3POUGhUmQkRUrGfPgM2bxTabxYh0x9tb9LfLzRVftWoBlSoBqalidBmVDiZCRFSsn34CMjPFqBdOxUWkW3m1QnnbrVqJ7RMnxKNSCdy8KfrpkX4wESKiIkmS6NMAiNogjmYh0q3OnfO3O3UCWrYU28ePA2fPAm+8AVSvLlawJ/1gIkRERTp7FrhwAbC2BgYOlDsaorKnRQtR2+rrC7z5Zn4i9McfQJs2+U1kS5YAeligncBEiIiK8eOP4rFbN6BCBXljISqLLCzy/+GwshIjyuzsgPR0IC0NaNJEzDH0+DEQGSl3tGUTEyEiKlRODrBhg9h+7z15YyEqy8zMxBcgEqPXX88/t2QJ8O67YnvNGvGYmAjcu1eqIZZpTISIqFAHDohfuC4uQHCw3NEQmY68n7d+/UTT2eDBYn/bNqBLF8DLS6x0f+2aOC5JwO+/A3//LUu4Ro+JEBEV6qefxGO/fsD/L5dERKVg7FgxyeLq1WK/QQMxWEGpBHbtArKygIwMsdo9AMyfL+Yl4vQWL4eJEBEVkJ4ObN0qtvOq5YmodJQrB3TtKgYp5Fm5Ejh2TEy2uHQpYG4ukqW1a/NHlG3YAFy8KE/MxszoFl0lIv3btk38x1mzJtC8udzREBEg5hjKm2fo0iXgu++AIUPyz0uSSIq4RId2WCNERAXkNYu9+y7nDiIyRHPmqA9i+OYb8fjrr8CdO/LEZKyYCBGRmvh4ICpKbHPuICLDVL48sG6dmHgxKgr46CNRWyRJwN69ckdnXJgIEZGajRtFp8wWLcSMtkRkuFq2FAu5AkBIiHjMm2/o8mXgyy+B69flic1YMBEiIjV5kyiykzSRcclLhA4eBP7zH7Gg6xdfiJGfeWuVZWaKhZQpHxMhIlK5fFnMcmthAfTpI3c0RKSNRo0AV1cx6nPFCtG/L2/m6kOHxACIN94Q8xCdPy93tIaDiRARqfz8s3js2FFMpEhExsPMDOjQIX//m2+ADz4Q2zNnAiNHiqTo+XNg+HAgN1eeOA0NEyEiAiCqzvOW1GAnaSLjlPez++67wJgxQFiYmHPo6FHR7K1QiI7Wp06J+YiIiRAR/b/Tp4Fbt8QvybffljsaInoZISFi5Oe6dSLp8fYGFiwQK9xXqiS2584V1375JZflAJgIEdH/27hRPHbtKpIhIjJObm7q83+NHQvExgL37wPjxolmMW9vsZbg99/LFaXhYCJERFAq82ej7d9f3liISL8sLfPXKZs4USzl0by5mEg1b3SZKWEiRET4/Xfg4UPAyYkrzROZgtBQ0VSWkSGG1J8+LWaqHjNG/GNkSpgIEZGqWaxnT8DKSt5YiEj/rK3FfEM//iiazaZOFc1pS5cCgYHAgQNyR1h6uOgqkYnLzgY2bxbb/frJGwsRlZ7atcUXIDpT+/oCgweLZTvatxedqidMkDfG0sAaISITd+gQ8OSJmIitbVu5oyEiufTtC8TFic7UgOhH9NZbYhRp1aoiaerVC7h2Td44dY01QkQmLq9Z7J13xCy0RGS6KlcWs1JXrgxMmwYcPqx+/to1sajrggVigsZ/jk4zVvy1R2TCMjOB334T22wWI6I8U6cCXbqI/kMZGUDDhqIZ/auvRN+iUaOAnTuBhQuBOnWArCxg9WogLQ1o1kz0M8pz86aocba3l+3tFIuJEJEJi4wEUlPF6JGWLeWOhogMSePG4uuf2rQBFi8Ww+737gX27RO1yY8eAceO5V83b56Y1frLL0XNUqVK4voGDUr1LWiEfYSITFhes1jfvmKdIiKi4piZAR9/DJw7JyZfzZuD7NgxUePTpYu47rPPgCZNRBIEAA8eAG++KZ5naPirj8hEPX8O7NghttksRkTaqFsX2L5dJDbDh4tO1X/8IY6FhooE6X//E8P0lywRSVBqquhs/ewZ8OIFEBUlms3kxqYxIhO1Z49o+/fxAZo2lTsaIjJGjRqJztX/tHQp4OEBODuL4fguLsCAAYC/v1jP0NsbyMkRv39sbS0wYkRldOokR/QCEyEiE5XXSbp377Ix8oOIDIO1NTBrlvqxChXEfGXt2wNJSeKYnR2Qnq7AokX+cHDIRXh46ccKsGmMyCS9eAHs2iW2e/aUNxYiMg1NmgB37gAXL4qvZ8+AKVNyUa5cDjp1km9dD9YIEZmgqCggPV3MFdKsmdzREJGpsLMD6tXL3582TYnq1Q+hSZO3ZIuJNUJEJiivWaxHD44WIyJ5OTu/kPX1+SuQyMRkZ4uRHYAYwUFEZMqYCBGZmKNHRdu8q6sY0kpEZMqYCBGZmC1bxGP37oC5uayhEBHJjokQkQnJzQW2bhXbbBYjImIiRGRSoqOBhATA0RFo21buaIiI5MdEiMiE5DWLde0KlCsnbyxERIaAiRCRiZCk/GHznESRiEhgIkRkImJigLt3AVtboEMHuaMhIjIMTISITERes1inToCNjbyxEBEZCiZCRCaAzWJERIVjIkRkAi5fBq5dEx2kO3eWOxoiIsPBRIjIBOTVBgUHAw4O8sZCRGRImAgRmQA2ixERFY6JEFEZd/MmEBsrltPo0kXuaIiIDIvRJULfffcdvL29YW1tjYCAAJw6darIay9duoRevXrB29sbCoUCixYtKr1AiQxEXm1QYCDg4iJvLEREhsaoEqFNmzYhLCwM06ZNw9mzZ+Hn54cOHTogMTGx0OszMjLg4+OD2bNnw93dvZSjJTIMeYkQ1xYjIirIqBKhhQsXYvjw4QgNDYWvry+WL18OW1tbrFq1qtDrmzVrhnnz5qFfv36wsrIq5WiJ5PfwoVhfDBCrzRMRkTqLl31iXFwcbty4gdatW8PGxgaSJEGhUOgyNjVZWVmIiYnBpEmTVMfMzMwQFBSE6Lzf9DqQmZmJzMxM1X5qaioAIDs7G9nZ2Tp5jbz76Op+ZRnLSjv/Lq8tW8wAmOP115Vwdc0FizEfP1vaYXlpjmWlOX2Wlab31DoRevr0Kfr27YtDhw5BoVDg+vXr8PHxwbBhw1ChQgUsWLBA62A18eTJE+Tm5sLNzU3tuJubG/766y+dvU5ERATCw8MLHN+/fz9sbW119joAEBUVpdP7lWUsK+3kldfKlW8AcEWdOpexZ88NeYMyUPxsaYflpTmWleb0UVYZGRkaXad1IjRu3DhYWFjg7t27qFu3rup43759ERYWprdEqLRMmjQJYWFhqv3U1FR4eXkhODgYDjqagCU7OxtRUVFo3749LC0tdXLPsoplpZ1/lldKiiUuXxY/4hMn1oaPT22ZozMs/Gxph+WlOZaV5vRZVnktOiXROhHav38/9u3bh8qVK6sdr1mzJu7cuaPt7TTm4uICc3NzJCQkqB1PSEjQaUdoKyurQvsTWVpa6vybpI97llUsK+1YWloiMtISublAo0ZA7dosu6Lws6UdlpfmWFaa09ffWE1o3Vn6+fPnhTYRJSUl6bVDcrly5eDv74+DBw+qjimVShw8eBAtWrTQ2+sSGStOokhEVDKtE6FWrVph3bp1qn2FQgGlUom5c+eibdu2Og3u38LCwrBy5UqsXbsWV65cwQcffIDnz58jNDQUADBo0CC1ztRZWVmIjY1FbGwssrKy8ODBA8TGxiIuLk6vcRLJLS0N2L9fbDMRIiIqmtZNY3PnzkW7du1w5swZZGVl4dNPP8WlS5eQlJSEEydO6CNGlb59++Lx48eYOnUq4uPj0ahRI0RGRqo6UN+9exdmZvm53cOHD9G4cWPV/vz58zF//nwEBgbiyJEjeo2VSE579iiQlQXUqgX4+sodDRGR4dI6Eapfvz6uXbuGJUuWwN7eHunp6ejZsydGjx4NDw8PfcSoZsyYMRgzZkyh5/6d3Hh7e0OSJL3HRGRotm0T/xD06gXocVYLIiKjp1UilJ2djZCQECxfvhxTpkzRV0xE9AoyM80QGSmyHzaLEREVT6s+QpaWljh//ry+YiEiHfjf/yri+XMFvLwAf3+5oyEiMmxad5Z+99138cMPP+gjFiLSgeho0UTdsyebxYiISqJ1H6GcnBysWrUKBw4cgL+/P8qXL692fuHChToLjoi0k5UFnDwpEiEuskpEVDKtE6GLFy+iSZMmAIBr166pndPnWmNEVLKoKAUyMizg4SGhZUv+PBIRlUTrROjw4cP6iIOIdGDz5rzRYkqYmZnLHA0RkeHTuo/QP92/fx/379/XVSxE9ApevAB27BC1QO+8w2kjiIg0oXUipFQqMWPGDDg6OqJq1aqoWrUqnJycMHPmTCiVSn3ESEQa2LcPSEtT4LXX/kZAABMhIiJNaN00NmXKFPzwww+YPXs2WrZsCQA4fvw4pk+fjhcvXmDWrFk6D5KISrZpk3hs2fIBzMyqyhsMEZGR0DoRWrt2Lb7//nt07dpVdaxhw4aoVKkSRo0axUSISAZ//w3s2CG233zzIQAmQkREmtC6aSwpKQl16tQpcLxOnTpISkrSSVBEpJ09e4Dnz4GqVSXUrPlM7nCIiIyG1omQn58flixZUuD4kiVL4Ofnp5OgiEg7ec1ivXopOYkiEZEWXmr1+c6dO+PAgQNo0aIFACA6Ohr37t3Dnj17dB4gERXv2bP8ZrE+fZSIj5c3HiIiY6J1jVBgYCCuXbuGHj16IDk5GcnJyejZsyeuXr2KVq1a6SNGIirGpk1AZiZQvz7QuLHc0RARGReta4QAwNPTk52iiQzE6tXiMTSUa4sREWlL4xqh69evo3///khNTS1wLiUlBQMGDMDNmzd1GhwRFe/yZeDUKcDCAnj3XbmjISIyPhonQvPmzYOXlxccHBwKnHN0dISXlxfmzZun0+CIqHh5tUGdOwMVK8obCxGRMdI4ETp69CjeeeedIs/36dMHhw4d0klQRFSy7Gzgxx/FdmiovLEQERkrjROhu3fvomIx/3K6uLjg3r17OgmKiEoWGQkkJIiaoE6d5I6GiMg4aZwIOTo64saNG0Wej4uLK7TZjIj0I69Z7N13AUtLeWMhIjJWGidCrVu3xrffflvk+cWLF3P4PFEpuXcvf+4gNosREb08jROhSZMmYe/evejduzdOnTqFlJQUpKSk4OTJk+jVqxf27duHSZMm6TNWIvp/y5cDublAmzZi/iAiIno5Gs8j1LhxY2zevBlDhw7F1q1b1c699tpr+OWXX9CkSROdB0hE6l68AFasENsffihvLERExk6rCRXffvtt3LlzB5GRkYiLi4MkSahVqxaCg4Nha2urrxiJ6B/WrweePAG8vICuXeWOhojIuGk9s7SNjQ169Oihj1iIqARKJTB3rtj+6CMxkSIREb08jfsIRUdHY9euXWrH1q1bh2rVqqFixYoYMWIEMjMzdR4gEeXbsQO4ehVwdARGjJA7GiIi46dxIjRjxgxcunRJtX/hwgUMGzYMQUFBmDhxInbu3ImIiAi9BElEgCQBeT9io0YBnK2CiOjVaZwIxcbGol27dqr9jRs3IiAgACtXrkRYWBgWL16MX375RS9BEhGwc6dYV8zWFhg7Vu5oiIjKBo0ToWfPnsHNzU21f/ToUXTs2FG136xZM84sTaQnubnAlClie+xY4B8/ikRE9Ao0ToTc3Nxw69YtAEBWVhbOnj2L119/XXU+LS0NlpzelkgvNm4ELl4EnJyACRPkjoaIqOzQOBHq1KkTJk6ciN9//x2TJk2Cra2t2kzS58+fR/Xq1fUSJJEpy84Gpk4V259+ClSoIG88RERlicaDb2fOnImePXsiMDAQdnZ2WLt2LcqVK6c6v2rVKgQHB+slSCJT9sMPwM2bojnso4/kjoaIqGzROBFycXHBsWPHkJKSAjs7O5ibm6ud//XXX2FnZ6fzAIlMWXIyMH262P78c6B8eTmjISIqe7Sejs3R0bHQ487Ozq8cDBGpmzwZSEgA6tThvEFERPqgcR8hIipdp06JxVUB8fiPlmgiItIRJkJEBignBxg5UkyiOHgwEBgod0RERGUTEyEiA/Tdd8C5c2KE2Lx5ckdDRFR2MREiMjDXrom+QQAwZw7g6ipvPEREZZnWidD9+/eRnp5e4Hh2djaOHTumk6CITFV2NvDuu0BGBvDWW8CwYXJHRERUtmmcCD169AjNmzdH1apV4eTkhEGDBqklRElJSWjbtq1egiQyFVOmAKdPixmk164FzFhnS0SkVxr/mp04cSLMzMxw8uRJREZG4vLly2jbti2ePXumukaSJL0ESWQKtm3L7w/0/fdA5cqyhkNEZBI0ToQOHDiAxYsXo2nTpggKCsKJEyfg4eGBt956C0lJSQAAhUKht0CJyrLYWOC998T2uHFAr16yhkNEZDI0ToRSUlJQ4R+LHFlZWeG3336Dt7c32rZti8TERL0ESFTW3b0LdO4MpKeLfkFz5sgdERGR6dA4EfLx8cH58+fVjllYWODXX3+Fj48P3n77bZ0HR1TWPXggkp+HDwFfX2DLFsDSUu6oiIhMh8aJUMeOHbFixYoCx/OSoUaNGukyLqIy7+ZNoE0b4MYNoFo1IDJSdJImIqLSo/FaY7NmzUJGRkbhN7GwwJYtW/DgwQOdBUZUlp04IfoBJSQAVasChw4BXl5yR0VEZHo0rhGysLCAg4NDseerVq2qk6CIyqrcXODLL4HWrUUS5OcH/PEH4O0td2RERKZJ61lKnjx5oo84iMq8mzeB9u2BL74AlEoxceLvvwOennJHRkRkurRKhG7fvo2WLVvqKxaiMik9XUyUWLcucPgwUL68mCzxxx8Be3u5oyMiMm0a9xG6ePEiQkJCMGrUKH3GQ1RmpKUBP/wgJkl8+FAca99eLKhas6a8sRERkaBRIvTHH3/g7bffxsiRIzE5bzVIIirU/fvA4sXAihVASoo45uMDLFwIdO0KcN5RIiLDoVEiFBwcjGHDhuGrr77SdzxERis2FliwANi4EcjJEcdq1wbCwoBBgwBra1nDIyKiQmjUR6h8+fJ49OiRQawl9t1338Hb2xvW1tYICAjAqVOnir3+119/RZ06dWBtbY0GDRpgz549pRQpmYpz54AuXYDGjYGffhJJUJs2wM6dwOXLwIgRTIKIiAyVRonQiRMncObMGQwdOlTf8RRr06ZNCAsLw7Rp03D27Fn4+fmhQ4cORS7v8ccff6B///4YNmwYzp07h+7du6N79+64ePFiKUdOZdH162IuoCZNgF27xErx/foBZ86ITtFvv83V44mIDJ1Gv6Zr1KiB48ePIyYmBqNHj9Z3TEVauHAhhg8fjtDQUPj6+mL58uWwtbXFqlWrCr3+m2++QUhICCZMmIC6deti5syZaNKkCZYsWVLKkVNZ8vw5MHkyUL8+8Ntvos/PgAHAlSvAhg2Av7/cERIRkaY0HjXm6emJo0ePyramWFZWFmJiYjBp0iTVMTMzMwQFBSE6OrrQ50RHRyMsLEztWIcOHbBt27YiXyczMxOZmZmq/dTUVABAdnY2srOzX+Ed5Mu7j67uV5YZWlnt3KnA2LHmuH9f9Hju0EGJ2bNzUa+eOC93mIZWXoaMZaUdlpfmWFaa02dZaXpPjRMhAKhQoQIOHDjwUgG9qidPniA3Nxdubm5qx93c3PDXX38V+pz4+PhCr4+Pjy/ydSIiIhAeHl7g+P79+2Fra/sSkRctKipKp/cry+Quq5SUcvj++wb4/ffKAICKFZ9j2LCLaN48HnfuAHfuyBpeAXKXlzFhWWmH5aU5lpXm9FFWRS0L9m9aJUIAYGNjo3UwxmTSpElqtUipqanw8vJCcHBwsUuMaCM7OxtRUVFo3749LLnUeLEMoax27FBg/HhzPHmigJmZhHHjlJg6tRxsbJrIEk9xDKG8jAXLSjssL82xrDSnz7LKa9EpidaJUFEePXqEWbNm6a3/jYuLC8zNzZGQkKB2PCEhAe7u7oU+x93dXavrAcDKygpWVlYFjltaWur8m6SPe5ZVcpTVixfAp58C334r9hs0AFatUqBpU3MA5qUai7b42dIcy0o7LC/Nsaw0p6+/sZrQakzLpUuXsGTJEqxYsQLJyckARJPVuHHj4OPjg8OHD2sdqKbKlSsHf39/HDx4UHVMqVTi4MGDaNGiRaHPadGihdr1gKh+K+p6ojxxcUCLFvlJ0CefiNFgTZvKGxcREemWxjVCO3bsQO/evZHz/zPFzZ07FytXrkSfPn3g7++PrVu3IiQkRG+BAkBYWBgGDx6Mpk2bonnz5li0aBGeP3+O0NBQAMCgQYNQqVIlREREAADGjh2LwMBALFiwAJ07d8bGjRtx5swZrFixQq9xknGLigL69AGSkwEXF7EuWKdOckdFRET6oHGN0JdffonRo0cjNTUVCxcuxM2bN/HRRx9hz549iIyM1HsSBAB9+/bF/PnzMXXqVDRq1AixsbGIjIxUdYi+e/cuHj16pLr+jTfewPr167FixQr4+flh8+bN2LZtG+rXr6/3WMn4SBKwaBEQEiKSoIAAMVs0kyAiorJL4xqhq1evYv369bCzs8OHH36ITz75BF9//TWaNWumz/gKGDNmDMaMGVPouSNHjhQ49s477+Cdd97Rc1Rk7DIzgZEjgTVrxP6QIcCyZZwRmoiorNM4EUpLS1ONmjI3N4eNjQ18fHz0FhhRaXnyRCyGGh0tZoJesAAYO5aLoxIRmQKtRo3t27cPjo6OAPI7Kv97uYquXbvqLjoiPbt5UzSFXb8OODkBmzYBwcFyR0VERKVFq0Ro8ODBavv/+c9/1PYVCgVyc3NfPSqiUhATI/r/JCYCVasCe/cCdevKHRUREZUmjRMhpVKpzziISlVkJNC7t1g3zM8P2LMH8PSUOyoiIiptXBubTM6aNUCXLiIJatcOOHaMSRARkaliIkQmQ5KAr74CQkOBnBxg4EBRE6SjlVOIiMgIMREikyBJwIQJwJQpYv+zz4B164By5eSNi4iI5KWztcaIDFVuLvDBB8DKlWL/66+Bjz+WNSQiIjIQTISoTMvOBgYNAjZuFHMErVwJDB0qd1RERGQotG4a8/HxwdOnTwscT05O5gSLZFBevAB69RJJkIUFsGEDkyAiIlKndY3Q7du3C50rKDMzEw8ePNBJUESvKj0d6NYNOHRILJOxZQvXDCMiooK0Wn0+zz9nmAaA3NxcHDx4EN7e3joNjuhlPHsmkp4//wTs7IBdu4DAQLmjIiIiQ6RxItS9e3cAYvbof88wbWlpCW9vbyxYsECnwRFpKyFBLJFx/jxQoYKYOLF5c7mjIiIiQ6X1zNLVqlXD6dOn4eLioregiF7Gw4digsS//gLc3YGoKKB+fbmjIiIiQ6Z1H6Fbt24VOJacnAwnJyddxEP0Uu7dA956C4iLA7y8RN+gGjXkjoqIiAyd1qPG5syZg02bNqn233nnHTg7O6NSpUr43//+p9PgiDRx+7boAxQXB3h7iyUzmAQREZEmtE6Eli9fDi8vLwBAVFQUDhw4gMjISHTs2BETJkzQeYBExYmLA1q3Bm7dAqpXF0kQ++wTEZGmtG4ai4+PVyVCu3btQp8+fRAcHAxvb28EBAToPECioly9KprDHj4EatcGDh4EKlWSOyoiIjImWtcIVahQAffu3QMAREZGIigoCAAgSVKh8wsR6cPly6I57OFDwNcXOHKESRAREWlP6xqhnj17YsCAAahZsyaePn2Kjh07AgDOnTuHGuyYQaXg5k0gKEgMlW/YEDhwAHB1lTsqIiIyRlonQl9//TW8vb1x7949zJ07F3Z2dgCAR48eYdSoUToPkOifHj4USdCjR2Jo/KFDwGuvyR0VEREZK60TIUtLS3zyyScFjo8bN04nAREV5elTMVnirVuAjw+wfz+TICIiejVa9xECgB9//BFvvvkmPD09cefOHQDAokWLsH37dp0GR5QnLU0sm3HpEuDhIZrDPDzkjoqIiIyd1onQsmXLEBYWho4dOyI5OVnVQdrJyQmLFi3SdXxEyMoyQ+/e5jh1CnB2FjNGV6smd1RERFQWaJ0Iffvtt1i5ciWmTJkCc3Nz1fGmTZviwoULOg2OKCcHWLCgKQ4fNoOdHbB3L1CvntxRERFRWaF1InTr1i00bty4wHErKys8f/5cJ0ERAYBSCYwYYY6TJz1gZSVh+3YuoEpERLqldSJUrVo1xMbGFjgeGRmJunXr6iImIkgSMG4c8NNPZjAzU+Lnn3Px1ltyR0VERGWNxqPGZsyYgU8++QRhYWEYPXo0Xrx4AUmScOrUKWzYsAERERH4/vvv9RkrmZDwcGDxYrH94Yex6Nq1gbwBERFRmaRxIhQeHo6RI0fi/fffh42NDT7//HNkZGRgwIAB8PT0xDfffIN+/frpM1YyEd98IxIhAPj661xUq3YPABMhIiLSPY0TIUmSVNsDBw7EwIEDkZGRgfT0dFSsWFEvwZHpWbMG+PhjsT1jBjB6tBJ79sgZERERlWVaTaioUCjU9m1tbWFra6vTgMh0bd8ODBsmtseNAz7/XIwaIyIi0hetEqFatWoVSIb+LSkp6ZUCItN09CjQt68YKTZkCLBgAVDCR42IiOiVaZUIhYeHw9HRUV+xkIk6dw7o2hXIzAS6dQNWrmQSREREpUOrRKhfv37sD0Q6FRcHhIQAqalA69bAhg2AhdYr4BEREb0cjecRKqlJjEhbjx6JRVQTEwE/P2DHDsDGRu6oiIjIlGicCP1z1BjRq0pOFjVBt24B1asDkZEAW12JiKi0adwIoVQq9RkHmZCMDKBLF+D8ecDdHdi/XzwSERGVNq2X2CB6FdnZYnTY8eOiBmjfPsDHR+6oiIjIVDERolKjVALvvw/s2gVYWwM7dwING8odFRERmTImQlQqJAn49FNg3TrA3Bz49VegVSu5oyIiIlPHRIhKxdy5YpJEAFi1Cnj7bXnjISIiApgIUSn44Qdg4kSxPX8+MGiQvPEQERHlYSJEerV1KzBihNj+7DNg/Hh54yEiIvonJkKkN8eOAf37i07Sw4YBERFyR0RERKSOiRDpxaVLYt2wzEyge3dg+XKuH0ZERIaHiRDp3IMHQMeOYvboN94A1q/n+mFERGSYmAiRTqWkiCTo3j2gdm2uH0ZERIaNiRDpTFYW0LMncOGCWDIjMhJ47TW5oyIiIioaEyHSCaUSCA0FDh0C7OyAPXsAb2+5oyIiIioeEyHSiYkT8/sC/fYb0Lix3BERERGVjIkQvbJvvwXmzRPbP/wAtG8vbzxERESaYiJEr2TLFmDsWLH91VecNZqIiIyL0SRCSUlJGDhwIBwcHODk5IRhw4YhPT292OesWLECbdq0gYODAxQKBZKTk0snWBNx8iTw7rtiQdUPPshfRoOIiMhYGE0iNHDgQFy6dAlRUVHYtWsXjh07hhF5azcUISMjAyEhIZg8eXIpRWk6bt8GunYFXrwQC6h++y0nTCQiIuNjFNPcXblyBZGRkTh9+jSaNm0KAPj222/RqVMnzJ8/H56enoU+7+OPPwYAHDlypJQiNQ0pKSL5SUwEGjUCNmwAzM3ljoqIiEh7RpEIRUdHw8nJSZUEAUBQUBDMzMxw8uRJ9OjRQ2evlZmZiczMTNV+amoqACA7OxvZ2dk6eY28++jqfqUpJwfo08ccly6ZwcNDwm+/5cDKCtDXWzHmspIDy0tzLCvtsLw0x7LSnD7LStN7GkUiFB8fj4oVK6ods7CwgLOzM+Lj43X6WhEREQgPDy9wfP/+/bC1tdXpa0VFRen0fvomScB//9sQ+/dXg5VVDsaPP47z51Nw/rz+X9vYykpuLC/Nsay0w/LSHMtKc/ooq4yMDI2ukzURmjhxIubMmVPsNVeuXCmlaIRJkyYhLCxMtZ+amgovLy8EBwfDwcFBJ6+RnZ2NqKgotG/fHpaWljq5Z2lYvNgMkZHmUCgk/PQT0K1bS72/prGWlVxYXppjWWmH5aU5lpXm9FlWeS06JZE1ERo/fjyGDBlS7DU+Pj5wd3dHYmKi2vGcnBwkJSXB3d1dpzFZWVnBysqqwHFLS0udf5P0cU992bkTmDBBbM+bp0Dv3qX70TGmsjIELC/Nsay0w/LSHMtKc/r6G6sJWRMhV1dXuLq6lnhdixYtkJycjJiYGPj7+wMADh06BKVSiYCAAH2HafIuXQIGDBBNYyNGAP+oMCMiIjJqRjF8vm7duggJCcHw4cNx6tQpnDhxAmPGjEG/fv1UI8YePHiAOnXq4NSpU6rnxcfHIzY2FnFxcQCACxcuIDY2FklJSbK8D2OUlAR06wakpwNt2wJLlnCYPBERlR1GkQgBwM8//4w6deqgXbt26NSpE958802sWLFCdT47OxtXr15V6xy1fPlyNG7cGMOHDwcAtG7dGo0bN8aOHTtKPX5jlJMD9OsH3LghFlD95ReAtbxERFSWGMWoMQBwdnbG+vXrizzv7e0NSZLUjk2fPh3Tp0/Xc2Rl18SJQFQUYGsLbNsGuLjIHREREZFuGU2NEJWun38GFiwQ26tXA35+8sZDRESkD0yEqICYGOD998X25MlAnz7yxkNERKQvTIRITUIC0L27WEOsc2dg5ky5IyIiItIfJkKkkp0NvPMOcP8+ULu2aB4z4yeEiIjKMP6ZI5XJk4Hffwfs7YHt2wFHR7kjIiIi0i8mQgQA2LoVmD9fbK9ZI2qEiIiIyjomQoQbN4C8lU7CwoCePWUNh4iIqNQwETJxf/8N9OoFpKYCb7wBzJ4td0RERESlh4mQifvwQ+B//wNcXTlzNBERmR4mQiZs3Trghx/E2mHr1wOVKskdERERUeliImSirl0DRo0S29OnA0FBsoZDREQkCyZCJigzUyym+vw50KYNMGWK3BERERHJg4mQCZo0CTh3DnB2Bn76CTA3lzsiIiIieTARMjF79gBffy22V69mvyAiIjJtTIRMyKNH+fMFffgh0LWrrOEQERHJjomQiVAqgUGDgMePAT8/YO5cuSMiIiKSHxMhE/Htt8CBA4CNDbBhA2BtLXdERERE8mMiZAKuXAEmThTb8+cDdevKGw8REZGhYCJUxmVnA++9B7x4AXToAHzwgdwRERERGQ4mQmXczJlATAxQoQKwapWYRZqIiIgEJkJl2MmTwFdfie1lywBPT3njISIiMjRMhMqo589Fk1huLtC/P9C3r9wRERERGR4mQmXU5MnA9etiwsTvvpM7GiIiIsPERKgMOn5cDJcHxOryFSrIGw8REZGhYiJUxmRkAEOHApIkHjt0kDsiIiIiw8VEqIyZOlU0iXl6AgsWyB0NERGRYWMiVIb8+Wf+gqorVgBOTrKGQ0REZPCYCJURL16IpjClUowW69xZ7oiIiIgMHxOhMmLGDLGUhpsbsGiR3NEQEREZByZCZUBMTP5q8suWAc7O8sZDRERkLJgIGbmsLCA0VEyc2Lcv0KOH3BEREREZDyZCRm7WLODCBcDVNX/uICIiItIMEyEjFhubv5bYkiUiGSIiIiLNMREyUkolMGwYkJMD9OwJvPOO3BEREREZHyZCRmrXLuDsWcDBQawlplDIHREREZHxYSJkhCQJiIgQ26NGAe7u8sZDRERkrJgIGaHffxezSFtZAWPHyh0NERGR8WIiZIT++1/xOGQIa4OIiIheBRMhI5OeDmzbJraHDpU1FCIiIqPHRMjI7NgBZGQANWoAzZrJHQ0REZFxYyJkZH7+WTwOGMCRYkRERK+KiZARefwY2LdPbA8YIG8sREREZQETISOyebNYU8zfH6hdW+5oiIiIjB8TISPyz2YxIiIienVMhIzE7dvAiROiX1C/fnJHQ0REVDYwETISGzeKx7ZtAU9PeWMhIiIqK5gIGYlffhGPrA0iIiLSHSZCRiAuDjh3DjA3B3r0kDsaIiKisoOJkBH49Vfx+NZbgIuLvLEQERGVJUyEjEBeIvTOO/LGQUREVNYwETJwbBYjIiLSH6NJhJKSkjBw4EA4ODjAyckJw4YNQ3p6erHXf/jhh6hduzZsbGxQpUoVfPTRR0hJSSnFqF8dm8WIiIj0x2gSoYEDB+LSpUuIiorCrl27cOzYMYwYMaLI6x8+fIiHDx9i/vz5uHjxItasWYPIyEgMGzasFKN+dWwWIyIi0h8LuQPQxJUrVxAZGYnTp0+jadOmAIBvv/0WnTp1wvz58+FZyMQ69evXx5YtW1T71atXx6xZs/Duu+8iJycHFhaG/9bZLEZERKRfhp8NAIiOjoaTk5MqCQKAoKAgmJmZ4eTJk+ihYZaQkpICBweHYpOgzMxMZGZmqvZTU1MBANnZ2cjOzn7Jd6Au7z4l3W/TJjMA5mjTRglHx1zo6OWNiqZlRQLLS3MsK+2wvDTHstKcPstK03saRSIUHx+PihUrqh2zsLCAs7Mz4uPjNbrHkydPMHPmzGKb0wAgIiIC4eHhBY7v378ftra2mgetgaioqGLPr13bGkAF1Kx5AXv23NbpaxubksqK1LG8NMey0g7LS3MsK83po6wyMjI0uk7WRGjixImYM2dOsddcuXLllV8nNTUVnTt3hq+vL6ZPn17stZMmTUJYWJjac728vBAcHAwHB4dXjgUQWWpUVBTat28PS0vLQq958AC4ft0SCoWESZN84eHhq5PXNjaalBXlY3lpjmWlHZaX5lhWmtNnWeW16JRE1kRo/PjxGDJkSLHX+Pj4wN3dHYmJiWrHc3JykJSUBHd392Kfn5aWhpCQENjb22Pr1q0lFrSVlRWsrKwKHLe0tNT5N6m4e+7dKx5ff12BKlX4g6SP8i/LWF6aY1lph+WlOZaV5vT1N1YTsiZCrq6ucHV1LfG6Fi1aIDk5GTExMfD39wcAHDp0CEqlEgEBAUU+LzU1FR06dICVlRV27NgBa2trncWub9u2icfu3eWMgoiIqGwziuHzdevWRUhICIYPH45Tp07hxIkTGDNmDPr166caMfbgwQPUqVMHp06dAiCSoODgYDx//hw//PADUlNTER8fj/j4eOTm5sr5dkqUnAwcOiS2mQgRERHpj1F0lgaAn3/+GWPGjEG7du1gZmaGXr16YfHixarz2dnZuHr1qqpz1NmzZ3Hy5EkAQI0aNdTudevWLXh7e5da7NrauxfIyQHq1gVq1ZI7GiIiorLLaBIhZ2dnrF+/vsjz3t7ekCRJtd+mTRu1fWPCZjEiIqLSYRRNY6YkMxPYs0dsMxEiIiLSLyZCBubQISA9HfD0BP4xfyQRERHpARMhA7N9u3js2hUw43eHiIhIr/in1oBIErB7t9ju2lXeWIiIiEwBEyEDcuECcP8+YGMDtGkjdzRERERlHxMhA5JXG9SunUiGiIiISL+YCBmQvESoc2d54yAiIjIVTIQMxNOnQHS02O7USd5YiIiITAUTIQMRGQkolUCDBkCVKnJHQ0REZBqYCBkINosRERGVPiZCBiAnR9QIAUyEiIiIShMTIQPw55/As2eAszPw+utyR0NERGQ6mAgZgLy1xUJCAAujWQaXiIjI+DERMgB5zWIdO8obBxERkalhIiSzxETg3Dmx3b69vLEQERGZGiZCMouKEo+NGgFubrKGQkREZHKYCMls/37x2KGDvHEQERGZIiZCMpKk/EQoOFjeWIiIiEwREyEZXbgAxMcDtrZAy5ZyR0NERGR6mAjJKCpKFH+bNoCVlbyxEBERmSImQjI6cEABgP2DiIiI5MJESCaZmeY4flwkQuwfREREJA8mQjK5dOk1ZGYqUKUKULu23NEQERGZJiZCMjl3riIAURukUMgcDBERkYliIiQTe/ssVK0qsX8QERGRjLjEp0z69LmG1atrwMLCUu5QiIiITBZrhGSkUABm/A4QERHJhn+GiYiIyGQxESIiIiKTxUSIiIiITBYTISIiIjJZTISIiIjIZDERIiIiIpPFRIiIiIhMFhMhIiIiMllMhIiIiMhkMREiIiIik8VEiIiIiEwWEyEiIiIyWUyEiIiIyGRZyB2AoZMkCQCQmpqqs3tmZ2cjIyMDqampsLS01Nl9yyKWlXZYXppjWWmH5aU5lpXm9FlWeX+38/6OF4WJUAnS0tIAAF5eXjJHQkRERNpKS0uDo6NjkecVUkmpkolTKpV4+PAh7O3toVAodHLP1NRUeHl54d69e3BwcNDJPcsqlpV2WF6aY1lph+WlOZaV5vRZVpIkIS0tDZ6enjAzK7onEGuESmBmZobKlSvr5d4ODg78IdEQy0o7LC/Nsay0w/LSHMtKc/oqq+JqgvKwszQRERGZLCZCREREZLKYCMnAysoK06ZNg5WVldyhGDyWlXZYXppjWWmH5aU5lpXmDKGs2FmaiIiITBZrhIiIiMhkMREiIiIik8VEiIiIiEwWEyEiIiIyWUyEStl3330Hb29vWFtbIyAgAKdOnZI7JIMwffp0KBQKta86deqozr948QKjR4/Ga6+9Bjs7O/Tq1QsJCQkyRlx6jh07hi5dusDT0xMKhQLbtm1TOy9JEqZOnQoPDw/Y2NggKCgI169fV7smKSkJAwcOhIODA5ycnDBs2DCkp6eX4rsoPSWV15AhQwp81kJCQtSuMYXyioiIQLNmzWBvb4+KFSuie/fuuHr1qto1mvzc3b17F507d4atrS0qVqyICRMmICcnpzTfSqnQpLzatGlT4LM1cuRItWtMobyWLVuGhg0bqiZJbNGiBfbu3as6b2ifKyZCpWjTpk0ICwvDtGnTcPbsWfj5+aFDhw5ITEyUOzSDUK9ePTx69Ej1dfz4cdW5cePGYefOnfj1119x9OhRPHz4ED179pQx2tLz/Plz+Pn54bvvviv0/Ny5c7F48WIsX74cJ0+eRPny5dGhQwe8ePFCdc3AgQNx6dIlREVFYdeuXTh27BhGjBhRWm+hVJVUXgAQEhKi9lnbsGGD2nlTKK+jR49i9OjR+PPPPxEVFYXs7GwEBwfj+fPnqmtK+rnLzc1F586dkZWVhT/++ANr167FmjVrMHXqVDnekl5pUl4AMHz4cLXP1ty5c1XnTKW8KleujNmzZyMmJgZnzpzBW2+9hW7duuHSpUsADPBzJVGpad68uTR69GjVfm5uruTp6SlFRETIGJVhmDZtmuTn51foueTkZMnS0lL69ddfVceuXLkiAZCio6NLKULDAEDaunWral+pVEru7u7SvHnzVMeSk5MlKysracOGDZIkSdLly5clANLp06dV1+zdu1dSKBTSgwcPSi12Ofy7vCRJkgYPHix169atyOeYanklJiZKAKSjR49KkqTZz92ePXskMzMzKT4+XnXNsmXLJAcHBykzM7N030Ap+3d5SZIkBQYGSmPHji3yOaZcXhUqVJC+//57g/xcsUaolGRlZSEmJgZBQUGqY2ZmZggKCkJ0dLSMkRmO69evw9PTEz4+Phg4cCDu3r0LAIiJiUF2drZa2dWpUwdVqlQx+bK7desW4uPj1crG0dERAQEBqrKJjo6Gk5MTmjZtqromKCgIZmZmOHnyZKnHbAiOHDmCihUronbt2vjggw/w9OlT1TlTLa+UlBQAgLOzMwDNfu6io6PRoEEDuLm5qa7p0KEDUlNTVf/9l1X/Lq88P//8M1xcXFC/fn1MmjQJGRkZqnOmWF65ubnYuHEjnj9/jhYtWhjk54qLrpaSJ0+eIDc3V+0bCwBubm7466+/ZIrKcAQEBGDNmjWoXbs2Hj16hPDwcLRq1QoXL15EfHw8ypUrBycnJ7XnuLm5IT4+Xp6ADUTe+y/sc5V3Lj4+HhUrVlQ7b2FhAWdnZ5Msv5CQEPTs2RPVqlXDjRs3MHnyZHTs2BHR0dEwNzc3yfJSKpX4+OOP0bJlS9SvXx8ANPq5i4+PL/Szl3eurCqsvABgwIABqFq1Kjw9PXH+/Hl89tlnuHr1Kn777TcAplVeFy5cQIsWLfDixQvY2dlh69at8PX1RWxsrMF9rpgIkUHo2LGjarthw4YICAhA1apV8csvv8DGxkbGyKis6devn2q7QYMGaNiwIapXr44jR46gXbt2MkYmn9GjR+PixYtq/fKoaEWV1z/7kTVo0AAeHh5o164dbty4gerVq5d2mLKqXbs2YmNjkZKSgs2bN2Pw4ME4evSo3GEVik1jpcTFxQXm5uYFesYnJCTA3d1dpqgMl5OTE2rVqoW4uDi4u7sjKysLycnJatew7KB6/8V9rtzd3Qt0yM/JyUFSUpLJlx8A+Pj4wMXFBXFxcQBMr7zGjBmDXbt24fDhw6hcubLquCY/d+7u7oV+9vLOlUVFlVdhAgICAEDts2Uq5VWuXDnUqFED/v7+iIiIgJ+fH7755huD/FwxESol5cqVg7+/Pw4ePKg6plQqcfDgQbRo0ULGyAxTeno6bty4AQ8PD/j7+8PS0lKt7K5evYq7d++afNlVq1YN7u7uamWTmpqKkydPqsqmRYsWSE5ORkxMjOqaQ4cOQalUqn5Rm7L79+/j6dOn8PDwAGA65SVJEsaMGYOtW7fi0KFDqFatmtp5TX7uWrRogQsXLqgljlFRUXBwcICvr2/pvJFSUlJ5FSY2NhYA1D5bplJe/6ZUKpGZmWmYnyudd7+mIm3cuFGysrKS1qxZI12+fFkaMWKE5OTkpNYz3lSNHz9eOnLkiHTr1i3pxIkTUlBQkOTi4iIlJiZKkiRJI0eOlKpUqSIdOnRIOnPmjNSiRQupRYsWMkddOtLS0qRz585J586dkwBICxculM6dOyfduXNHkiRJmj17tuTk5CRt375dOn/+vNStWzepWrVq0t9//626R0hIiNS4cWPp5MmT0vHjx6WaNWtK/fv3l+st6VVx5ZWWliZ98sknUnR0tHTr1i3pwIEDUpMmTaSaNWtKL168UN3DFMrrgw8+kBwdHaUjR45Ijx49Un1lZGSorinp5y4nJ0eqX7++FBwcLMXGxkqRkZGSq6urNGnSJDnekl6VVF5xcXHSjBkzpDNnzki3bt2Stm/fLvn4+EitW7dW3cNUymvixInS0aNHpVu3bknnz5+XJk6cKCkUCmn//v2SJBne54qJUCn79ttvpSpVqkjlypWTmjdvLv35559yh2QQ+vbtK3l4eEjlypWTKlWqJPXt21eKi4tTnf/777+lUaNGSRUqVJBsbW2lHj16SI8ePZIx4tJz+PBhCUCBr8GDB0uSJIbQf/HFF5Kbm5tkZWUltWvXTrp69araPZ4+fSr1799fsrOzkxwcHKTQ0FApLS1Nhnejf8WVV0ZGhhQcHCy5urpKlpaWUtWqVaXhw4cX+GfEFMqrsDICIK1evVp1jSY/d7dv35Y6duwo2djYSC4uLtL48eOl7OzsUn43+ldSed29e1dq3bq15OzsLFlZWUk1atSQJkyYIKWkpKjdxxTKa+jQoVLVqlWlcuXKSa6urlK7du1USZAkGd7nSiFJkqT7eiYiIiIiw8c+QkRERGSymAgRERGRyWIiRERERCaLiRARERGZLCZCREREZLKYCBEREZHJYiJEREREJouJEBFRKTly5AgUCkWBdZaISD5MhIiIiMhkMREiIiIik8VEiIh0rk2bNvjoo4/w6aefwtnZGe7u7pg+fToA4Pbt21AoFKqVuQEgOTkZCoUCR44cAZDfhLRv3z40btwYNjY2eOutt5CYmIi9e/eibt26cHBwwIABA5CRkaFRTEqlEhEREahWrRpsbGzg5+eHzZs3q87nvebu3bvRsGFDWFtb4/XXX8fFixfV7rNlyxbUq1cPVlZW8Pb2xoIFC9TOZ2Zm4rPPPoOXlxesrKxQo0YN/PDDD2rXxMTEoGnTprC1tcUbb7yBq1evqs7973//Q9u2bWFvbw8HBwf4+/vjzJkzGr1HItIeEyEi0ou1a9eifPnyOHnyJObOnYsZM2YgKipKq3tMnz4dS5YswR9//IF79+6hT58+WLRoEdavX4/du3dj//79+PbbbzW6V0REBNatW4fly5fj0qVLGDduHN59910cPXpU7boJEyZgwYIFOH36NFxdXdGlSxdkZ2cDEAlMnz590K9fP1y4cAHTp0/HF198gTVr1qieP2jQIGzYsAGLFy/GlStX8N///hd2dnZqrzFlyhQsWLAAZ86cgYWFBYYOHao6N3DgQFSuXBmnT59GTEwMJk6cCEtLS63KjYi0oJelXInIpAUGBkpvvvmm2rFmzZpJn332mXTr1i0JgHTu3DnVuWfPnkkApMOHD0uSlL+C/IEDB1TXRERESACkGzduqI795z//kTp06FBiPC9evJBsbW2lP/74Q+34sGHDpP79+6u95saNG1Xnnz59KtnY2EibNm2SJEmSBgwYILVv317tHhMmTJB8fX0lSZKkq1evSgCkqKioQuMo7H3t3r1bAiD9/fffkiRJkr29vbRmzZoS3xMR6QZrhIhILxo2bKi27+HhgcTExJe+h5ubG2xtbeHj46N2TJN7xsXFISMjA+3bt4ednZ3qa926dbhx44batS1atFBtOzs7o3bt2rhy5QoA4MqVK2jZsqXa9S1btsT169eRm5uL2NhYmJubIzAwUOP35eHhAQCq9xEWFob3338fQUFBmD17doH4iEi3LOQOgIjKpn835ygUCiiVSpiZif+/JElSnctreiruHgqFosh7liQ9PR0AsHv3blSqVEntnJWVVYnP15SNjY1G1/37fQFQvY/p06djwIAB2L17N/bu3Ytp06Zh48aN6NGjh87iJKJ8rBEiolLl6uoKAHj06JHq2D87TuuDr68vrKyscPfuXdSoUUPty8vLS+3aP//8U7X97NkzXLt2DXXr1gUA1K1bFydOnFC7/sSJE6hVqxbMzc3RoEEDKJXKAv2OtFWrVi2MGzcO+/fvR8+ePbF69epXuh8RFY01QkRUqmxsbPD6669j9uzZqFatGhITE/H555/r9TXt7e3xySefYNy4cVAqlXjzzTeRkpKCEydOwMHBAYMHD1ZdO2PGDLz22mtwc3PDlClT4OLigu7duwMAxo8fj2bNmmHmzJno27cvoqOjsWTJEixduhQA4O3tjcGDB2Po0KFYvHgx/Pz8cOfOHSQmJqJPnz4lxvn3339jwoQJ6N27N6pVq4b79+/j9OnT6NWrl17KhYiYCBGRDFatWoVhw4bB398ftWvXxty5cxEcHKzX15w5cyZcXV0RERGBmzdvwsnJCU2aNMHkyZPVrps9ezbGjh2L69evo1GjRti5cyfKlSsHAGjSpAl++eUXTJ06FTNnzoSHhwdmzJiBIUOGqJ6/bNkyTJ48GaNGjcLTp09RpUqVAq9RFHNzczx9+hSDBg1CQkICXFxc0LNnT4SHh+usHIhInUL6Z0M9EZGJOnLkCNq2bYtnz57ByclJ7nCIqJSwjxARERGZLCZCRGT07t69qzYs/t9fd+/elTtEIjJQbBojIqOXk5OD27dvF3ne29sbFhbsEklEBTERIiIiIpPFpjEiIiIyWUyEiIiIyGQxESIiIiKTxUSIiIiITBYTISIiIjJZTISIiIjIZDERIiIiIpPFRIiIiIhM1v8BrIoABjSWLs4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.2876154151963989\n",
      "Corresponding RMSE: 0.2848615667519758\n",
      "Corresponding num_epochs: 190\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.02999269115053993\n",
      "Corresponding RMSE: 0.2848615667519758\n",
      "Corresponding num_epochs: 190\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.09122232926948703\n",
      "Corresponding RMSE: 0.2850349377472079\n",
      "Corresponding num_epochs: 193\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
