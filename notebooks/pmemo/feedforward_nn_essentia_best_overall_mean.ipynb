{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Overall Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.540440</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.386979</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099370</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652892</td>\n",
       "      <td>3.385207</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.melbands_kurtosis.mean  \\\n",
       "0          1                        -0.290830   \n",
       "1          4                         2.148017   \n",
       "2          5                         4.061978   \n",
       "3          6                         0.609811   \n",
       "4          7                        -0.407720   \n",
       "..       ...                              ...   \n",
       "762      993                        -0.341976   \n",
       "763      996                         0.208018   \n",
       "764      997                        -0.448361   \n",
       "765      999                        -0.296933   \n",
       "766     1000                        -0.599766   \n",
       "\n",
       "     lowlevel.melbands_skewness.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                          -0.161553                      -0.674310   \n",
       "1                           2.355089                       6.176068   \n",
       "2                           3.102229                       2.654601   \n",
       "3                           1.478902                       0.594623   \n",
       "4                          -0.322652                      -0.043632   \n",
       "..                               ...                            ...   \n",
       "762                        -0.376271                      -0.184984   \n",
       "763                        -0.004852                      -0.227602   \n",
       "764                        -0.267697                      -0.099497   \n",
       "765                         0.022632                       0.258205   \n",
       "766                        -0.456381                      -0.371258   \n",
       "\n",
       "     tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                     -0.966508                -0.692188   \n",
       "1                     -0.692793                -0.875022   \n",
       "2                      0.480322                 0.431641   \n",
       "3                     -0.265029                -1.486659   \n",
       "4                      0.343143                 1.010394   \n",
       "..                          ...                      ...   \n",
       "762                    1.148765                 0.172968   \n",
       "763                    1.123782                 0.880366   \n",
       "764                   -1.869459                 0.089641   \n",
       "765                   -1.921844                -0.625854   \n",
       "766                   -0.462222                 0.334123   \n",
       "\n",
       "     tonal.key_edma.strength  tonal.key_temperley.strength  \\\n",
       "0                  -0.466809                     -0.224314   \n",
       "1                  -2.504826                     -2.346037   \n",
       "2                   0.529917                      0.178117   \n",
       "3                  -1.304917                     -1.560061   \n",
       "4                   0.710881                      0.859542   \n",
       "..                       ...                           ...   \n",
       "762                -0.030536                      0.217640   \n",
       "763                 0.113897                      0.233136   \n",
       "764                -0.349415                     -0.176068   \n",
       "765                -0.779872                     -0.526150   \n",
       "766                -1.187954                     -1.193146   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                                  -0.490847   \n",
       "1                                   1.052759   \n",
       "2                                   1.201395   \n",
       "3                                   1.129215   \n",
       "4                                   1.536379   \n",
       "..                                       ...   \n",
       "762                                -0.104210   \n",
       "763                                -1.932951   \n",
       "764                                -0.148750   \n",
       "765                                 0.666036   \n",
       "766                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  ...  tonal.chords_histogram_14  \\\n",
       "0                                  -0.744314  ...                  -0.196902   \n",
       "1                                  -0.515532  ...                   0.004772   \n",
       "2                                  -0.649196  ...                  -0.196902   \n",
       "3                                  -0.547660  ...                  -0.196902   \n",
       "4                                  -0.715476  ...                  -0.196902   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                 1.736366  ...                  -0.196902   \n",
       "763                                 1.310770  ...                  -0.196902   \n",
       "764                                -0.310712  ...                  -0.099370   \n",
       "765                                -1.240647  ...                  -0.196902   \n",
       "766                                 0.160289  ...                   1.652892   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                    -0.312361                  -0.251295   \n",
       "1                     0.540440                   4.181020   \n",
       "2                    -0.312361                  -0.251295   \n",
       "3                     0.386979                  -0.251295   \n",
       "4                    -0.312361                  -0.251295   \n",
       "..                         ...                        ...   \n",
       "762                  -0.312361                  -0.251295   \n",
       "763                  -0.312361                  -0.251295   \n",
       "764                  -0.027929                  -0.251295   \n",
       "765                  -0.312361                  -0.251295   \n",
       "766                   3.385207                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.671714  \n",
       "1                    -0.053671  \n",
       "2                    -0.907128  \n",
       "3                    -0.907128  \n",
       "4                    -0.375831  \n",
       "..                         ...  \n",
       "762                   3.825664  \n",
       "763                  -0.814508  \n",
       "764                  -0.088299  \n",
       "765                  -0.798329  \n",
       "766                  -0.907128  \n",
       "\n",
       "[767 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean = pd.read_csv(get_pmemo_path('processed/features/standardised_essentia_best_overall_features_mean.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_overall_features_mean = df_essentia_best_overall_features_mean[df_essentia_best_overall_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_overall_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   song_id                                  767 non-null    int64  \n",
      " 1   lowlevel.melbands_kurtosis.mean          767 non-null    float64\n",
      " 2   lowlevel.melbands_skewness.mean          767 non-null    float64\n",
      " 3   lowlevel.spectral_energy.mean            767 non-null    float64\n",
      " 4   tonal.chords_strength.mean               767 non-null    float64\n",
      " 5   tonal.hpcp_entropy.mean                  767 non-null    float64\n",
      " 6   tonal.key_edma.strength                  767 non-null    float64\n",
      " 7   tonal.key_temperley.strength             767 non-null    float64\n",
      " 8   rhythm.beats_loudness_band_ratio.mean_0  767 non-null    float64\n",
      " 9   rhythm.beats_loudness_band_ratio.mean_1  767 non-null    float64\n",
      " 10  rhythm.beats_loudness_band_ratio.mean_2  767 non-null    float64\n",
      " 11  rhythm.beats_loudness_band_ratio.mean_3  767 non-null    float64\n",
      " 12  rhythm.beats_loudness_band_ratio.mean_4  767 non-null    float64\n",
      " 13  rhythm.beats_loudness_band_ratio.mean_5  767 non-null    float64\n",
      " 14  tonal.chords_histogram_0                 767 non-null    float64\n",
      " 15  tonal.chords_histogram_1                 767 non-null    float64\n",
      " 16  tonal.chords_histogram_2                 767 non-null    float64\n",
      " 17  tonal.chords_histogram_3                 767 non-null    float64\n",
      " 18  tonal.chords_histogram_4                 767 non-null    float64\n",
      " 19  tonal.chords_histogram_5                 767 non-null    float64\n",
      " 20  tonal.chords_histogram_6                 767 non-null    float64\n",
      " 21  tonal.chords_histogram_7                 767 non-null    float64\n",
      " 22  tonal.chords_histogram_8                 767 non-null    float64\n",
      " 23  tonal.chords_histogram_9                 767 non-null    float64\n",
      " 24  tonal.chords_histogram_10                767 non-null    float64\n",
      " 25  tonal.chords_histogram_11                767 non-null    float64\n",
      " 26  tonal.chords_histogram_12                767 non-null    float64\n",
      " 27  tonal.chords_histogram_13                767 non-null    float64\n",
      " 28  tonal.chords_histogram_14                767 non-null    float64\n",
      " 29  tonal.chords_histogram_15                767 non-null    float64\n",
      " 30  tonal.chords_histogram_16                767 non-null    float64\n",
      " 31  tonal.chords_histogram_17                767 non-null    float64\n",
      " 32  tonal.chords_histogram_18                767 non-null    float64\n",
      " 33  tonal.chords_histogram_19                767 non-null    float64\n",
      " 34  tonal.chords_histogram_20                767 non-null    float64\n",
      " 35  tonal.chords_histogram_21                767 non-null    float64\n",
      " 36  tonal.chords_histogram_22                767 non-null    float64\n",
      " 37  tonal.chords_histogram_23                767 non-null    float64\n",
      "dtypes: float64(37), int64(1)\n",
      "memory usage: 227.8 KB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>-0.154840</td>\n",
       "      <td>...</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>-1.032420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>-1.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>-1.128256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>-0.502120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>-0.198898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>-0.747041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>-1.225680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>-0.090239</td>\n",
       "      <td>...</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                          -0.290830                        -0.161553   \n",
       "1                           2.148017                         2.355089   \n",
       "2                           4.061978                         3.102229   \n",
       "3                           0.609811                         1.478902   \n",
       "4                          -0.407720                        -0.322652   \n",
       "..                               ...                              ...   \n",
       "762                        -0.341976                        -0.376271   \n",
       "763                         0.208018                        -0.004852   \n",
       "764                        -0.448361                        -0.267697   \n",
       "765                        -0.296933                         0.022632   \n",
       "766                        -0.599766                        -0.456381   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                        -0.674310                   -0.966508   \n",
       "1                         6.176068                   -0.692793   \n",
       "2                         2.654601                    0.480322   \n",
       "3                         0.594623                   -0.265029   \n",
       "4                        -0.043632                    0.343143   \n",
       "..                             ...                         ...   \n",
       "762                      -0.184984                    1.148765   \n",
       "763                      -0.227602                    1.123782   \n",
       "764                      -0.099497                   -1.869459   \n",
       "765                       0.258205                   -1.921844   \n",
       "766                      -0.371258                   -0.462222   \n",
       "\n",
       "     tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                  -0.692188                -0.466809   \n",
       "1                  -0.875022                -2.504826   \n",
       "2                   0.431641                 0.529917   \n",
       "3                  -1.486659                -1.304917   \n",
       "4                   1.010394                 0.710881   \n",
       "..                       ...                      ...   \n",
       "762                 0.172968                -0.030536   \n",
       "763                 0.880366                 0.113897   \n",
       "764                 0.089641                -0.349415   \n",
       "765                -0.625854                -0.779872   \n",
       "766                 0.334123                -1.187954   \n",
       "\n",
       "     tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                       -0.224314                                -0.490847   \n",
       "1                       -2.346037                                 1.052759   \n",
       "2                        0.178117                                 1.201395   \n",
       "3                       -1.560061                                 1.129215   \n",
       "4                        0.859542                                 1.536379   \n",
       "..                            ...                                      ...   \n",
       "762                      0.217640                                -0.104210   \n",
       "763                      0.233136                                -1.932951   \n",
       "764                     -0.176068                                -0.148750   \n",
       "765                     -0.526150                                 0.666036   \n",
       "766                     -1.193146                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                  -0.744314   \n",
       "1                                  -0.515532   \n",
       "2                                  -0.649196   \n",
       "3                                  -0.547660   \n",
       "4                                  -0.715476   \n",
       "..                                       ...   \n",
       "762                                 1.736366   \n",
       "763                                 1.310770   \n",
       "764                                -0.310712   \n",
       "765                                -1.240647   \n",
       "766                                 0.160289   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_16  \\\n",
       "0                                   0.149480  ...                  -0.251295   \n",
       "1                                  -0.154840  ...                   4.181020   \n",
       "2                                  -1.032420  ...                  -0.251295   \n",
       "3                                  -1.166668  ...                  -0.251295   \n",
       "4                                  -1.128256  ...                  -0.251295   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                -0.502120  ...                  -0.251295   \n",
       "763                                -0.198898  ...                  -0.251295   \n",
       "764                                -0.747041  ...                  -0.251295   \n",
       "765                                -1.225680  ...                  -0.251295   \n",
       "766                                -0.090239  ...                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  valence_mean_mapped  arousal_mean_mapped  \n",
       "0                     0.671714                0.150               -0.200  \n",
       "1                    -0.053671               -0.425               -0.475  \n",
       "2                    -0.907128               -0.600               -0.700  \n",
       "3                    -0.907128               -0.300                0.025  \n",
       "4                    -0.375831                0.450                0.400  \n",
       "..                         ...                  ...                  ...  \n",
       "762                   3.825664                0.525                0.725  \n",
       "763                  -0.814508                0.125                0.750  \n",
       "764                  -0.088299                0.325                0.425  \n",
       "765                  -0.798329                0.550                0.750  \n",
       "766                  -0.907128                0.150                0.325  \n",
       "\n",
       "[767 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean_whole = pd.merge(df_essentia_best_overall_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_overall_features_mean_whole = df_essentia_best_overall_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_overall_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.161553</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.966508</td>\n",
       "      <td>-0.692188</td>\n",
       "      <td>-0.466809</td>\n",
       "      <td>-0.224314</td>\n",
       "      <td>-0.490847</td>\n",
       "      <td>-0.744314</td>\n",
       "      <td>0.149480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>1.021745</td>\n",
       "      <td>-0.039876</td>\n",
       "      <td>0.671714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.355089</td>\n",
       "      <td>6.176068</td>\n",
       "      <td>-0.692793</td>\n",
       "      <td>-0.875022</td>\n",
       "      <td>-2.504826</td>\n",
       "      <td>-2.346037</td>\n",
       "      <td>1.052759</td>\n",
       "      <td>-0.515532</td>\n",
       "      <td>-0.154840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.540440</td>\n",
       "      <td>4.181020</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.576715</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.213223</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.053671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.102229</td>\n",
       "      <td>2.654601</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.529917</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>1.201395</td>\n",
       "      <td>-0.649196</td>\n",
       "      <td>-1.032420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.305337</td>\n",
       "      <td>-0.203477</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.478902</td>\n",
       "      <td>0.594623</td>\n",
       "      <td>-0.265029</td>\n",
       "      <td>-1.486659</td>\n",
       "      <td>-1.304917</td>\n",
       "      <td>-1.560061</td>\n",
       "      <td>1.129215</td>\n",
       "      <td>-0.547660</td>\n",
       "      <td>-1.166668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>0.386979</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.179551</td>\n",
       "      <td>-0.846705</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.322652</td>\n",
       "      <td>-0.043632</td>\n",
       "      <td>0.343143</td>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.710881</td>\n",
       "      <td>0.859542</td>\n",
       "      <td>1.536379</td>\n",
       "      <td>-0.715476</td>\n",
       "      <td>-1.128256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.457974</td>\n",
       "      <td>0.503538</td>\n",
       "      <td>-0.375831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.376271</td>\n",
       "      <td>-0.184984</td>\n",
       "      <td>1.148765</td>\n",
       "      <td>0.172968</td>\n",
       "      <td>-0.030536</td>\n",
       "      <td>0.217640</td>\n",
       "      <td>-0.104210</td>\n",
       "      <td>1.736366</td>\n",
       "      <td>-0.502120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.129750</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.443708</td>\n",
       "      <td>0.429159</td>\n",
       "      <td>3.825664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.227602</td>\n",
       "      <td>1.123782</td>\n",
       "      <td>0.880366</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.233136</td>\n",
       "      <td>-1.932951</td>\n",
       "      <td>1.310770</td>\n",
       "      <td>-0.198898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.483101</td>\n",
       "      <td>-0.814508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.267697</td>\n",
       "      <td>-0.099497</td>\n",
       "      <td>-1.869459</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>-0.349415</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>-0.148750</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>-0.747041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099370</td>\n",
       "      <td>-0.027929</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>0.099811</td>\n",
       "      <td>-0.278709</td>\n",
       "      <td>-0.150331</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>-0.557852</td>\n",
       "      <td>-0.088299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.296933</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.258205</td>\n",
       "      <td>-1.921844</td>\n",
       "      <td>-0.625854</td>\n",
       "      <td>-0.779872</td>\n",
       "      <td>-0.526150</td>\n",
       "      <td>0.666036</td>\n",
       "      <td>-1.240647</td>\n",
       "      <td>-1.225680</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196902</td>\n",
       "      <td>-0.312361</td>\n",
       "      <td>-0.251295</td>\n",
       "      <td>-0.456893</td>\n",
       "      <td>-0.229701</td>\n",
       "      <td>-0.374009</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>0.072929</td>\n",
       "      <td>-0.798329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.456381</td>\n",
       "      <td>-0.371258</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>0.334123</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-1.193146</td>\n",
       "      <td>-0.476121</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>-0.090239</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652892</td>\n",
       "      <td>3.385207</td>\n",
       "      <td>5.071215</td>\n",
       "      <td>0.307603</td>\n",
       "      <td>1.287375</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.285526</td>\n",
       "      <td>-0.535816</td>\n",
       "      <td>-0.840485</td>\n",
       "      <td>-0.907128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                          -0.290830                        -0.161553   \n",
       "1                           2.148017                         2.355089   \n",
       "2                           4.061978                         3.102229   \n",
       "3                           0.609811                         1.478902   \n",
       "4                          -0.407720                        -0.322652   \n",
       "..                               ...                              ...   \n",
       "762                        -0.341976                        -0.376271   \n",
       "763                         0.208018                        -0.004852   \n",
       "764                        -0.448361                        -0.267697   \n",
       "765                        -0.296933                         0.022632   \n",
       "766                        -0.599766                        -0.456381   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                        -0.674310                   -0.966508   \n",
       "1                         6.176068                   -0.692793   \n",
       "2                         2.654601                    0.480322   \n",
       "3                         0.594623                   -0.265029   \n",
       "4                        -0.043632                    0.343143   \n",
       "..                             ...                         ...   \n",
       "762                      -0.184984                    1.148765   \n",
       "763                      -0.227602                    1.123782   \n",
       "764                      -0.099497                   -1.869459   \n",
       "765                       0.258205                   -1.921844   \n",
       "766                      -0.371258                   -0.462222   \n",
       "\n",
       "     tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                  -0.692188                -0.466809   \n",
       "1                  -0.875022                -2.504826   \n",
       "2                   0.431641                 0.529917   \n",
       "3                  -1.486659                -1.304917   \n",
       "4                   1.010394                 0.710881   \n",
       "..                       ...                      ...   \n",
       "762                 0.172968                -0.030536   \n",
       "763                 0.880366                 0.113897   \n",
       "764                 0.089641                -0.349415   \n",
       "765                -0.625854                -0.779872   \n",
       "766                 0.334123                -1.187954   \n",
       "\n",
       "     tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                       -0.224314                                -0.490847   \n",
       "1                       -2.346037                                 1.052759   \n",
       "2                        0.178117                                 1.201395   \n",
       "3                       -1.560061                                 1.129215   \n",
       "4                        0.859542                                 1.536379   \n",
       "..                            ...                                      ...   \n",
       "762                      0.217640                                -0.104210   \n",
       "763                      0.233136                                -1.932951   \n",
       "764                     -0.176068                                -0.148750   \n",
       "765                     -0.526150                                 0.666036   \n",
       "766                     -1.193146                                -0.476121   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                  -0.744314   \n",
       "1                                  -0.515532   \n",
       "2                                  -0.649196   \n",
       "3                                  -0.547660   \n",
       "4                                  -0.715476   \n",
       "..                                       ...   \n",
       "762                                 1.736366   \n",
       "763                                 1.310770   \n",
       "764                                -0.310712   \n",
       "765                                -1.240647   \n",
       "766                                 0.160289   \n",
       "\n",
       "     rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_14  \\\n",
       "0                                   0.149480  ...                  -0.196902   \n",
       "1                                  -0.154840  ...                   0.004772   \n",
       "2                                  -1.032420  ...                  -0.196902   \n",
       "3                                  -1.166668  ...                  -0.196902   \n",
       "4                                  -1.128256  ...                  -0.196902   \n",
       "..                                       ...  ...                        ...   \n",
       "762                                -0.502120  ...                  -0.196902   \n",
       "763                                -0.198898  ...                  -0.196902   \n",
       "764                                -0.747041  ...                  -0.099370   \n",
       "765                                -1.225680  ...                  -0.196902   \n",
       "766                                -0.090239  ...                   1.652892   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                    -0.312361                  -0.251295   \n",
       "1                     0.540440                   4.181020   \n",
       "2                    -0.312361                  -0.251295   \n",
       "3                     0.386979                  -0.251295   \n",
       "4                    -0.312361                  -0.251295   \n",
       "..                         ...                        ...   \n",
       "762                  -0.312361                  -0.251295   \n",
       "763                  -0.312361                  -0.251295   \n",
       "764                  -0.027929                  -0.251295   \n",
       "765                  -0.312361                  -0.251295   \n",
       "766                   3.385207                   5.071215   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                    -0.456893                  -0.278709   \n",
       "1                    -0.456893                  -0.005882   \n",
       "2                    -0.456893                  -0.278709   \n",
       "3                    -0.456893                  -0.278709   \n",
       "4                    -0.456893                  -0.278709   \n",
       "..                         ...                        ...   \n",
       "762                  -0.129750                  -0.278709   \n",
       "763                  -0.456893                  -0.278709   \n",
       "764                   0.099811                  -0.278709   \n",
       "765                  -0.456893                  -0.229701   \n",
       "766                   0.307603                   1.287375   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.345904                  -0.285526   \n",
       "1                     0.576715                  -0.285526   \n",
       "2                    -0.305337                  -0.203477   \n",
       "3                    -0.374009                  -0.285526   \n",
       "4                    -0.374009                  -0.285526   \n",
       "..                         ...                        ...   \n",
       "762                  -0.374009                  -0.285526   \n",
       "763                  -0.374009                  -0.285526   \n",
       "764                  -0.150331                  -0.285526   \n",
       "765                  -0.374009                   0.288011   \n",
       "766                  -0.214807                  -0.285526   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     1.021745                  -0.039876   \n",
       "1                     0.213223                  -0.846705   \n",
       "2                    -0.535816                  -0.846705   \n",
       "3                    -0.179551                  -0.846705   \n",
       "4                     0.457974                   0.503538   \n",
       "..                         ...                        ...   \n",
       "762                  -0.443708                   0.429159   \n",
       "763                  -0.535816                  -0.483101   \n",
       "764                   0.505638                  -0.557852   \n",
       "765                  -0.535816                   0.072929   \n",
       "766                  -0.535816                  -0.840485   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.671714  \n",
       "1                    -0.053671  \n",
       "2                    -0.907128  \n",
       "3                    -0.907128  \n",
       "4                    -0.375831  \n",
       "..                         ...  \n",
       "762                   3.825664  \n",
       "763                  -0.814508  \n",
       "764                  -0.088299  \n",
       "765                  -0.798329  \n",
       "766                  -0.907128  \n",
       "\n",
       "[767 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_overall_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117033e70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 37])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 37])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {r2_score}')\n",
    "  return test_pred, rmse, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2848615667519758\n",
      "Valence RMSE: 0.2623395437817343\n",
      "Arousal RMSE: 0.30572894562532393\n",
      "Test R^2 score: tensor([0.2646, 0.3107], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_overall_mean_standardised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1279, -0.0993],\n",
       "        [ 0.0413,  0.0752],\n",
       "        [ 0.4736,  0.5887],\n",
       "        [ 0.2325,  0.2671],\n",
       "        [ 0.4503,  0.5489],\n",
       "        [-0.0501, -0.0167],\n",
       "        [ 0.0609,  0.0949],\n",
       "        [ 0.0431,  0.0769],\n",
       "        [-0.1252, -0.0982],\n",
       "        [-0.1237, -0.0975],\n",
       "        [ 0.3342,  0.3693],\n",
       "        [-0.1317, -0.1009],\n",
       "        [-0.1306, -0.1004],\n",
       "        [-0.1291, -0.0998],\n",
       "        [ 0.4180,  0.4942],\n",
       "        [ 0.0271,  0.0609],\n",
       "        [ 0.4902,  0.6169],\n",
       "        [ 0.2454,  0.2801],\n",
       "        [-0.1265, -0.0987],\n",
       "        [-0.1348, -0.1022],\n",
       "        [ 0.1483,  0.1826],\n",
       "        [-0.1253, -0.0982],\n",
       "        [ 0.4057,  0.4731],\n",
       "        [-0.1314, -0.1008],\n",
       "        [-0.1260, -0.0985],\n",
       "        [-0.1302, -0.1003],\n",
       "        [ 0.5588,  0.7335],\n",
       "        [-0.1304, -0.1003],\n",
       "        [ 0.4189,  0.4957],\n",
       "        [ 0.5069,  0.6454],\n",
       "        [-0.1294, -0.1000],\n",
       "        [ 0.3106,  0.3456],\n",
       "        [ 0.3025,  0.3375],\n",
       "        [ 0.2537,  0.2885],\n",
       "        [ 0.1242,  0.1584],\n",
       "        [ 0.4053,  0.4725],\n",
       "        [ 0.1588,  0.1931],\n",
       "        [ 0.4363,  0.5253],\n",
       "        [ 0.3391,  0.3742],\n",
       "        [ 0.4247,  0.5055],\n",
       "        [ 0.4234,  0.5032],\n",
       "        [ 0.2129,  0.2475],\n",
       "        [ 0.2374,  0.2721],\n",
       "        [-0.1253, -0.0982],\n",
       "        [ 0.4298,  0.5143],\n",
       "        [ 0.4162,  0.4910],\n",
       "        [ 0.3875,  0.4421],\n",
       "        [ 0.1244,  0.1587],\n",
       "        [-0.1309, -0.1006],\n",
       "        [ 0.0866,  0.1207],\n",
       "        [-0.1289, -0.0997],\n",
       "        [ 0.2342,  0.2689],\n",
       "        [ 0.2103,  0.2449],\n",
       "        [ 0.3750,  0.4211],\n",
       "        [-0.0201,  0.0134],\n",
       "        [ 0.4924,  0.6208],\n",
       "        [-0.0845, -0.0512],\n",
       "        [ 0.0442,  0.0781],\n",
       "        [ 0.4402,  0.5319],\n",
       "        [ 0.3855,  0.4389],\n",
       "        [ 0.4278,  0.5107],\n",
       "        [-0.1325, -0.1012],\n",
       "        [ 0.4843,  0.6068],\n",
       "        [ 0.4045,  0.4711],\n",
       "        [-0.1266, -0.0935],\n",
       "        [-0.1317, -0.1009],\n",
       "        [ 0.1530,  0.1873],\n",
       "        [ 0.0080,  0.0416],\n",
       "        [ 0.4148,  0.4887],\n",
       "        [ 0.2488,  0.2835],\n",
       "        [-0.0878, -0.0545],\n",
       "        [-0.1272, -0.0990],\n",
       "        [ 0.5350,  0.6931],\n",
       "        [ 0.4859,  0.6096],\n",
       "        [-0.1297, -0.1001],\n",
       "        [ 0.4180,  0.4941],\n",
       "        [-0.1244, -0.0978],\n",
       "        [-0.0081,  0.0255],\n",
       "        [-0.0835, -0.0502],\n",
       "        [ 0.2035,  0.2380],\n",
       "        [ 0.0969,  0.1310],\n",
       "        [ 0.0055,  0.0392],\n",
       "        [ 0.4902,  0.6168],\n",
       "        [ 0.3574,  0.3926],\n",
       "        [-0.1257, -0.0984],\n",
       "        [ 0.3255,  0.3606],\n",
       "        [-0.1302, -0.1003],\n",
       "        [-0.1291, -0.0998],\n",
       "        [ 0.4579,  0.5620],\n",
       "        [-0.1291, -0.0998],\n",
       "        [ 0.3966,  0.4577],\n",
       "        [-0.0665, -0.0331],\n",
       "        [ 0.4863,  0.6103],\n",
       "        [-0.1090, -0.0758],\n",
       "        [-0.1247, -0.0979],\n",
       "        [ 0.4012,  0.4656],\n",
       "        [ 0.4381,  0.5282],\n",
       "        [ 0.4469,  0.5433],\n",
       "        [ 0.4173,  0.4929],\n",
       "        [ 0.1481,  0.1824],\n",
       "        [-0.1251, -0.0981],\n",
       "        [ 0.4064,  0.4743],\n",
       "        [-0.1234, -0.0974],\n",
       "        [ 0.4233,  0.5031],\n",
       "        [-0.1306, -0.1004],\n",
       "        [-0.1243, -0.0978],\n",
       "        [-0.0849, -0.0516],\n",
       "        [-0.0682, -0.0348],\n",
       "        [ 0.4656,  0.5750],\n",
       "        [ 0.4539,  0.5552],\n",
       "        [ 0.2181,  0.2527],\n",
       "        [ 0.4578,  0.5617],\n",
       "        [ 0.0305,  0.0642],\n",
       "        [ 0.5053,  0.6426],\n",
       "        [-0.1280, -0.0949],\n",
       "        [ 0.3438,  0.3789],\n",
       "        [ 0.4489,  0.5467],\n",
       "        [ 0.2483,  0.2830],\n",
       "        [ 0.4714,  0.5848],\n",
       "        [-0.1258, -0.0984],\n",
       "        [-0.1236, -0.0975],\n",
       "        [-0.0042,  0.0294],\n",
       "        [ 0.0216,  0.0554],\n",
       "        [ 0.2812,  0.3160],\n",
       "        [ 0.3582,  0.3934],\n",
       "        [ 0.3763,  0.4232],\n",
       "        [-0.1252, -0.0982],\n",
       "        [-0.1291, -0.0998],\n",
       "        [-0.1312, -0.1007],\n",
       "        [ 0.3927,  0.4511],\n",
       "        [-0.1309, -0.1006],\n",
       "        [ 0.1289,  0.1631],\n",
       "        [ 0.3924,  0.4506],\n",
       "        [-0.1260, -0.0985],\n",
       "        [ 0.2378,  0.2725],\n",
       "        [-0.1254, -0.0983],\n",
       "        [-0.1030, -0.0698],\n",
       "        [ 0.3850,  0.4380],\n",
       "        [-0.1321, -0.1011],\n",
       "        [ 0.3263,  0.3614],\n",
       "        [ 0.5285,  0.6821],\n",
       "        [-0.1321, -0.1011],\n",
       "        [ 0.4147,  0.4885],\n",
       "        [ 0.0985,  0.1325],\n",
       "        [ 0.2518,  0.2865],\n",
       "        [-0.1286, -0.0996],\n",
       "        [ 0.4685,  0.5800],\n",
       "        [-0.1245, -0.0979],\n",
       "        [-0.1233, -0.0974],\n",
       "        [-0.0940, -0.0608],\n",
       "        [ 0.3957,  0.4561],\n",
       "        [ 0.3966,  0.4576],\n",
       "        [-0.1304, -0.1004],\n",
       "        [-0.1257, -0.0984]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0882, 0.1805], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3829316391788403\n",
      "Valence RMSE: 0.3176099438398145\n",
      "Arousal RMSE: 0.43863105697437454\n",
      "Test R^2 score: tensor([-0.0780, -0.4189], dtype=torch.float64)\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3815076206326156\n",
      "Valence RMSE: 0.3168122681424735\n",
      "Arousal RMSE: 0.4367220122182313\n",
      "Test R^2 score: tensor([-0.0726, -0.4066], dtype=torch.float64)\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3801182492323225\n",
      "Valence RMSE: 0.3160526252295839\n",
      "Arousal RMSE: 0.4348453804335287\n",
      "Test R^2 score: tensor([-0.0674, -0.3945], dtype=torch.float64)\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.378763411836325\n",
      "Valence RMSE: 0.3153327851235013\n",
      "Arousal RMSE: 0.43299962923545654\n",
      "Test R^2 score: tensor([-0.0626, -0.3827], dtype=torch.float64)\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3774421895508644\n",
      "Valence RMSE: 0.3146567671944061\n",
      "Arousal RMSE: 0.43118016160842393\n",
      "Test R^2 score: tensor([-0.0580, -0.3711], dtype=torch.float64)\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37615388286760565\n",
      "Valence RMSE: 0.3140257719731031\n",
      "Arousal RMSE: 0.4293847944786231\n",
      "Test R^2 score: tensor([-0.0538, -0.3597], dtype=torch.float64)\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3748974328693431\n",
      "Valence RMSE: 0.31343929260303915\n",
      "Arousal RMSE: 0.4276119504837927\n",
      "Test R^2 score: tensor([-0.0498, -0.3485], dtype=torch.float64)\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3736713407171971\n",
      "Valence RMSE: 0.31289398362418974\n",
      "Arousal RMSE: 0.42586135861165014\n",
      "Test R^2 score: tensor([-0.0462, -0.3375], dtype=torch.float64)\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3724741530159041\n",
      "Valence RMSE: 0.31238491066837853\n",
      "Arousal RMSE: 0.4241340082055899\n",
      "Test R^2 score: tensor([-0.0428, -0.3267], dtype=torch.float64)\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37130455248573796\n",
      "Valence RMSE: 0.3119073710927526\n",
      "Arousal RMSE: 0.42243098045867356\n",
      "Test R^2 score: tensor([-0.0396, -0.3160], dtype=torch.float64)\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37016130326451413\n",
      "Valence RMSE: 0.3114578217854242\n",
      "Arousal RMSE: 0.4207526662038472\n",
      "Test R^2 score: tensor([-0.0366, -0.3056], dtype=torch.float64)\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3692878458045309\n",
      "Valence RMSE: 0.31110195317311373\n",
      "Arousal RMSE: 0.4194789635366423\n",
      "Test R^2 score: tensor([-0.0342, -0.2977], dtype=torch.float64)\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.368683390515426\n",
      "Valence RMSE: 0.31071421575791563\n",
      "Arousal RMSE: 0.418702234302426\n",
      "Test R^2 score: tensor([-0.0317, -0.2929], dtype=torch.float64)\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36807728956495833\n",
      "Valence RMSE: 0.31033422456134563\n",
      "Arousal RMSE: 0.41791679943845356\n",
      "Test R^2 score: tensor([-0.0291, -0.2881], dtype=torch.float64)\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3674720716266858\n",
      "Valence RMSE: 0.3099660117288426\n",
      "Arousal RMSE: 0.4171241043432173\n",
      "Test R^2 score: tensor([-0.0267, -0.2832], dtype=torch.float64)\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3668696055459032\n",
      "Valence RMSE: 0.3096121608339658\n",
      "Arousal RMSE: 0.4163255034351555\n",
      "Test R^2 score: tensor([-0.0244, -0.2783], dtype=torch.float64)\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3662712104401011\n",
      "Valence RMSE: 0.30927442492841856\n",
      "Arousal RMSE: 0.415521996144259\n",
      "Test R^2 score: tensor([-0.0221, -0.2733], dtype=torch.float64)\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3656777785396888\n",
      "Valence RMSE: 0.30895402565716457\n",
      "Arousal RMSE: 0.4147142214413143\n",
      "Test R^2 score: tensor([-0.0200, -0.2684], dtype=torch.float64)\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3650899957624731\n",
      "Valence RMSE: 0.3086518662038159\n",
      "Arousal RMSE: 0.41390268844329453\n",
      "Test R^2 score: tensor([-0.0180, -0.2634], dtype=torch.float64)\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.364508267436626\n",
      "Valence RMSE: 0.3083684610752393\n",
      "Arousal RMSE: 0.4130876980416996\n",
      "Test R^2 score: tensor([-0.0161, -0.2585], dtype=torch.float64)\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3639327658944404\n",
      "Valence RMSE: 0.30810400065877125\n",
      "Arousal RMSE: 0.4122693791214853\n",
      "Test R^2 score: tensor([-0.0144, -0.2535], dtype=torch.float64)\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36336353528641574\n",
      "Valence RMSE: 0.3078584329692851\n",
      "Arousal RMSE: 0.41144781297435623\n",
      "Test R^2 score: tensor([-0.0128, -0.2485], dtype=torch.float64)\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36280050832860566\n",
      "Valence RMSE: 0.3076314563203375\n",
      "Arousal RMSE: 0.41062306896863165\n",
      "Test R^2 score: tensor([-0.0113, -0.2435], dtype=torch.float64)\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.362243522501153\n",
      "Valence RMSE: 0.30742265884846454\n",
      "Arousal RMSE: 0.40979512932028267\n",
      "Test R^2 score: tensor([-0.0099, -0.2385], dtype=torch.float64)\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3616923071411773\n",
      "Valence RMSE: 0.30723146456135636\n",
      "Arousal RMSE: 0.4089639070550108\n",
      "Test R^2 score: tensor([-0.0087, -0.2335], dtype=torch.float64)\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3611465660225622\n",
      "Valence RMSE: 0.30705720716431983\n",
      "Arousal RMSE: 0.4081293371325146\n",
      "Test R^2 score: tensor([-0.0075, -0.2284], dtype=torch.float64)\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3606059761632885\n",
      "Valence RMSE: 0.30689915285442165\n",
      "Arousal RMSE: 0.4072913577116444\n",
      "Test R^2 score: tensor([-0.0065, -0.2234], dtype=torch.float64)\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3600701862439348\n",
      "Valence RMSE: 0.30675650143875216\n",
      "Arousal RMSE: 0.40644990696092015\n",
      "Test R^2 score: tensor([-0.0055, -0.2184], dtype=torch.float64)\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3595388492913789\n",
      "Valence RMSE: 0.30662845592108157\n",
      "Arousal RMSE: 0.4056049288642721\n",
      "Test R^2 score: tensor([-0.0047, -0.2133], dtype=torch.float64)\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3590116060986461\n",
      "Valence RMSE: 0.30651418327409324\n",
      "Arousal RMSE: 0.4047563737347127\n",
      "Test R^2 score: tensor([-0.0040, -0.2082], dtype=torch.float64)\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35848808952449224\n",
      "Valence RMSE: 0.3064128044418558\n",
      "Arousal RMSE: 0.403904213813025\n",
      "Test R^2 score: tensor([-0.0033, -0.2031], dtype=torch.float64)\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35796795665920306\n",
      "Valence RMSE: 0.30632348466098436\n",
      "Arousal RMSE: 0.4030484322444164\n",
      "Test R^2 score: tensor([-0.0027, -0.1980], dtype=torch.float64)\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3574508659541662\n",
      "Valence RMSE: 0.30624537382512407\n",
      "Arousal RMSE: 0.4021890278879778\n",
      "Test R^2 score: tensor([-0.0022, -0.1929], dtype=torch.float64)\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3569365109016145\n",
      "Valence RMSE: 0.30617768052901473\n",
      "Arousal RMSE: 0.4013260190607\n",
      "Test R^2 score: tensor([-0.0018, -0.1878], dtype=torch.float64)\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3564245822635816\n",
      "Valence RMSE: 0.30611955160499593\n",
      "Arousal RMSE: 0.4004594683718859\n",
      "Test R^2 score: tensor([-0.0014, -0.1827], dtype=torch.float64)\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3559147970715327\n",
      "Valence RMSE: 0.3060701931602141\n",
      "Arousal RMSE: 0.39958944231274407\n",
      "Test R^2 score: tensor([-0.0011, -0.1776], dtype=torch.float64)\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3554068657011622\n",
      "Valence RMSE: 0.3060287601021646\n",
      "Arousal RMSE: 0.39871603725631566\n",
      "Test R^2 score: tensor([-0.0008, -0.1724], dtype=torch.float64)\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35490054437724106\n",
      "Valence RMSE: 0.3059944677654796\n",
      "Arousal RMSE: 0.397839387813028\n",
      "Test R^2 score: tensor([-0.0006, -0.1673], dtype=torch.float64)\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.354395549254977\n",
      "Valence RMSE: 0.30596639688914545\n",
      "Arousal RMSE: 0.39695966374198216\n",
      "Test R^2 score: tensor([-0.0004, -0.1621], dtype=torch.float64)\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3538916262528904\n",
      "Valence RMSE: 0.30594377224871355\n",
      "Arousal RMSE: 0.39607698050512635\n",
      "Test R^2 score: tensor([-0.0002, -0.1570], dtype=torch.float64)\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3533885365117894\n",
      "Valence RMSE: 0.3059258220572162\n",
      "Arousal RMSE: 0.3951914812777536\n",
      "Test R^2 score: tensor([-1.0666e-04, -1.5179e-01], dtype=torch.float64)\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3528860469562801\n",
      "Valence RMSE: 0.3059117857770135\n",
      "Arousal RMSE: 0.39430331420820997\n",
      "Test R^2 score: tensor([-1.4888e-05, -1.4662e-01], dtype=torch.float64)\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35238395002190154\n",
      "Valence RMSE: 0.30590093328910206\n",
      "Arousal RMSE: 0.3934126529217538\n",
      "Test R^2 score: tensor([ 5.6064e-05, -1.4145e-01], dtype=torch.float64)\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35188205140911094\n",
      "Valence RMSE: 0.30589255496227374\n",
      "Arousal RMSE: 0.39251968234270845\n",
      "Test R^2 score: tensor([ 1.1084e-04, -1.3627e-01], dtype=torch.float64)\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35138022601342955\n",
      "Valence RMSE: 0.30588602325262015\n",
      "Arousal RMSE: 0.39162465096721794\n",
      "Test R^2 score: tensor([ 0.0002, -0.1311], dtype=torch.float64)\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.350878339091725\n",
      "Valence RMSE: 0.30588071945357936\n",
      "Arousal RMSE: 0.3907277865139562\n",
      "Test R^2 score: tensor([ 0.0002, -0.1259], dtype=torch.float64)\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3503763286117606\n",
      "Valence RMSE: 0.3058762027646873\n",
      "Arousal RMSE: 0.38982931121860087\n",
      "Test R^2 score: tensor([ 0.0002, -0.1207], dtype=torch.float64)\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3498741501231649\n",
      "Valence RMSE: 0.30587197305777863\n",
      "Arousal RMSE: 0.38892952825229843\n",
      "Test R^2 score: tensor([ 0.0002, -0.1156], dtype=torch.float64)\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3493717215741798\n",
      "Valence RMSE: 0.3058676783534076\n",
      "Arousal RMSE: 0.388028559528946\n",
      "Test R^2 score: tensor([ 0.0003, -0.1104], dtype=torch.float64)\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3488692387120745\n",
      "Valence RMSE: 0.3058630574732932\n",
      "Arousal RMSE: 0.38712695787321993\n",
      "Test R^2 score: tensor([ 0.0003, -0.1053], dtype=torch.float64)\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3483661192391597\n",
      "Valence RMSE: 0.30585584863404375\n",
      "Arousal RMSE: 0.3862254599632886\n",
      "Test R^2 score: tensor([ 0.0004, -0.1001], dtype=torch.float64)\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3478603005612302\n",
      "Valence RMSE: 0.30584263411079393\n",
      "Arousal RMSE: 0.385323060007145\n",
      "Test R^2 score: tensor([ 0.0004, -0.0950], dtype=torch.float64)\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3473562280416179\n",
      "Valence RMSE: 0.30583481544436963\n",
      "Arousal RMSE: 0.3844187352103294\n",
      "Test R^2 score: tensor([ 0.0005, -0.0899], dtype=torch.float64)\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3468775910785801\n",
      "Valence RMSE: 0.3058296291961336\n",
      "Arousal RMSE: 0.3835575110602225\n",
      "Test R^2 score: tensor([ 0.0005, -0.0850], dtype=torch.float64)\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3464060322449378\n",
      "Valence RMSE: 0.3058370920590606\n",
      "Arousal RMSE: 0.38269825120088996\n",
      "Test R^2 score: tensor([ 0.0005, -0.0801], dtype=torch.float64)\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3459449881354004\n",
      "Valence RMSE: 0.3058323906297381\n",
      "Arousal RMSE: 0.3818670167397336\n",
      "Test R^2 score: tensor([ 0.0005, -0.0754], dtype=torch.float64)\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3454902345164049\n",
      "Valence RMSE: 0.3058326487648921\n",
      "Arousal RMSE: 0.3810425110691077\n",
      "Test R^2 score: tensor([ 0.0005, -0.0708], dtype=torch.float64)\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34503798740906033\n",
      "Valence RMSE: 0.30582933689324315\n",
      "Arousal RMSE: 0.3802247259266931\n",
      "Test R^2 score: tensor([ 0.0005, -0.0662], dtype=torch.float64)\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3445864486045869\n",
      "Valence RMSE: 0.30582046895549536\n",
      "Arousal RMSE: 0.37941202127988005\n",
      "Test R^2 score: tensor([ 0.0006, -0.0617], dtype=torch.float64)\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.344137684273111\n",
      "Valence RMSE: 0.30580971423914804\n",
      "Arousal RMSE: 0.3786052167504952\n",
      "Test R^2 score: tensor([ 0.0007, -0.0571], dtype=torch.float64)\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34369109435275175\n",
      "Valence RMSE: 0.3057984185470862\n",
      "Arousal RMSE: 0.3778021491321685\n",
      "Test R^2 score: tensor([ 0.0007, -0.0527], dtype=torch.float64)\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.343246687470888\n",
      "Valence RMSE: 0.305792304977128\n",
      "Arousal RMSE: 0.37699820044166\n",
      "Test R^2 score: tensor([ 0.0008, -0.0482], dtype=torch.float64)\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34280641038726195\n",
      "Valence RMSE: 0.3057900767225835\n",
      "Arousal RMSE: 0.37619795185938504\n",
      "Test R^2 score: tensor([ 0.0008, -0.0437], dtype=torch.float64)\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34237098576925096\n",
      "Valence RMSE: 0.30579019410303887\n",
      "Arousal RMSE: 0.37540397038875745\n",
      "Test R^2 score: tensor([ 0.0008, -0.0393], dtype=torch.float64)\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34194147455847634\n",
      "Valence RMSE: 0.30579273455370964\n",
      "Arousal RMSE: 0.3746181356269538\n",
      "Test R^2 score: tensor([ 0.0008, -0.0350], dtype=torch.float64)\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34151867037910244\n",
      "Valence RMSE: 0.3057979837373719\n",
      "Arousal RMSE: 0.3738416744788869\n",
      "Test R^2 score: tensor([ 0.0007, -0.0307], dtype=torch.float64)\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34110406128283993\n",
      "Valence RMSE: 0.30580645502898\n",
      "Arousal RMSE: 0.37307689999503224\n",
      "Test R^2 score: tensor([ 0.0007, -0.0265], dtype=torch.float64)\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34069894870863454\n",
      "Valence RMSE: 0.30581778869127424\n",
      "Arousal RMSE: 0.37232650647290316\n",
      "Test R^2 score: tensor([ 0.0006, -0.0224], dtype=torch.float64)\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3403042580995189\n",
      "Valence RMSE: 0.3058322125307276\n",
      "Arousal RMSE: 0.3715920262060098\n",
      "Test R^2 score: tensor([ 0.0005, -0.0183], dtype=torch.float64)\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33992185047951917\n",
      "Valence RMSE: 0.30584962540552496\n",
      "Arousal RMSE: 0.3708769816342627\n",
      "Test R^2 score: tensor([ 0.0004, -0.0144], dtype=torch.float64)\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33955401466808466\n",
      "Valence RMSE: 0.3058701457101461\n",
      "Arousal RMSE: 0.3701855098699593\n",
      "Test R^2 score: tensor([ 0.0003, -0.0106], dtype=torch.float64)\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3392050050597231\n",
      "Valence RMSE: 0.3058981946173243\n",
      "Arousal RMSE: 0.36952180645395627\n",
      "Test R^2 score: tensor([ 7.3968e-05, -7.0238e-03], dtype=torch.float64)\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33887499263751575\n",
      "Valence RMSE: 0.30593450187387716\n",
      "Arousal RMSE: 0.3688856216137128\n",
      "Test R^2 score: tensor([-0.0002, -0.0036], dtype=torch.float64)\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3385603546766976\n",
      "Valence RMSE: 0.3059764402698833\n",
      "Arousal RMSE: 0.36827251528914434\n",
      "Test R^2 score: tensor([-0.0004, -0.0002], dtype=torch.float64)\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382586472629775\n",
      "Valence RMSE: 0.306019545399192\n",
      "Arousal RMSE: 0.3676817410887175\n",
      "Test R^2 score: tensor([-0.0007,  0.0030], dtype=torch.float64)\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3379705559879011\n",
      "Valence RMSE: 0.30605437067880736\n",
      "Arousal RMSE: 0.36712248040393974\n",
      "Test R^2 score: tensor([-0.0009,  0.0060], dtype=torch.float64)\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33770298924264675\n",
      "Valence RMSE: 0.3060887416799836\n",
      "Arousal RMSE: 0.36660100941432594\n",
      "Test R^2 score: tensor([-0.0012,  0.0088], dtype=torch.float64)\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3374589551591157\n",
      "Valence RMSE: 0.3061356533479086\n",
      "Arousal RMSE: 0.3661120792645514\n",
      "Test R^2 score: tensor([-0.0015,  0.0115], dtype=torch.float64)\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3372360945266441\n",
      "Valence RMSE: 0.30618447453810166\n",
      "Arousal RMSE: 0.3656602719123228\n",
      "Test R^2 score: tensor([-0.0018,  0.0139], dtype=torch.float64)\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33701651874568717\n",
      "Valence RMSE: 0.30622167460396693\n",
      "Arousal RMSE: 0.36522397760506686\n",
      "Test R^2 score: tensor([-0.0020,  0.0163], dtype=torch.float64)\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33679231058961323\n",
      "Valence RMSE: 0.3062542525747899\n",
      "Arousal RMSE: 0.36478274866615856\n",
      "Test R^2 score: tensor([-0.0023,  0.0186], dtype=torch.float64)\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33655537178266925\n",
      "Valence RMSE: 0.3062934559356471\n",
      "Arousal RMSE: 0.3643121675192022\n",
      "Test R^2 score: tensor([-0.0025,  0.0212], dtype=torch.float64)\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33628407772348573\n",
      "Valence RMSE: 0.3063182670154312\n",
      "Arousal RMSE: 0.3637898859964826\n",
      "Test R^2 score: tensor([-0.0027,  0.0240], dtype=torch.float64)\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33598015948636684\n",
      "Valence RMSE: 0.30634358884149976\n",
      "Arousal RMSE: 0.36320647118777927\n",
      "Test R^2 score: tensor([-0.0028,  0.0271], dtype=torch.float64)\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3356386002715633\n",
      "Valence RMSE: 0.3063729628236987\n",
      "Arousal RMSE: 0.36254951059839674\n",
      "Test R^2 score: tensor([-0.0030,  0.0306], dtype=torch.float64)\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3352564901803991\n",
      "Valence RMSE: 0.3064050154077892\n",
      "Arousal RMSE: 0.36181458642392045\n",
      "Test R^2 score: tensor([-0.0032,  0.0345], dtype=torch.float64)\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3348147766671287\n",
      "Valence RMSE: 0.3064284415799014\n",
      "Arousal RMSE: 0.3609757326195647\n",
      "Test R^2 score: tensor([-0.0034,  0.0390], dtype=torch.float64)\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3343088464508614\n",
      "Valence RMSE: 0.30644655670568866\n",
      "Arousal RMSE: 0.3600212736962059\n",
      "Test R^2 score: tensor([-0.0035,  0.0441], dtype=torch.float64)\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33374447663379425\n",
      "Valence RMSE: 0.30645594898570083\n",
      "Arousal RMSE: 0.3589644866813485\n",
      "Test R^2 score: tensor([-0.0036,  0.0497], dtype=torch.float64)\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33311098131531675\n",
      "Valence RMSE: 0.3064518667450755\n",
      "Arousal RMSE: 0.3577891908850309\n",
      "Test R^2 score: tensor([-0.0035,  0.0559], dtype=torch.float64)\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33241879863779833\n",
      "Valence RMSE: 0.30641815945447437\n",
      "Arousal RMSE: 0.35652829751946274\n",
      "Test R^2 score: tensor([-0.0033,  0.0626], dtype=torch.float64)\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33166533402248694\n",
      "Valence RMSE: 0.30635039911251577\n",
      "Arousal RMSE: 0.3551805464099884\n",
      "Test R^2 score: tensor([-0.0029,  0.0696], dtype=torch.float64)\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33086128170882284\n",
      "Valence RMSE: 0.30624100652316083\n",
      "Arousal RMSE: 0.3537722733506562\n",
      "Test R^2 score: tensor([-0.0022,  0.0770], dtype=torch.float64)\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3299905631017301\n",
      "Valence RMSE: 0.30608557073103493\n",
      "Arousal RMSE: 0.35227711657536087\n",
      "Test R^2 score: tensor([-0.0012,  0.0848], dtype=torch.float64)\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3290522674457441\n",
      "Valence RMSE: 0.305883125575396\n",
      "Arousal RMSE: 0.35069403033213653\n",
      "Test R^2 score: tensor([0.0002, 0.0930], dtype=torch.float64)\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3280410028091715\n",
      "Valence RMSE: 0.3056091587871168\n",
      "Arousal RMSE: 0.34903415465183996\n",
      "Test R^2 score: tensor([0.0020, 0.1015], dtype=torch.float64)\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3269489853739776\n",
      "Valence RMSE: 0.3052380074688434\n",
      "Arousal RMSE: 0.3473053942434485\n",
      "Test R^2 score: tensor([0.0044, 0.1104], dtype=torch.float64)\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32577575321222463\n",
      "Valence RMSE: 0.3047649488762014\n",
      "Arousal RMSE: 0.34551122803531414\n",
      "Test R^2 score: tensor([0.0075, 0.1196], dtype=torch.float64)\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32452179295675476\n",
      "Valence RMSE: 0.30418235519495007\n",
      "Arousal RMSE: 0.3436595451835827\n",
      "Test R^2 score: tensor([0.0113, 0.1290], dtype=torch.float64)\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32320459084160474\n",
      "Valence RMSE: 0.30349530768376043\n",
      "Arousal RMSE: 0.3417791879212627\n",
      "Test R^2 score: tensor([0.0157, 0.1385], dtype=torch.float64)\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3218422809064984\n",
      "Valence RMSE: 0.3027152018182801\n",
      "Arousal RMSE: 0.339894710382955\n",
      "Test R^2 score: tensor([0.0208, 0.1480], dtype=torch.float64)\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32046238469833777\n",
      "Valence RMSE: 0.301875801934259\n",
      "Arousal RMSE: 0.3380285198317427\n",
      "Test R^2 score: tensor([0.0262, 0.1573], dtype=torch.float64)\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3190697237523714\n",
      "Valence RMSE: 0.30097227898576273\n",
      "Arousal RMSE: 0.33619438501103105\n",
      "Test R^2 score: tensor([0.0320, 0.1664], dtype=torch.float64)\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3176698797562265\n",
      "Valence RMSE: 0.3000173369644416\n",
      "Arousal RMSE: 0.3343918398068882\n",
      "Test R^2 score: tensor([0.0382, 0.1753], dtype=torch.float64)\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3162636201242379\n",
      "Valence RMSE: 0.29899228064156086\n",
      "Arousal RMSE: 0.33263940077046594\n",
      "Test R^2 score: tensor([0.0447, 0.1840], dtype=torch.float64)\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3148610977070609\n",
      "Valence RMSE: 0.2979018967041168\n",
      "Arousal RMSE: 0.33095238575765007\n",
      "Test R^2 score: tensor([0.0517, 0.1922], dtype=torch.float64)\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3134829315055465\n",
      "Valence RMSE: 0.29678478446902096\n",
      "Arousal RMSE: 0.32933552556367063\n",
      "Test R^2 score: tensor([0.0588, 0.2001], dtype=torch.float64)\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31214213146085\n",
      "Valence RMSE: 0.29566804943979685\n",
      "Arousal RMSE: 0.3277892997129574\n",
      "Test R^2 score: tensor([0.0658, 0.2076], dtype=torch.float64)\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31082641740353006\n",
      "Valence RMSE: 0.2945367053046927\n",
      "Arousal RMSE: 0.32630392694554505\n",
      "Test R^2 score: tensor([0.0730, 0.2148], dtype=torch.float64)\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3095313047581721\n",
      "Valence RMSE: 0.29336748140367985\n",
      "Arousal RMSE: 0.32489194835430185\n",
      "Test R^2 score: tensor([0.0803, 0.2215], dtype=torch.float64)\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30828782988104053\n",
      "Valence RMSE: 0.29222609135369354\n",
      "Arousal RMSE: 0.3235532160830202\n",
      "Test R^2 score: tensor([0.0875, 0.2279], dtype=torch.float64)\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30704184829668985\n",
      "Valence RMSE: 0.29107189599926336\n",
      "Arousal RMSE: 0.32222126647738353\n",
      "Test R^2 score: tensor([0.0947, 0.2343], dtype=torch.float64)\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30580829867781295\n",
      "Valence RMSE: 0.2899013181049555\n",
      "Arousal RMSE: 0.32092780627649936\n",
      "Test R^2 score: tensor([0.1019, 0.2404], dtype=torch.float64)\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3045889500296313\n",
      "Valence RMSE: 0.2887522248868486\n",
      "Arousal RMSE: 0.3196420022199856\n",
      "Test R^2 score: tensor([0.1090, 0.2465], dtype=torch.float64)\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30342207832758955\n",
      "Valence RMSE: 0.2876484127696085\n",
      "Arousal RMSE: 0.31841530406749097\n",
      "Test R^2 score: tensor([0.1158, 0.2523], dtype=torch.float64)\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30230327343470764\n",
      "Valence RMSE: 0.2865424853351615\n",
      "Arousal RMSE: 0.317282117927607\n",
      "Test R^2 score: tensor([0.1226, 0.2576], dtype=torch.float64)\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30119171444340903\n",
      "Valence RMSE: 0.2853457663786933\n",
      "Arousal RMSE: 0.3162446700080119\n",
      "Test R^2 score: tensor([0.1299, 0.2624], dtype=torch.float64)\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30006732464317737\n",
      "Valence RMSE: 0.28402510543172177\n",
      "Arousal RMSE: 0.31529436741167344\n",
      "Test R^2 score: tensor([0.1380, 0.2669], dtype=torch.float64)\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2990121153861007\n",
      "Valence RMSE: 0.2827641757209868\n",
      "Arousal RMSE: 0.31442155019046086\n",
      "Test R^2 score: tensor([0.1456, 0.2709], dtype=torch.float64)\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29797446245008535\n",
      "Valence RMSE: 0.2815780775404516\n",
      "Arousal RMSE: 0.31351450810681486\n",
      "Test R^2 score: tensor([0.1527, 0.2751], dtype=torch.float64)\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29694977626968255\n",
      "Valence RMSE: 0.2805144367784679\n",
      "Arousal RMSE: 0.3125219832461198\n",
      "Test R^2 score: tensor([0.1591, 0.2797], dtype=torch.float64)\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2959934687415044\n",
      "Valence RMSE: 0.2795773411960283\n",
      "Arousal RMSE: 0.31154578694794677\n",
      "Test R^2 score: tensor([0.1647, 0.2842], dtype=torch.float64)\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2951140752946225\n",
      "Valence RMSE: 0.2787153722618278\n",
      "Arousal RMSE: 0.31064831584760116\n",
      "Test R^2 score: tensor([0.1699, 0.2883], dtype=torch.float64)\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2942962433392941\n",
      "Valence RMSE: 0.27786691874831043\n",
      "Arousal RMSE: 0.30985566503222406\n",
      "Test R^2 score: tensor([0.1749, 0.2919], dtype=torch.float64)\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2935815904495503\n",
      "Valence RMSE: 0.2770044382367608\n",
      "Arousal RMSE: 0.30927146926108706\n",
      "Test R^2 score: tensor([0.1800, 0.2946], dtype=torch.float64)\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929228448727869\n",
      "Valence RMSE: 0.2761953494972992\n",
      "Arousal RMSE: 0.30874538865025714\n",
      "Test R^2 score: tensor([0.1848, 0.2970], dtype=torch.float64)\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2923762893943542\n",
      "Valence RMSE: 0.27556969781694374\n",
      "Arousal RMSE: 0.3082679529972271\n",
      "Test R^2 score: tensor([0.1885, 0.2992], dtype=torch.float64)\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2918866666722439\n",
      "Valence RMSE: 0.27512134742670796\n",
      "Arousal RMSE: 0.3077399820500741\n",
      "Test R^2 score: tensor([0.1912, 0.3016], dtype=torch.float64)\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.291465004748596\n",
      "Valence RMSE: 0.27467889726972694\n",
      "Arousal RMSE: 0.3073356493817223\n",
      "Test R^2 score: tensor([0.1938, 0.3034], dtype=torch.float64)\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2910398992685829\n",
      "Valence RMSE: 0.27401719291906396\n",
      "Arousal RMSE: 0.3071205364629501\n",
      "Test R^2 score: tensor([0.1976, 0.3044], dtype=torch.float64)\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2906816906484499\n",
      "Valence RMSE: 0.2734172485244689\n",
      "Arousal RMSE: 0.30697670720396864\n",
      "Test R^2 score: tensor([0.2011, 0.3050], dtype=torch.float64)\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29036645900382235\n",
      "Valence RMSE: 0.273087534715602\n",
      "Arousal RMSE: 0.30667337577916953\n",
      "Test R^2 score: tensor([0.2031, 0.3064], dtype=torch.float64)\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29017366051475363\n",
      "Valence RMSE: 0.2728517224130523\n",
      "Arousal RMSE: 0.30651826061312176\n",
      "Test R^2 score: tensor([0.2045, 0.3071], dtype=torch.float64)\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2899133424450111\n",
      "Valence RMSE: 0.2723218658280332\n",
      "Arousal RMSE: 0.30649680854327865\n",
      "Test R^2 score: tensor([0.2075, 0.3072], dtype=torch.float64)\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.289786277588123\n",
      "Valence RMSE: 0.2720196144975567\n",
      "Arousal RMSE: 0.30652488102169123\n",
      "Test R^2 score: tensor([0.2093, 0.3071], dtype=torch.float64)\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2897934492076325\n",
      "Valence RMSE: 0.27202856462350317\n",
      "Arousal RMSE: 0.3065304983785299\n",
      "Test R^2 score: tensor([0.2092, 0.3070], dtype=torch.float64)\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28968243917069036\n",
      "Valence RMSE: 0.2716841773575972\n",
      "Arousal RMSE: 0.30662605711401486\n",
      "Test R^2 score: tensor([0.2112, 0.3066], dtype=torch.float64)\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28942574018841283\n",
      "Valence RMSE: 0.2710221706133357\n",
      "Arousal RMSE: 0.30672707934458154\n",
      "Test R^2 score: tensor([0.2151, 0.3062], dtype=torch.float64)\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28936739148047635\n",
      "Valence RMSE: 0.2708412078660862\n",
      "Arousal RMSE: 0.30677681565931614\n",
      "Test R^2 score: tensor([0.2161, 0.3059], dtype=torch.float64)\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2892892456800021\n",
      "Valence RMSE: 0.27060355359010835\n",
      "Arousal RMSE: 0.306839130680255\n",
      "Test R^2 score: tensor([0.2175, 0.3056], dtype=torch.float64)\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2891738333884623\n",
      "Valence RMSE: 0.27029471992419524\n",
      "Arousal RMSE: 0.3068937539511959\n",
      "Test R^2 score: tensor([0.2193, 0.3054], dtype=torch.float64)\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2891979015091391\n",
      "Valence RMSE: 0.27025699700029604\n",
      "Arousal RMSE: 0.3069723245619406\n",
      "Test R^2 score: tensor([0.2195, 0.3050], dtype=torch.float64)\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28898458300818586\n",
      "Valence RMSE: 0.26971112218304605\n",
      "Arousal RMSE: 0.30705062938152755\n",
      "Test R^2 score: tensor([0.2227, 0.3047], dtype=torch.float64)\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2889547755093251\n",
      "Valence RMSE: 0.26954151137629334\n",
      "Arousal RMSE: 0.3071434489359531\n",
      "Test R^2 score: tensor([0.2236, 0.3043], dtype=torch.float64)\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28885425300516776\n",
      "Valence RMSE: 0.2692493680357019\n",
      "Arousal RMSE: 0.30721057398911594\n",
      "Test R^2 score: tensor([0.2253, 0.3040], dtype=torch.float64)\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2885091658771479\n",
      "Valence RMSE: 0.26852889034299826\n",
      "Arousal RMSE: 0.3071926311639218\n",
      "Test R^2 score: tensor([0.2295, 0.3040], dtype=torch.float64)\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28881491946467736\n",
      "Valence RMSE: 0.26908796199149365\n",
      "Arousal RMSE: 0.30727802414432537\n",
      "Test R^2 score: tensor([0.2262, 0.3037], dtype=torch.float64)\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.288483383302199\n",
      "Valence RMSE: 0.2684672359157372\n",
      "Arousal RMSE: 0.30719809264175274\n",
      "Test R^2 score: tensor([0.2298, 0.3040], dtype=torch.float64)\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882625191236871\n",
      "Valence RMSE: 0.26803329068484427\n",
      "Arousal RMSE: 0.3071623592625273\n",
      "Test R^2 score: tensor([0.2323, 0.3042], dtype=torch.float64)\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28848171781145204\n",
      "Valence RMSE: 0.2683906403358392\n",
      "Arousal RMSE: 0.3072618870003417\n",
      "Test R^2 score: tensor([0.2303, 0.3037], dtype=torch.float64)\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28806324832395136\n",
      "Valence RMSE: 0.26752717049378\n",
      "Arousal RMSE: 0.3072296911391946\n",
      "Test R^2 score: tensor([0.2352, 0.3039], dtype=torch.float64)\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879491673764928\n",
      "Valence RMSE: 0.2671932447204277\n",
      "Arousal RMSE: 0.30730638776537095\n",
      "Test R^2 score: tensor([0.2371, 0.3035], dtype=torch.float64)\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2880842955144799\n",
      "Valence RMSE: 0.2673424739261453\n",
      "Arousal RMSE: 0.3074298688793863\n",
      "Test R^2 score: tensor([0.2363, 0.3030], dtype=torch.float64)\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28796574769223743\n",
      "Valence RMSE: 0.26700827584181736\n",
      "Arousal RMSE: 0.3074981696203713\n",
      "Test R^2 score: tensor([0.2382, 0.3027], dtype=torch.float64)\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882790914756246\n",
      "Valence RMSE: 0.2674157481138524\n",
      "Arousal RMSE: 0.30773119247929953\n",
      "Test R^2 score: tensor([0.2358, 0.3016], dtype=torch.float64)\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2877289050531618\n",
      "Valence RMSE: 0.2662344348977469\n",
      "Arousal RMSE: 0.30772564287179605\n",
      "Test R^2 score: tensor([0.2426, 0.3016], dtype=torch.float64)\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28828849924592215\n",
      "Valence RMSE: 0.26709833316531034\n",
      "Arousal RMSE: 0.3080243464650886\n",
      "Test R^2 score: tensor([0.2376, 0.3003], dtype=torch.float64)\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2874519221075196\n",
      "Valence RMSE: 0.2656431961755618\n",
      "Arousal RMSE: 0.30771887717890656\n",
      "Test R^2 score: tensor([0.2459, 0.3017], dtype=torch.float64)\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882433872503134\n",
      "Valence RMSE: 0.2671342709036991\n",
      "Arousal RMSE: 0.3079087233188066\n",
      "Test R^2 score: tensor([0.2374, 0.3008], dtype=torch.float64)\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28794264082085785\n",
      "Valence RMSE: 0.2666075550801094\n",
      "Arousal RMSE: 0.3078024372547845\n",
      "Test R^2 score: tensor([0.2404, 0.3013], dtype=torch.float64)\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28762851108571863\n",
      "Valence RMSE: 0.26595846785252275\n",
      "Arousal RMSE: 0.3077765653137245\n",
      "Test R^2 score: tensor([0.2441, 0.3014], dtype=torch.float64)\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2883333533038241\n",
      "Valence RMSE: 0.267116082172252\n",
      "Arousal RMSE: 0.3080929143940224\n",
      "Test R^2 score: tensor([0.2375, 0.3000], dtype=torch.float64)\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2877481710964235\n",
      "Valence RMSE: 0.26600153099308516\n",
      "Arousal RMSE: 0.3079629936339884\n",
      "Test R^2 score: tensor([0.2439, 0.3006], dtype=torch.float64)\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2876716867302983\n",
      "Valence RMSE: 0.26588809623449866\n",
      "Arousal RMSE: 0.3079180393762336\n",
      "Test R^2 score: tensor([0.2445, 0.3008], dtype=torch.float64)\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2882908373464193\n",
      "Valence RMSE: 0.267087521404394\n",
      "Arousal RMSE: 0.3080380978156055\n",
      "Test R^2 score: tensor([0.2377, 0.3002], dtype=torch.float64)\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28738913710351366\n",
      "Valence RMSE: 0.2655273074371726\n",
      "Arousal RMSE: 0.30770161074549085\n",
      "Test R^2 score: tensor([0.2466, 0.3017], dtype=torch.float64)\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28833551529368023\n",
      "Valence RMSE: 0.2672569339359563\n",
      "Arousal RMSE: 0.3079747879656647\n",
      "Test R^2 score: tensor([0.2367, 0.3005], dtype=torch.float64)\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28760161143486346\n",
      "Valence RMSE: 0.2659180850960976\n",
      "Arousal RMSE: 0.30776118309280803\n",
      "Test R^2 score: tensor([0.2444, 0.3015], dtype=torch.float64)\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28763455048495273\n",
      "Valence RMSE: 0.2659876989151405\n",
      "Arousal RMSE: 0.30776259241693127\n",
      "Test R^2 score: tensor([0.2440, 0.3015], dtype=torch.float64)\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2883237014003148\n",
      "Valence RMSE: 0.2672848027533163\n",
      "Arousal RMSE: 0.30792847837684095\n",
      "Test R^2 score: tensor([0.2366, 0.3007], dtype=torch.float64)\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2872241661552604\n",
      "Valence RMSE: 0.26544312825273764\n",
      "Arousal RMSE: 0.3074660776582841\n",
      "Test R^2 score: tensor([0.2471, 0.3028], dtype=torch.float64)\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879213151762973\n",
      "Valence RMSE: 0.2668557683109012\n",
      "Arousal RMSE: 0.30754734007124185\n",
      "Test R^2 score: tensor([0.2390, 0.3024], dtype=torch.float64)\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2873567913112259\n",
      "Valence RMSE: 0.2659560585160426\n",
      "Arousal RMSE: 0.3072706070615577\n",
      "Test R^2 score: tensor([0.2442, 0.3037], dtype=torch.float64)\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28677080907696983\n",
      "Valence RMSE: 0.2650444169216497\n",
      "Arousal RMSE: 0.30696327294316905\n",
      "Test R^2 score: tensor([0.2493, 0.3051], dtype=torch.float64)\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2872441236879416\n",
      "Valence RMSE: 0.26609551075806853\n",
      "Arousal RMSE: 0.3069390042678017\n",
      "Test R^2 score: tensor([0.2434, 0.3052], dtype=torch.float64)\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2864720839367878\n",
      "Valence RMSE: 0.26493760368737\n",
      "Arousal RMSE: 0.30649726899691293\n",
      "Test R^2 score: tensor([0.2499, 0.3072], dtype=torch.float64)\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28660606412940837\n",
      "Valence RMSE: 0.26534758335842523\n",
      "Arousal RMSE: 0.3063931004401775\n",
      "Test R^2 score: tensor([0.2476, 0.3077], dtype=torch.float64)\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2863033303213568\n",
      "Valence RMSE: 0.26489293658120444\n",
      "Arousal RMSE: 0.30622038804688684\n",
      "Test R^2 score: tensor([0.2502, 0.3084], dtype=torch.float64)\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28610852019391947\n",
      "Valence RMSE: 0.2645743461914968\n",
      "Arousal RMSE: 0.3061316481392456\n",
      "Test R^2 score: tensor([0.2520, 0.3088], dtype=torch.float64)\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28590689867135144\n",
      "Valence RMSE: 0.2642907804604941\n",
      "Arousal RMSE: 0.3059998248027665\n",
      "Test R^2 score: tensor([0.2536, 0.3094], dtype=torch.float64)\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28586480666105324\n",
      "Valence RMSE: 0.2643385836527484\n",
      "Arousal RMSE: 0.3058798596952432\n",
      "Test R^2 score: tensor([0.2533, 0.3100], dtype=torch.float64)\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2857859483090722\n",
      "Valence RMSE: 0.26429313827103396\n",
      "Arousal RMSE: 0.30577173441094824\n",
      "Test R^2 score: tensor([0.2536, 0.3105], dtype=torch.float64)\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28579564035946464\n",
      "Valence RMSE: 0.2643242616034539\n",
      "Arousal RMSE: 0.30576294874418936\n",
      "Test R^2 score: tensor([0.2534, 0.3105], dtype=torch.float64)\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28569754426429095\n",
      "Valence RMSE: 0.26410605787541264\n",
      "Arousal RMSE: 0.305768153656986\n",
      "Test R^2 score: tensor([0.2546, 0.3105], dtype=torch.float64)\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28583488484067315\n",
      "Valence RMSE: 0.26433425590241233\n",
      "Arousal RMSE: 0.30582767033131547\n",
      "Test R^2 score: tensor([0.2533, 0.3102], dtype=torch.float64)\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28563537863889155\n",
      "Valence RMSE: 0.2640021466924524\n",
      "Arousal RMSE: 0.305741730226907\n",
      "Test R^2 score: tensor([0.2552, 0.3106], dtype=torch.float64)\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28622353607979156\n",
      "Valence RMSE: 0.2650699890844947\n",
      "Arousal RMSE: 0.3059178420733014\n",
      "Test R^2 score: tensor([0.2492, 0.3098], dtype=torch.float64)\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28526476382485605\n",
      "Valence RMSE: 0.26320876311062785\n",
      "Arousal RMSE: 0.30573373706850737\n",
      "Test R^2 score: tensor([0.2597, 0.3106], dtype=torch.float64)\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2874096978466854\n",
      "Valence RMSE: 0.266928905289133\n",
      "Arousal RMSE: 0.30652508601057604\n",
      "Test R^2 score: tensor([0.2386, 0.3071], dtype=torch.float64)\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2848615667519758\n",
      "Valence RMSE: 0.2623395437817343\n",
      "Arousal RMSE: 0.30572894562532393\n",
      "Test R^2 score: tensor([0.2646, 0.3107], dtype=torch.float64)\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28662020363288876\n",
      "Valence RMSE: 0.26556459391609466\n",
      "Arousal RMSE: 0.306231495309181\n",
      "Test R^2 score: tensor([0.2464, 0.3084], dtype=torch.float64)\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28668990280247114\n",
      "Valence RMSE: 0.26576655807603317\n",
      "Arousal RMSE: 0.3061867687314382\n",
      "Test R^2 score: tensor([0.2452, 0.3086], dtype=torch.float64)\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2850349377472079\n",
      "Valence RMSE: 0.26280127323975366\n",
      "Arousal RMSE: 0.3056555614685813\n",
      "Test R^2 score: tensor([0.2620, 0.3110], dtype=torch.float64)\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2860782357193477\n",
      "Valence RMSE: 0.2646717883767682\n",
      "Arousal RMSE: 0.30599078146577\n",
      "Test R^2 score: tensor([0.2514, 0.3095], dtype=torch.float64)\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28711395424595293\n",
      "Valence RMSE: 0.2663412500793238\n",
      "Arousal RMSE: 0.30648194718723226\n",
      "Test R^2 score: tensor([0.2420, 0.3073], dtype=torch.float64)\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2855981597691848\n",
      "Valence RMSE: 0.26359138827389633\n",
      "Arousal RMSE: 0.3060264657753144\n",
      "Test R^2 score: tensor([0.2575, 0.3093], dtype=torch.float64)\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2861094069996141\n",
      "Valence RMSE: 0.26435637367627235\n",
      "Arousal RMSE: 0.3063215520398014\n",
      "Test R^2 score: tensor([0.2532, 0.3080], dtype=torch.float64)\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28796367748827734\n",
      "Valence RMSE: 0.26741862572169217\n",
      "Arousal RMSE: 0.3071374899328747\n",
      "Test R^2 score: tensor([0.2358, 0.3043], dtype=torch.float64)\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28712031428746426\n",
      "Valence RMSE: 0.26588940218030455\n",
      "Arousal RMSE: 0.3068859324916422\n",
      "Test R^2 score: tensor([0.2445, 0.3054], dtype=torch.float64)\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2870239537912849\n",
      "Valence RMSE: 0.26555605891819994\n",
      "Arousal RMSE: 0.3069942665129054\n",
      "Test R^2 score: tensor([0.2464, 0.3049], dtype=torch.float64)\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28855272003363397\n",
      "Valence RMSE: 0.2680787107741652\n",
      "Arousal RMSE: 0.3076672704518292\n",
      "Test R^2 score: tensor([0.2320, 0.3019], dtype=torch.float64)\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28860946113827496\n",
      "Valence RMSE: 0.2679571593304896\n",
      "Arousal RMSE: 0.30787952656937995\n",
      "Test R^2 score: tensor([0.2327, 0.3009], dtype=torch.float64)\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2876940102886438\n",
      "Valence RMSE: 0.26613070649791937\n",
      "Arousal RMSE: 0.3077501164432645\n",
      "Test R^2 score: tensor([0.2432, 0.3015], dtype=torch.float64)\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28894656296887283\n",
      "Valence RMSE: 0.2679979675031424\n",
      "Arousal RMSE: 0.30847580442756645\n",
      "Test R^2 score: tensor([0.2325, 0.2982], dtype=torch.float64)\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2901874227434622\n",
      "Valence RMSE: 0.2698481478012816\n",
      "Arousal RMSE: 0.3091916198172315\n",
      "Test R^2 score: tensor([0.2219, 0.2950], dtype=torch.float64)\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2894094198150833\n",
      "Valence RMSE: 0.26837591228773217\n",
      "Arousal RMSE: 0.3090145534746442\n",
      "Test R^2 score: tensor([0.2303, 0.2958], dtype=torch.float64)\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29016438682732876\n",
      "Valence RMSE: 0.26952779976429775\n",
      "Arousal RMSE: 0.3094277103298539\n",
      "Test R^2 score: tensor([0.2237, 0.2939], dtype=torch.float64)\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2914773191699309\n",
      "Valence RMSE: 0.27163869544461\n",
      "Arousal RMSE: 0.31004914822997004\n",
      "Test R^2 score: tensor([0.2115, 0.2910], dtype=torch.float64)\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2909681002416308\n",
      "Valence RMSE: 0.2705245867682062\n",
      "Arousal RMSE: 0.31006663585484107\n",
      "Test R^2 score: tensor([0.2180, 0.2910], dtype=torch.float64)\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29079568303774345\n",
      "Valence RMSE: 0.2699401533617037\n",
      "Arousal RMSE: 0.3102524329474877\n",
      "Test R^2 score: tensor([0.2213, 0.2901], dtype=torch.float64)\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29178087034368894\n",
      "Valence RMSE: 0.27132689247054687\n",
      "Arousal RMSE: 0.310892055252812\n",
      "Test R^2 score: tensor([0.2133, 0.2872], dtype=torch.float64)\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29214314470058333\n",
      "Valence RMSE: 0.2717507056193824\n",
      "Arousal RMSE: 0.3112021657804773\n",
      "Test R^2 score: tensor([0.2109, 0.2858], dtype=torch.float64)\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2920026166188645\n",
      "Valence RMSE: 0.27149974968933593\n",
      "Arousal RMSE: 0.31115742341000807\n",
      "Test R^2 score: tensor([0.2123, 0.2860], dtype=torch.float64)\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29266253699773204\n",
      "Valence RMSE: 0.27260788191084806\n",
      "Arousal RMSE: 0.31142842491329986\n",
      "Test R^2 score: tensor([0.2059, 0.2847], dtype=torch.float64)\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929596065975097\n",
      "Valence RMSE: 0.273096306275568\n",
      "Arousal RMSE: 0.3115590950272138\n",
      "Test R^2 score: tensor([0.2030, 0.2841], dtype=torch.float64)\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29230021134925765\n",
      "Valence RMSE: 0.27191283467016863\n",
      "Arousal RMSE: 0.3113554840552434\n",
      "Test R^2 score: tensor([0.2099, 0.2851], dtype=torch.float64)\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29232013186064687\n",
      "Valence RMSE: 0.27192798574796256\n",
      "Arousal RMSE: 0.31137965500190895\n",
      "Test R^2 score: tensor([0.2098, 0.2849], dtype=torch.float64)\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29259305164045507\n",
      "Valence RMSE: 0.27240777986705167\n",
      "Arousal RMSE: 0.3114729349469259\n",
      "Test R^2 score: tensor([0.2070, 0.2845], dtype=torch.float64)\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2922100220089234\n",
      "Valence RMSE: 0.271855489211026\n",
      "Arousal RMSE: 0.3112362236481235\n",
      "Test R^2 score: tensor([0.2102, 0.2856], dtype=torch.float64)\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2924213510007262\n",
      "Valence RMSE: 0.27235034458938145\n",
      "Arousal RMSE: 0.31120055084177634\n",
      "Test R^2 score: tensor([0.2074, 0.2858], dtype=torch.float64)\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2929472847571226\n",
      "Valence RMSE: 0.2733274295184263\n",
      "Arousal RMSE: 0.3113331648989408\n",
      "Test R^2 score: tensor([0.2017, 0.2852], dtype=torch.float64)\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2925795214459847\n",
      "Valence RMSE: 0.27258999420464214\n",
      "Arousal RMSE: 0.31128804634716706\n",
      "Test R^2 score: tensor([0.2060, 0.2854], dtype=torch.float64)\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2927948851891282\n",
      "Valence RMSE: 0.2728114328718597\n",
      "Arousal RMSE: 0.3114989754079976\n",
      "Test R^2 score: tensor([0.2047, 0.2844], dtype=torch.float64)\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2931738380078511\n",
      "Valence RMSE: 0.27326668941999793\n",
      "Arousal RMSE: 0.3118126280924841\n",
      "Test R^2 score: tensor([0.2020, 0.2830], dtype=torch.float64)\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29294827531144446\n",
      "Valence RMSE: 0.2725661665686345\n",
      "Arousal RMSE: 0.312001712908726\n",
      "Test R^2 score: tensor([0.2061, 0.2821], dtype=torch.float64)\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2933609465426808\n",
      "Valence RMSE: 0.27305803062685247\n",
      "Arousal RMSE: 0.3123469254259119\n",
      "Test R^2 score: tensor([0.2032, 0.2805], dtype=torch.float64)\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29387038414525646\n",
      "Valence RMSE: 0.2737533725931609\n",
      "Arousal RMSE: 0.31269585278546824\n",
      "Test R^2 score: tensor([0.1992, 0.2789], dtype=torch.float64)\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2939504434155804\n",
      "Valence RMSE: 0.2736295449710385\n",
      "Arousal RMSE: 0.312954626882836\n",
      "Test R^2 score: tensor([0.1999, 0.2777], dtype=torch.float64)\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.294357873848638\n",
      "Valence RMSE: 0.27414641869969736\n",
      "Arousal RMSE: 0.31326802726660696\n",
      "Test R^2 score: tensor([0.1969, 0.2762], dtype=torch.float64)\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.294656218074882\n",
      "Valence RMSE: 0.2744598138831839\n",
      "Arousal RMSE: 0.3135544358856893\n",
      "Test R^2 score: tensor([0.1950, 0.2749], dtype=torch.float64)\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2946797056933362\n",
      "Valence RMSE: 0.2742344242292588\n",
      "Arousal RMSE: 0.31379569541768626\n",
      "Test R^2 score: tensor([0.1964, 0.2738], dtype=torch.float64)\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2953879679811758\n",
      "Valence RMSE: 0.2752831383375385\n",
      "Arousal RMSE: 0.3142090021039052\n",
      "Test R^2 score: tensor([0.1902, 0.2719], dtype=torch.float64)\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2954464292893343\n",
      "Valence RMSE: 0.2751750933024728\n",
      "Arousal RMSE: 0.3144135066844466\n",
      "Test R^2 score: tensor([0.1908, 0.2709], dtype=torch.float64)\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2956898263271874\n",
      "Valence RMSE: 0.2754705895208991\n",
      "Arousal RMSE: 0.31461230283606634\n",
      "Test R^2 score: tensor([0.1891, 0.2700], dtype=torch.float64)\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2958861156959847\n",
      "Valence RMSE: 0.2757875629617923\n",
      "Arousal RMSE: 0.31470368132405246\n",
      "Test R^2 score: tensor([0.1872, 0.2696], dtype=torch.float64)\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29561492953160445\n",
      "Valence RMSE: 0.2752360718848242\n",
      "Arousal RMSE: 0.3146767831559279\n",
      "Test R^2 score: tensor([0.1905, 0.2697], dtype=torch.float64)\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29611965856243805\n",
      "Valence RMSE: 0.27610046969035285\n",
      "Arousal RMSE: 0.3148685995951904\n",
      "Test R^2 score: tensor([0.1854, 0.2688], dtype=torch.float64)\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29602959701727766\n",
      "Valence RMSE: 0.27575678347163857\n",
      "Arousal RMSE: 0.31500038252325896\n",
      "Test R^2 score: tensor([0.1874, 0.2682], dtype=torch.float64)\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2965817326045836\n",
      "Valence RMSE: 0.2765820848868966\n",
      "Arousal RMSE: 0.3153153953569203\n",
      "Test R^2 score: tensor([0.1825, 0.2668], dtype=torch.float64)\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29666078898214593\n",
      "Valence RMSE: 0.2764521885918132\n",
      "Arousal RMSE: 0.31557793785658583\n",
      "Test R^2 score: tensor([0.1833, 0.2655], dtype=torch.float64)\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2971584418898111\n",
      "Valence RMSE: 0.2771077577837726\n",
      "Arousal RMSE: 0.3159391867888668\n",
      "Test R^2 score: tensor([0.1794, 0.2638], dtype=torch.float64)\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29742843795439605\n",
      "Valence RMSE: 0.2772812361603109\n",
      "Arousal RMSE: 0.31629490587329084\n",
      "Test R^2 score: tensor([0.1784, 0.2622], dtype=torch.float64)\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29785737265229895\n",
      "Valence RMSE: 0.27778141258057104\n",
      "Arousal RMSE: 0.3166630949627751\n",
      "Test R^2 score: tensor([0.1754, 0.2605], dtype=torch.float64)\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29805645883343135\n",
      "Valence RMSE: 0.2778641752388468\n",
      "Arousal RMSE: 0.3169649908483354\n",
      "Test R^2 score: tensor([0.1750, 0.2591], dtype=torch.float64)\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2984864581972415\n",
      "Valence RMSE: 0.2784163567979423\n",
      "Arousal RMSE: 0.3172895581667133\n",
      "Test R^2 score: tensor([0.1717, 0.2575], dtype=torch.float64)\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29853124485162874\n",
      "Valence RMSE: 0.27818232021423545\n",
      "Arousal RMSE: 0.31757897447021083\n",
      "Test R^2 score: tensor([0.1731, 0.2562], dtype=torch.float64)\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2994401237979002\n",
      "Valence RMSE: 0.27958339284347766\n",
      "Arousal RMSE: 0.318059588640766\n",
      "Test R^2 score: tensor([0.1647, 0.2539], dtype=torch.float64)\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2989604539341679\n",
      "Valence RMSE: 0.2782787011537851\n",
      "Arousal RMSE: 0.31830122606928457\n",
      "Test R^2 score: tensor([0.1725, 0.2528], dtype=torch.float64)\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3018235119805793\n",
      "Valence RMSE: 0.283232230126791\n",
      "Arousal RMSE: 0.319334258397038\n",
      "Test R^2 score: tensor([0.1428, 0.2479], dtype=torch.float64)\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2996718557684815\n",
      "Valence RMSE: 0.27858633075189915\n",
      "Arousal RMSE: 0.3193682805127094\n",
      "Test R^2 score: tensor([0.1707, 0.2478], dtype=torch.float64)\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30225940529050166\n",
      "Valence RMSE: 0.2833893640405964\n",
      "Arousal RMSE: 0.32001869401927424\n",
      "Test R^2 score: tensor([0.1418, 0.2447], dtype=torch.float64)\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3018863371296633\n",
      "Valence RMSE: 0.28239113951626693\n",
      "Arousal RMSE: 0.3201967604674252\n",
      "Test R^2 score: tensor([0.1478, 0.2439], dtype=torch.float64)\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30088845061472635\n",
      "Valence RMSE: 0.27985335038616393\n",
      "Arousal RMSE: 0.3205461303842244\n",
      "Test R^2 score: tensor([0.1631, 0.2422], dtype=torch.float64)\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30273656420596384\n",
      "Valence RMSE: 0.2831022334557792\n",
      "Arousal RMSE: 0.32117281956419164\n",
      "Test R^2 score: tensor([0.1436, 0.2393], dtype=torch.float64)\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3035185348388068\n",
      "Valence RMSE: 0.28410071621499433\n",
      "Arousal RMSE: 0.321766662392982\n",
      "Test R^2 score: tensor([0.1375, 0.2364], dtype=torch.float64)\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3026113805706449\n",
      "Valence RMSE: 0.2816979691914178\n",
      "Arousal RMSE: 0.3221700629406376\n",
      "Test R^2 score: tensor([0.1520, 0.2345], dtype=torch.float64)\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3036896427791308\n",
      "Valence RMSE: 0.2834382866595848\n",
      "Arousal RMSE: 0.3226724901786814\n",
      "Test R^2 score: tensor([0.1415, 0.2321], dtype=torch.float64)\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3050640275091231\n",
      "Valence RMSE: 0.28554239129159886\n",
      "Arousal RMSE: 0.3234094379198751\n",
      "Test R^2 score: tensor([0.1287, 0.2286], dtype=torch.float64)\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3037868108102016\n",
      "Valence RMSE: 0.2827242015959313\n",
      "Arousal RMSE: 0.3234808783783202\n",
      "Test R^2 score: tensor([0.1458, 0.2283], dtype=torch.float64)\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3043562769159149\n",
      "Valence RMSE: 0.2834453694223164\n",
      "Arousal RMSE: 0.32392006598739864\n",
      "Test R^2 score: tensor([0.1415, 0.2262], dtype=torch.float64)\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3060596695888041\n",
      "Valence RMSE: 0.2863509711661301\n",
      "Arousal RMSE: 0.32457381904557553\n",
      "Test R^2 score: tensor([0.1238, 0.2231], dtype=torch.float64)\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3054252006099941\n",
      "Valence RMSE: 0.28485623430214563\n",
      "Arousal RMSE: 0.32469375127111916\n",
      "Test R^2 score: tensor([0.1329, 0.2225], dtype=torch.float64)\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3052643854576452\n",
      "Valence RMSE: 0.28415365510723484\n",
      "Arousal RMSE: 0.3250067543096085\n",
      "Test R^2 score: tensor([0.1372, 0.2210], dtype=torch.float64)\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3063054071482679\n",
      "Valence RMSE: 0.28598864034432525\n",
      "Arousal RMSE: 0.32535596274009876\n",
      "Test R^2 score: tensor([0.1260, 0.2193], dtype=torch.float64)\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3065295240911935\n",
      "Valence RMSE: 0.28623376535819417\n",
      "Arousal RMSE: 0.3255624822488264\n",
      "Test R^2 score: tensor([0.1245, 0.2183], dtype=torch.float64)\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.306372500991966\n",
      "Valence RMSE: 0.28556017026095837\n",
      "Arousal RMSE: 0.32585826349607416\n",
      "Test R^2 score: tensor([0.1286, 0.2169], dtype=torch.float64)\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3070644624850925\n",
      "Valence RMSE: 0.28676206899369494\n",
      "Arousal RMSE: 0.32610532658785707\n",
      "Test R^2 score: tensor([0.1213, 0.2157], dtype=torch.float64)\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3073448006864258\n",
      "Valence RMSE: 0.28706185075096735\n",
      "Arousal RMSE: 0.3263696475798369\n",
      "Test R^2 score: tensor([0.1194, 0.2144], dtype=torch.float64)\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30671149279090754\n",
      "Valence RMSE: 0.2855239984432391\n",
      "Arousal RMSE: 0.3265270676881765\n",
      "Test R^2 score: tensor([0.1288, 0.2137], dtype=torch.float64)\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30753829004010147\n",
      "Valence RMSE: 0.28667323342230516\n",
      "Arousal RMSE: 0.3270750019808602\n",
      "Test R^2 score: tensor([0.1218, 0.2110], dtype=torch.float64)\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3089645005902127\n",
      "Valence RMSE: 0.28890126710052527\n",
      "Arousal RMSE: 0.32780204867820767\n",
      "Test R^2 score: tensor([0.1081, 0.2075], dtype=torch.float64)\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3085337898515243\n",
      "Valence RMSE: 0.2876623745667701\n",
      "Arousal RMSE: 0.3280801079292928\n",
      "Test R^2 score: tensor([0.1157, 0.2062], dtype=torch.float64)\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30839294464769734\n",
      "Valence RMSE: 0.28716188306105606\n",
      "Arousal RMSE: 0.3282536664437791\n",
      "Test R^2 score: tensor([0.1188, 0.2053], dtype=torch.float64)\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3092212413302072\n",
      "Valence RMSE: 0.28865915062658304\n",
      "Arousal RMSE: 0.3284987776828525\n",
      "Test R^2 score: tensor([0.1096, 0.2042], dtype=torch.float64)\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30927603518577096\n",
      "Valence RMSE: 0.288665237521329\n",
      "Arousal RMSE: 0.32859658021229493\n",
      "Test R^2 score: tensor([0.1096, 0.2037], dtype=torch.float64)\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30877442544103223\n",
      "Valence RMSE: 0.2876990441224216\n",
      "Arousal RMSE: 0.32850045909240977\n",
      "Test R^2 score: tensor([0.1155, 0.2041], dtype=torch.float64)\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30939681839361755\n",
      "Valence RMSE: 0.28869636062409904\n",
      "Arousal RMSE: 0.32879658426842956\n",
      "Test R^2 score: tensor([0.1094, 0.2027], dtype=torch.float64)\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31024760411631974\n",
      "Valence RMSE: 0.28987604831417835\n",
      "Arousal RMSE: 0.32936154653145144\n",
      "Test R^2 score: tensor([0.1021, 0.2000], dtype=torch.float64)\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3102126420263119\n",
      "Valence RMSE: 0.2893983228815722\n",
      "Arousal RMSE: 0.32971560056998017\n",
      "Test R^2 score: tensor([0.1050, 0.1983], dtype=torch.float64)\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3104137286788401\n",
      "Valence RMSE: 0.2894613401734126\n",
      "Arousal RMSE: 0.33003863175333437\n",
      "Test R^2 score: tensor([0.1046, 0.1967], dtype=torch.float64)\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3107938061666158\n",
      "Valence RMSE: 0.28998006439576923\n",
      "Arousal RMSE: 0.33029856517413064\n",
      "Test R^2 score: tensor([0.1014, 0.1954], dtype=torch.float64)\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3109913834261665\n",
      "Valence RMSE: 0.2902246155240112\n",
      "Arousal RMSE: 0.33045567580930074\n",
      "Test R^2 score: tensor([0.0999, 0.1946], dtype=torch.float64)\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31138884691918345\n",
      "Valence RMSE: 0.2907932465391937\n",
      "Arousal RMSE: 0.3307042723318117\n",
      "Test R^2 score: tensor([0.0964, 0.1934], dtype=torch.float64)\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31144143780327255\n",
      "Valence RMSE: 0.29086393075706374\n",
      "Arousal RMSE: 0.33074115580993146\n",
      "Test R^2 score: tensor([0.0959, 0.1933], dtype=torch.float64)\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.311384296972082\n",
      "Valence RMSE: 0.2906306114399884\n",
      "Arousal RMSE: 0.3308386441992742\n",
      "Test R^2 score: tensor([0.0974, 0.1928], dtype=torch.float64)\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31144889217556637\n",
      "Valence RMSE: 0.29060110167386083\n",
      "Arousal RMSE: 0.3309861395598214\n",
      "Test R^2 score: tensor([0.0976, 0.1921], dtype=torch.float64)\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31176313490222174\n",
      "Valence RMSE: 0.29090813555982437\n",
      "Arousal RMSE: 0.3313079552821343\n",
      "Test R^2 score: tensor([0.0957, 0.1905], dtype=torch.float64)\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3120534070455397\n",
      "Valence RMSE: 0.29120931381364856\n",
      "Arousal RMSE: 0.3315897966549069\n",
      "Test R^2 score: tensor([0.0938, 0.1891], dtype=torch.float64)\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31226483120746606\n",
      "Valence RMSE: 0.2914859622985973\n",
      "Arousal RMSE: 0.3317447564030444\n",
      "Test R^2 score: tensor([0.0921, 0.1884], dtype=torch.float64)\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3124345892352993\n",
      "Valence RMSE: 0.29169973439592933\n",
      "Arousal RMSE: 0.3318764981956461\n",
      "Test R^2 score: tensor([0.0907, 0.1877], dtype=torch.float64)\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31211980154338453\n",
      "Valence RMSE: 0.29107086213066596\n",
      "Arousal RMSE: 0.33183624613576246\n",
      "Test R^2 score: tensor([0.0947, 0.1879], dtype=torch.float64)\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3121869928434783\n",
      "Valence RMSE: 0.2910908136327024\n",
      "Arousal RMSE: 0.33194513887080684\n",
      "Test R^2 score: tensor([0.0945, 0.1874], dtype=torch.float64)\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3126932297767064\n",
      "Valence RMSE: 0.29181692743369053\n",
      "Arousal RMSE: 0.3322604291207973\n",
      "Test R^2 score: tensor([0.0900, 0.1858], dtype=torch.float64)\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3125508317797283\n",
      "Valence RMSE: 0.29149437825037433\n",
      "Arousal RMSE: 0.3322755969685823\n",
      "Test R^2 score: tensor([0.0920, 0.1858], dtype=torch.float64)\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3122571956386113\n",
      "Valence RMSE: 0.29088021863493113\n",
      "Arousal RMSE: 0.3322616602364387\n",
      "Test R^2 score: tensor([0.0958, 0.1858], dtype=torch.float64)\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31260142196441426\n",
      "Valence RMSE: 0.29144912763382314\n",
      "Arousal RMSE: 0.3324104451274524\n",
      "Test R^2 score: tensor([0.0923, 0.1851], dtype=torch.float64)\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3131192215282502\n",
      "Valence RMSE: 0.2922150911519453\n",
      "Arousal RMSE: 0.3327125400161154\n",
      "Test R^2 score: tensor([0.0875, 0.1836], dtype=torch.float64)\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.313164650872715\n",
      "Valence RMSE: 0.29214478894669504\n",
      "Arousal RMSE: 0.33285975936383483\n",
      "Test R^2 score: tensor([0.0880, 0.1829], dtype=torch.float64)\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Epoch 299, Loss: 0.45046328763266535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31307362849076503\n",
      "Valence RMSE: 0.2917747159990193\n",
      "Arousal RMSE: 0.333013076044228\n",
      "Test R^2 score: tensor([0.0903, 0.1821], dtype=torch.float64)\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.6246373077883901\n",
      "Epoch 2, Loss: 0.6235530315994445\n",
      "Epoch 3, Loss: 0.6224941564385242\n",
      "Epoch 4, Loss: 0.6214607402112515\n",
      "Epoch 5, Loss: 0.6204527900466451\n",
      "Epoch 6, Loss: 0.6194694440108736\n",
      "Epoch 7, Loss: 0.6185101689710817\n",
      "Epoch 8, Loss: 0.6175741360829309\n",
      "Epoch 9, Loss: 0.6166603154637212\n",
      "Epoch 10, Loss: 0.6157677896340541\n",
      "Epoch 11, Loss: 0.6148956819775698\n",
      "Epoch 12, Loss: 0.6140432058705364\n",
      "Epoch 13, Loss: 0.6133933724123385\n",
      "Epoch 14, Loss: 0.6129507644352314\n",
      "Epoch 15, Loss: 0.612507350060095\n",
      "Epoch 16, Loss: 0.6120650265031023\n",
      "Epoch 17, Loss: 0.6116250386568451\n",
      "Epoch 18, Loss: 0.6111883424112772\n",
      "Epoch 19, Loss: 0.6107556279688645\n",
      "Epoch 20, Loss: 0.6103272944394164\n",
      "Epoch 21, Loss: 0.6099035953730362\n",
      "Epoch 22, Loss: 0.6094845893326112\n",
      "Epoch 23, Loss: 0.6090703349345943\n",
      "Epoch 24, Loss: 0.6086606705077068\n",
      "Epoch 25, Loss: 0.6082555073333157\n",
      "Epoch 26, Loss: 0.6078545602401318\n",
      "Epoch 27, Loss: 0.6074575677424636\n",
      "Epoch 28, Loss: 0.6070642921360818\n",
      "Epoch 29, Loss: 0.606674445904995\n",
      "Epoch 30, Loss: 0.6062877653098363\n",
      "Epoch 31, Loss: 0.6059038629191196\n",
      "Epoch 32, Loss: 0.6055225471469267\n",
      "Epoch 33, Loss: 0.6051434535203463\n",
      "Epoch 34, Loss: 0.6047663398228905\n",
      "Epoch 35, Loss: 0.6043909632123988\n",
      "Epoch 36, Loss: 0.6040170062162543\n",
      "Epoch 37, Loss: 0.6036442739901563\n",
      "Epoch 38, Loss: 0.6032724723977644\n",
      "Epoch 39, Loss: 0.6029014301505805\n",
      "Epoch 40, Loss: 0.6025308518800894\n",
      "Epoch 41, Loss: 0.6021605404736554\n",
      "Epoch 42, Loss: 0.6017902488101043\n",
      "Epoch 43, Loss: 0.6014197539361773\n",
      "Epoch 44, Loss: 0.6010488819326117\n",
      "Epoch 45, Loss: 0.6006774336428161\n",
      "Epoch 46, Loss: 0.6003053335291268\n",
      "Epoch 47, Loss: 0.5999324561883781\n",
      "Epoch 48, Loss: 0.599558750463361\n",
      "Epoch 49, Loss: 0.5991842396731899\n",
      "Epoch 50, Loss: 0.5988089471920054\n",
      "Epoch 51, Loss: 0.5984330209509656\n",
      "Epoch 52, Loss: 0.5980561856791513\n",
      "Epoch 53, Loss: 0.5976771930690247\n",
      "Epoch 54, Loss: 0.5972993819961787\n",
      "Epoch 55, Loss: 0.596942625159372\n",
      "Epoch 56, Loss: 0.5965897762403425\n",
      "Epoch 57, Loss: 0.5962440661169077\n",
      "Epoch 58, Loss: 0.5959021814179315\n",
      "Epoch 59, Loss: 0.5955623022506086\n",
      "Epoch 60, Loss: 0.5952235057723133\n",
      "Epoch 61, Loss: 0.5948874470498098\n",
      "Epoch 62, Loss: 0.5945536545330382\n",
      "Epoch 63, Loss: 0.5942230097266682\n",
      "Epoch 64, Loss: 0.5938969731377866\n",
      "Epoch 65, Loss: 0.5935763054815085\n",
      "Epoch 66, Loss: 0.5932616685153014\n",
      "Epoch 67, Loss: 0.5929535995779446\n",
      "Epoch 68, Loss: 0.5926533157835178\n",
      "Epoch 69, Loss: 0.5923616591040206\n",
      "Epoch 70, Loss: 0.5920800265014708\n",
      "Epoch 71, Loss: 0.5918123853810061\n",
      "Epoch 72, Loss: 0.5915593340950719\n",
      "Epoch 73, Loss: 0.5913202865792704\n",
      "Epoch 74, Loss: 0.5910953858706097\n",
      "Epoch 75, Loss: 0.5908861612242683\n",
      "Epoch 76, Loss: 0.5906898795929476\n",
      "Epoch 77, Loss: 0.590505191216515\n",
      "Epoch 78, Loss: 0.5903300118865379\n",
      "Epoch 79, Loss: 0.5901659912646671\n",
      "Epoch 80, Loss: 0.5900078096923553\n",
      "Epoch 81, Loss: 0.5898570381335112\n",
      "Epoch 82, Loss: 0.5897022355446595\n",
      "Epoch 83, Loss: 0.5895275506318812\n",
      "Epoch 84, Loss: 0.5893220425839641\n",
      "Epoch 85, Loss: 0.589080164717859\n",
      "Epoch 86, Loss: 0.5888013913672013\n",
      "Epoch 87, Loss: 0.5884830366880633\n",
      "Epoch 88, Loss: 0.5881195636461527\n",
      "Epoch 89, Loss: 0.5877052090776013\n",
      "Epoch 90, Loss: 0.5872383174050861\n",
      "Epoch 91, Loss: 0.5867169091898032\n",
      "Epoch 92, Loss: 0.5861433049209519\n",
      "Epoch 93, Loss: 0.5855201760911423\n",
      "Epoch 94, Loss: 0.5848468293484239\n",
      "Epoch 95, Loss: 0.584121381833744\n",
      "Epoch 96, Loss: 0.583341059179461\n",
      "Epoch 97, Loss: 0.5824978385525005\n",
      "Epoch 98, Loss: 0.5815903446556333\n",
      "Epoch 99, Loss: 0.5806128352587802\n",
      "Epoch 100, Loss: 0.5795636191215887\n",
      "Epoch 101, Loss: 0.5784534088617952\n",
      "Epoch 102, Loss: 0.5772959977199387\n",
      "Epoch 103, Loss: 0.5761016285634077\n",
      "Epoch 104, Loss: 0.5748771214836907\n",
      "Epoch 105, Loss: 0.573626180057804\n",
      "Epoch 106, Loss: 0.5723513906741082\n",
      "Epoch 107, Loss: 0.5710570296185948\n",
      "Epoch 108, Loss: 0.5697535819995155\n",
      "Epoch 109, Loss: 0.5684461494614014\n",
      "Epoch 110, Loss: 0.5671351248358865\n",
      "Epoch 111, Loss: 0.5658488457687779\n",
      "Epoch 112, Loss: 0.564558527520008\n",
      "Epoch 113, Loss: 0.5632974746631919\n",
      "Epoch 114, Loss: 0.5620539008914073\n",
      "Epoch 115, Loss: 0.5608405961940474\n",
      "Epoch 116, Loss: 0.559668221154969\n",
      "Epoch 117, Loss: 0.5585259082918451\n",
      "Epoch 118, Loss: 0.5573783670406272\n",
      "Epoch 119, Loss: 0.5562479072702882\n",
      "Epoch 120, Loss: 0.5551617703645141\n",
      "Epoch 121, Loss: 0.5540902320475186\n",
      "Epoch 122, Loss: 0.5529977285655799\n",
      "Epoch 123, Loss: 0.5519039804403205\n",
      "Epoch 124, Loss: 0.5508371148963123\n",
      "Epoch 125, Loss: 0.5497621889103221\n",
      "Epoch 126, Loss: 0.5487087296366597\n",
      "Epoch 127, Loss: 0.547671446541777\n",
      "Epoch 128, Loss: 0.5466528032403869\n",
      "Epoch 129, Loss: 0.5456389491165095\n",
      "Epoch 130, Loss: 0.5446646572100872\n",
      "Epoch 131, Loss: 0.543711504176423\n",
      "Epoch 132, Loss: 0.5428701077881348\n",
      "Epoch 133, Loss: 0.5420311716167078\n",
      "Epoch 134, Loss: 0.5412373002568851\n",
      "Epoch 135, Loss: 0.5404371618836925\n",
      "Epoch 136, Loss: 0.5396710706196934\n",
      "Epoch 137, Loss: 0.5389056599548242\n",
      "Epoch 138, Loss: 0.5381540023484754\n",
      "Epoch 139, Loss: 0.5374128004800518\n",
      "Epoch 140, Loss: 0.5366564696490375\n",
      "Epoch 141, Loss: 0.5359012680485026\n",
      "Epoch 142, Loss: 0.5351483699469025\n",
      "Epoch 143, Loss: 0.5343944389867684\n",
      "Epoch 144, Loss: 0.5336443853299702\n",
      "Epoch 145, Loss: 0.5329023637808303\n",
      "Epoch 146, Loss: 0.5321773961307015\n",
      "Epoch 147, Loss: 0.531477739101188\n",
      "Epoch 148, Loss: 0.5307571705725955\n",
      "Epoch 149, Loss: 0.5300270196487095\n",
      "Epoch 150, Loss: 0.5293225778342594\n",
      "Epoch 151, Loss: 0.5286290364971645\n",
      "Epoch 152, Loss: 0.5279173662933234\n",
      "Epoch 153, Loss: 0.5272076466435625\n",
      "Epoch 154, Loss: 0.5265000552497103\n",
      "Epoch 155, Loss: 0.5257950258079936\n",
      "Epoch 156, Loss: 0.5251112978899439\n",
      "Epoch 157, Loss: 0.5244348333638978\n",
      "Epoch 158, Loss: 0.5237683062210662\n",
      "Epoch 159, Loss: 0.5231048893680518\n",
      "Epoch 160, Loss: 0.5223954774722874\n",
      "Epoch 161, Loss: 0.5217038953413782\n",
      "Epoch 162, Loss: 0.5210725965881778\n",
      "Epoch 163, Loss: 0.5204099237459751\n",
      "Epoch 164, Loss: 0.5197085601429763\n",
      "Epoch 165, Loss: 0.5190290160290975\n",
      "Epoch 166, Loss: 0.5183790811157737\n",
      "Epoch 167, Loss: 0.5177214226196957\n",
      "Epoch 168, Loss: 0.5170212827190377\n",
      "Epoch 169, Loss: 0.5162922847416642\n",
      "Epoch 170, Loss: 0.5155933832821247\n",
      "Epoch 171, Loss: 0.5149476486282009\n",
      "Epoch 172, Loss: 0.5143007845080269\n",
      "Epoch 173, Loss: 0.5135893954935863\n",
      "Epoch 174, Loss: 0.5128790825781965\n",
      "Epoch 175, Loss: 0.5122167479081643\n",
      "Epoch 176, Loss: 0.5115494484422037\n",
      "Epoch 177, Loss: 0.5108367657299796\n",
      "Epoch 178, Loss: 0.5101225907582693\n",
      "Epoch 179, Loss: 0.5094173893521162\n",
      "Epoch 180, Loss: 0.5087041509433818\n",
      "Epoch 181, Loss: 0.5079844550453924\n",
      "Epoch 182, Loss: 0.5072559828482964\n",
      "Epoch 183, Loss: 0.5065259922904846\n",
      "Epoch 184, Loss: 0.5057975407222725\n",
      "Epoch 185, Loss: 0.5050600135665545\n",
      "Epoch 186, Loss: 0.5043186008728717\n",
      "Epoch 187, Loss: 0.5035757711043564\n",
      "Epoch 188, Loss: 0.5028495947014711\n",
      "Epoch 189, Loss: 0.5021800852847157\n",
      "Epoch 190, Loss: 0.5016609558728814\n",
      "Epoch 191, Loss: 0.5010861877308508\n",
      "Epoch 192, Loss: 0.4999655622305211\n",
      "Epoch 193, Loss: 0.4992641159721035\n",
      "Epoch 194, Loss: 0.49873372008349\n",
      "Epoch 195, Loss: 0.49770717629763517\n",
      "Epoch 196, Loss: 0.4971971930728439\n",
      "Epoch 197, Loss: 0.49637161943374347\n",
      "Epoch 198, Loss: 0.4955288231123806\n",
      "Epoch 199, Loss: 0.49497025094562463\n",
      "Epoch 200, Loss: 0.4940264272097897\n",
      "Epoch 201, Loss: 0.4934398220072851\n",
      "Epoch 202, Loss: 0.49262006612851217\n",
      "Epoch 203, Loss: 0.49183907361894375\n",
      "Epoch 204, Loss: 0.4912688446798423\n",
      "Epoch 205, Loss: 0.49034766297836146\n",
      "Epoch 206, Loss: 0.4896699155852708\n",
      "Epoch 207, Loss: 0.4888476634194178\n",
      "Epoch 208, Loss: 0.4880411396938386\n",
      "Epoch 209, Loss: 0.48731609323489744\n",
      "Epoch 210, Loss: 0.48650690798741\n",
      "Epoch 211, Loss: 0.48587439332732296\n",
      "Epoch 212, Loss: 0.4851355119778728\n",
      "Epoch 213, Loss: 0.48439822602730775\n",
      "Epoch 214, Loss: 0.4837105870270647\n",
      "Epoch 215, Loss: 0.4829841148279752\n",
      "Epoch 216, Loss: 0.4823163646885729\n",
      "Epoch 217, Loss: 0.4816362416361073\n",
      "Epoch 218, Loss: 0.4809682776980621\n",
      "Epoch 219, Loss: 0.48035812655118765\n",
      "Epoch 220, Loss: 0.47972174469425155\n",
      "Epoch 221, Loss: 0.4791004732961672\n",
      "Epoch 222, Loss: 0.47854484935970365\n",
      "Epoch 223, Loss: 0.4779443561655317\n",
      "Epoch 224, Loss: 0.47734160914463886\n",
      "Epoch 225, Loss: 0.4767556815156425\n",
      "Epoch 226, Loss: 0.4761250004216297\n",
      "Epoch 227, Loss: 0.47549675765270843\n",
      "Epoch 228, Loss: 0.47490638889642806\n",
      "Epoch 229, Loss: 0.47431415434018653\n",
      "Epoch 230, Loss: 0.4736939538248516\n",
      "Epoch 231, Loss: 0.4730992566188938\n",
      "Epoch 232, Loss: 0.4725421263318015\n",
      "Epoch 233, Loss: 0.47199392020221537\n",
      "Epoch 234, Loss: 0.4714224451537078\n",
      "Epoch 235, Loss: 0.47085600463612887\n",
      "Epoch 236, Loss: 0.4703034567993245\n",
      "Epoch 237, Loss: 0.46976383560984336\n",
      "Epoch 238, Loss: 0.4692135108657803\n",
      "Epoch 239, Loss: 0.46865996449771347\n",
      "Epoch 240, Loss: 0.4681148676673358\n",
      "Epoch 241, Loss: 0.4675701392439125\n",
      "Epoch 242, Loss: 0.46704200469339535\n",
      "Epoch 243, Loss: 0.4665187501931394\n",
      "Epoch 244, Loss: 0.4660203931873101\n",
      "Epoch 245, Loss: 0.46553102543445124\n",
      "Epoch 246, Loss: 0.4650410626082845\n",
      "Epoch 247, Loss: 0.4645584096196038\n",
      "Epoch 248, Loss: 0.4641301423683562\n",
      "Epoch 249, Loss: 0.46385666684718263\n",
      "Epoch 250, Loss: 0.4640255948686196\n",
      "Epoch 251, Loss: 0.4644799772541952\n",
      "Epoch 252, Loss: 0.46281847814064186\n",
      "Epoch 253, Loss: 0.46203058559458965\n",
      "Epoch 254, Loss: 0.4625431478838784\n",
      "Epoch 255, Loss: 0.46125517420980877\n",
      "Epoch 256, Loss: 0.4610207530985558\n",
      "Epoch 257, Loss: 0.46101042607034026\n",
      "Epoch 258, Loss: 0.45994101317191805\n",
      "Epoch 259, Loss: 0.4602198914589956\n",
      "Epoch 260, Loss: 0.4595288557491559\n",
      "Epoch 261, Loss: 0.4590688424811614\n",
      "Epoch 262, Loss: 0.4591290834257235\n",
      "Epoch 263, Loss: 0.45831218764625975\n",
      "Epoch 264, Loss: 0.45832688334290367\n",
      "Epoch 265, Loss: 0.4579043729078864\n",
      "Epoch 266, Loss: 0.457488084361718\n",
      "Epoch 267, Loss: 0.45748466432122475\n",
      "Epoch 268, Loss: 0.4569212789357812\n",
      "Epoch 269, Loss: 0.45686958583749643\n",
      "Epoch 270, Loss: 0.4564589479211001\n",
      "Epoch 271, Loss: 0.45617510821069984\n",
      "Epoch 272, Loss: 0.4560973578346634\n",
      "Epoch 273, Loss: 0.4556325228714987\n",
      "Epoch 274, Loss: 0.455493066573737\n",
      "Epoch 275, Loss: 0.45521443498946385\n",
      "Epoch 276, Loss: 0.4549274768251883\n",
      "Epoch 277, Loss: 0.4547834465546814\n",
      "Epoch 278, Loss: 0.4544552809775777\n",
      "Epoch 279, Loss: 0.4542292261637861\n",
      "Epoch 280, Loss: 0.45401128056035445\n",
      "Epoch 281, Loss: 0.45380336035248836\n",
      "Epoch 282, Loss: 0.4535717241942791\n",
      "Epoch 283, Loss: 0.45337014308607027\n",
      "Epoch 284, Loss: 0.45320296438944574\n",
      "Epoch 285, Loss: 0.4529800018618414\n",
      "Epoch 286, Loss: 0.4527995321866591\n",
      "Epoch 287, Loss: 0.45260893274511427\n",
      "Epoch 288, Loss: 0.4523989681609889\n",
      "Epoch 289, Loss: 0.45218444087661835\n",
      "Epoch 290, Loss: 0.4520139226351631\n",
      "Epoch 291, Loss: 0.4518406687683625\n",
      "Epoch 292, Loss: 0.4516663586989663\n",
      "Epoch 293, Loss: 0.4515037306983139\n",
      "Epoch 294, Loss: 0.45129272361715894\n",
      "Epoch 295, Loss: 0.45112920121678085\n",
      "Epoch 296, Loss: 0.45102114450872544\n",
      "Epoch 297, Loss: 0.4508148696500777\n",
      "Epoch 298, Loss: 0.450617461968175\n",
      "Epoch 299, Loss: 0.45046328763266535\n",
      "Epoch 300, Loss: 0.4503020948098693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31340519017676255\n",
      "Valence RMSE: 0.2921102081000748\n",
      "Arousal RMSE: 0.3333425457141593\n",
      "Test R^2 score: tensor([0.0882, 0.1805], dtype=torch.float64)\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, r2_score = test_model(model)\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0780, -0.4189], dtype=torch.float64),\n",
       " tensor([-0.0726, -0.4066], dtype=torch.float64),\n",
       " tensor([-0.0674, -0.3945], dtype=torch.float64),\n",
       " tensor([-0.0626, -0.3827], dtype=torch.float64),\n",
       " tensor([-0.0580, -0.3711], dtype=torch.float64),\n",
       " tensor([-0.0538, -0.3597], dtype=torch.float64),\n",
       " tensor([-0.0498, -0.3485], dtype=torch.float64),\n",
       " tensor([-0.0462, -0.3375], dtype=torch.float64),\n",
       " tensor([-0.0428, -0.3267], dtype=torch.float64),\n",
       " tensor([-0.0396, -0.3160], dtype=torch.float64),\n",
       " tensor([-0.0366, -0.3056], dtype=torch.float64),\n",
       " tensor([-0.0342, -0.2977], dtype=torch.float64),\n",
       " tensor([-0.0317, -0.2929], dtype=torch.float64),\n",
       " tensor([-0.0291, -0.2881], dtype=torch.float64),\n",
       " tensor([-0.0267, -0.2832], dtype=torch.float64),\n",
       " tensor([-0.0244, -0.2783], dtype=torch.float64),\n",
       " tensor([-0.0221, -0.2733], dtype=torch.float64),\n",
       " tensor([-0.0200, -0.2684], dtype=torch.float64),\n",
       " tensor([-0.0180, -0.2634], dtype=torch.float64),\n",
       " tensor([-0.0161, -0.2585], dtype=torch.float64),\n",
       " tensor([-0.0144, -0.2535], dtype=torch.float64),\n",
       " tensor([-0.0128, -0.2485], dtype=torch.float64),\n",
       " tensor([-0.0113, -0.2435], dtype=torch.float64),\n",
       " tensor([-0.0099, -0.2385], dtype=torch.float64),\n",
       " tensor([-0.0087, -0.2335], dtype=torch.float64),\n",
       " tensor([-0.0075, -0.2284], dtype=torch.float64),\n",
       " tensor([-0.0065, -0.2234], dtype=torch.float64),\n",
       " tensor([-0.0055, -0.2184], dtype=torch.float64),\n",
       " tensor([-0.0047, -0.2133], dtype=torch.float64),\n",
       " tensor([-0.0040, -0.2082], dtype=torch.float64),\n",
       " tensor([-0.0033, -0.2031], dtype=torch.float64),\n",
       " tensor([-0.0027, -0.1980], dtype=torch.float64),\n",
       " tensor([-0.0022, -0.1929], dtype=torch.float64),\n",
       " tensor([-0.0018, -0.1878], dtype=torch.float64),\n",
       " tensor([-0.0014, -0.1827], dtype=torch.float64),\n",
       " tensor([-0.0011, -0.1776], dtype=torch.float64),\n",
       " tensor([-0.0008, -0.1724], dtype=torch.float64),\n",
       " tensor([-0.0006, -0.1673], dtype=torch.float64),\n",
       " tensor([-0.0004, -0.1621], dtype=torch.float64),\n",
       " tensor([-0.0002, -0.1570], dtype=torch.float64),\n",
       " tensor([-1.0666e-04, -1.5179e-01], dtype=torch.float64),\n",
       " tensor([-1.4888e-05, -1.4662e-01], dtype=torch.float64),\n",
       " tensor([ 5.6064e-05, -1.4145e-01], dtype=torch.float64),\n",
       " tensor([ 1.1084e-04, -1.3627e-01], dtype=torch.float64),\n",
       " tensor([ 0.0002, -0.1311], dtype=torch.float64),\n",
       " tensor([ 0.0002, -0.1259], dtype=torch.float64),\n",
       " tensor([ 0.0002, -0.1207], dtype=torch.float64),\n",
       " tensor([ 0.0002, -0.1156], dtype=torch.float64),\n",
       " tensor([ 0.0003, -0.1104], dtype=torch.float64),\n",
       " tensor([ 0.0003, -0.1053], dtype=torch.float64),\n",
       " tensor([ 0.0004, -0.1001], dtype=torch.float64),\n",
       " tensor([ 0.0004, -0.0950], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0899], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0850], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0801], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0754], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0708], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0662], dtype=torch.float64),\n",
       " tensor([ 0.0006, -0.0617], dtype=torch.float64),\n",
       " tensor([ 0.0007, -0.0571], dtype=torch.float64),\n",
       " tensor([ 0.0007, -0.0527], dtype=torch.float64),\n",
       " tensor([ 0.0008, -0.0482], dtype=torch.float64),\n",
       " tensor([ 0.0008, -0.0437], dtype=torch.float64),\n",
       " tensor([ 0.0008, -0.0393], dtype=torch.float64),\n",
       " tensor([ 0.0008, -0.0350], dtype=torch.float64),\n",
       " tensor([ 0.0007, -0.0307], dtype=torch.float64),\n",
       " tensor([ 0.0007, -0.0265], dtype=torch.float64),\n",
       " tensor([ 0.0006, -0.0224], dtype=torch.float64),\n",
       " tensor([ 0.0005, -0.0183], dtype=torch.float64),\n",
       " tensor([ 0.0004, -0.0144], dtype=torch.float64),\n",
       " tensor([ 0.0003, -0.0106], dtype=torch.float64),\n",
       " tensor([ 7.3968e-05, -7.0238e-03], dtype=torch.float64),\n",
       " tensor([-0.0002, -0.0036], dtype=torch.float64),\n",
       " tensor([-0.0004, -0.0002], dtype=torch.float64),\n",
       " tensor([-0.0007,  0.0030], dtype=torch.float64),\n",
       " tensor([-0.0009,  0.0060], dtype=torch.float64),\n",
       " tensor([-0.0012,  0.0088], dtype=torch.float64),\n",
       " tensor([-0.0015,  0.0115], dtype=torch.float64),\n",
       " tensor([-0.0018,  0.0139], dtype=torch.float64),\n",
       " tensor([-0.0020,  0.0163], dtype=torch.float64),\n",
       " tensor([-0.0023,  0.0186], dtype=torch.float64),\n",
       " tensor([-0.0025,  0.0212], dtype=torch.float64),\n",
       " tensor([-0.0027,  0.0240], dtype=torch.float64),\n",
       " tensor([-0.0028,  0.0271], dtype=torch.float64),\n",
       " tensor([-0.0030,  0.0306], dtype=torch.float64),\n",
       " tensor([-0.0032,  0.0345], dtype=torch.float64),\n",
       " tensor([-0.0034,  0.0390], dtype=torch.float64),\n",
       " tensor([-0.0035,  0.0441], dtype=torch.float64),\n",
       " tensor([-0.0036,  0.0497], dtype=torch.float64),\n",
       " tensor([-0.0035,  0.0559], dtype=torch.float64),\n",
       " tensor([-0.0033,  0.0626], dtype=torch.float64),\n",
       " tensor([-0.0029,  0.0696], dtype=torch.float64),\n",
       " tensor([-0.0022,  0.0770], dtype=torch.float64),\n",
       " tensor([-0.0012,  0.0848], dtype=torch.float64),\n",
       " tensor([0.0002, 0.0930], dtype=torch.float64),\n",
       " tensor([0.0020, 0.1015], dtype=torch.float64),\n",
       " tensor([0.0044, 0.1104], dtype=torch.float64),\n",
       " tensor([0.0075, 0.1196], dtype=torch.float64),\n",
       " tensor([0.0113, 0.1290], dtype=torch.float64),\n",
       " tensor([0.0157, 0.1385], dtype=torch.float64),\n",
       " tensor([0.0208, 0.1480], dtype=torch.float64),\n",
       " tensor([0.0262, 0.1573], dtype=torch.float64),\n",
       " tensor([0.0320, 0.1664], dtype=torch.float64),\n",
       " tensor([0.0382, 0.1753], dtype=torch.float64),\n",
       " tensor([0.0447, 0.1840], dtype=torch.float64),\n",
       " tensor([0.0517, 0.1922], dtype=torch.float64),\n",
       " tensor([0.0588, 0.2001], dtype=torch.float64),\n",
       " tensor([0.0658, 0.2076], dtype=torch.float64),\n",
       " tensor([0.0730, 0.2148], dtype=torch.float64),\n",
       " tensor([0.0803, 0.2215], dtype=torch.float64),\n",
       " tensor([0.0875, 0.2279], dtype=torch.float64),\n",
       " tensor([0.0947, 0.2343], dtype=torch.float64),\n",
       " tensor([0.1019, 0.2404], dtype=torch.float64),\n",
       " tensor([0.1090, 0.2465], dtype=torch.float64),\n",
       " tensor([0.1158, 0.2523], dtype=torch.float64),\n",
       " tensor([0.1226, 0.2576], dtype=torch.float64),\n",
       " tensor([0.1299, 0.2624], dtype=torch.float64),\n",
       " tensor([0.1380, 0.2669], dtype=torch.float64),\n",
       " tensor([0.1456, 0.2709], dtype=torch.float64),\n",
       " tensor([0.1527, 0.2751], dtype=torch.float64),\n",
       " tensor([0.1591, 0.2797], dtype=torch.float64),\n",
       " tensor([0.1647, 0.2842], dtype=torch.float64),\n",
       " tensor([0.1699, 0.2883], dtype=torch.float64),\n",
       " tensor([0.1749, 0.2919], dtype=torch.float64),\n",
       " tensor([0.1800, 0.2946], dtype=torch.float64),\n",
       " tensor([0.1848, 0.2970], dtype=torch.float64),\n",
       " tensor([0.1885, 0.2992], dtype=torch.float64),\n",
       " tensor([0.1912, 0.3016], dtype=torch.float64),\n",
       " tensor([0.1938, 0.3034], dtype=torch.float64),\n",
       " tensor([0.1976, 0.3044], dtype=torch.float64),\n",
       " tensor([0.2011, 0.3050], dtype=torch.float64),\n",
       " tensor([0.2031, 0.3064], dtype=torch.float64),\n",
       " tensor([0.2045, 0.3071], dtype=torch.float64),\n",
       " tensor([0.2075, 0.3072], dtype=torch.float64),\n",
       " tensor([0.2093, 0.3071], dtype=torch.float64),\n",
       " tensor([0.2092, 0.3070], dtype=torch.float64),\n",
       " tensor([0.2112, 0.3066], dtype=torch.float64),\n",
       " tensor([0.2151, 0.3062], dtype=torch.float64),\n",
       " tensor([0.2161, 0.3059], dtype=torch.float64),\n",
       " tensor([0.2175, 0.3056], dtype=torch.float64),\n",
       " tensor([0.2193, 0.3054], dtype=torch.float64),\n",
       " tensor([0.2195, 0.3050], dtype=torch.float64),\n",
       " tensor([0.2227, 0.3047], dtype=torch.float64),\n",
       " tensor([0.2236, 0.3043], dtype=torch.float64),\n",
       " tensor([0.2253, 0.3040], dtype=torch.float64),\n",
       " tensor([0.2295, 0.3040], dtype=torch.float64),\n",
       " tensor([0.2262, 0.3037], dtype=torch.float64),\n",
       " tensor([0.2298, 0.3040], dtype=torch.float64),\n",
       " tensor([0.2323, 0.3042], dtype=torch.float64),\n",
       " tensor([0.2303, 0.3037], dtype=torch.float64),\n",
       " tensor([0.2352, 0.3039], dtype=torch.float64),\n",
       " tensor([0.2371, 0.3035], dtype=torch.float64),\n",
       " tensor([0.2363, 0.3030], dtype=torch.float64),\n",
       " tensor([0.2382, 0.3027], dtype=torch.float64),\n",
       " tensor([0.2358, 0.3016], dtype=torch.float64),\n",
       " tensor([0.2426, 0.3016], dtype=torch.float64),\n",
       " tensor([0.2376, 0.3003], dtype=torch.float64),\n",
       " tensor([0.2459, 0.3017], dtype=torch.float64),\n",
       " tensor([0.2374, 0.3008], dtype=torch.float64),\n",
       " tensor([0.2404, 0.3013], dtype=torch.float64),\n",
       " tensor([0.2441, 0.3014], dtype=torch.float64),\n",
       " tensor([0.2375, 0.3000], dtype=torch.float64),\n",
       " tensor([0.2439, 0.3006], dtype=torch.float64),\n",
       " tensor([0.2445, 0.3008], dtype=torch.float64),\n",
       " tensor([0.2377, 0.3002], dtype=torch.float64),\n",
       " tensor([0.2466, 0.3017], dtype=torch.float64),\n",
       " tensor([0.2367, 0.3005], dtype=torch.float64),\n",
       " tensor([0.2444, 0.3015], dtype=torch.float64),\n",
       " tensor([0.2440, 0.3015], dtype=torch.float64),\n",
       " tensor([0.2366, 0.3007], dtype=torch.float64),\n",
       " tensor([0.2471, 0.3028], dtype=torch.float64),\n",
       " tensor([0.2390, 0.3024], dtype=torch.float64),\n",
       " tensor([0.2442, 0.3037], dtype=torch.float64),\n",
       " tensor([0.2493, 0.3051], dtype=torch.float64),\n",
       " tensor([0.2434, 0.3052], dtype=torch.float64),\n",
       " tensor([0.2499, 0.3072], dtype=torch.float64),\n",
       " tensor([0.2476, 0.3077], dtype=torch.float64),\n",
       " tensor([0.2502, 0.3084], dtype=torch.float64),\n",
       " tensor([0.2520, 0.3088], dtype=torch.float64),\n",
       " tensor([0.2536, 0.3094], dtype=torch.float64),\n",
       " tensor([0.2533, 0.3100], dtype=torch.float64),\n",
       " tensor([0.2536, 0.3105], dtype=torch.float64),\n",
       " tensor([0.2534, 0.3105], dtype=torch.float64),\n",
       " tensor([0.2546, 0.3105], dtype=torch.float64),\n",
       " tensor([0.2533, 0.3102], dtype=torch.float64),\n",
       " tensor([0.2552, 0.3106], dtype=torch.float64),\n",
       " tensor([0.2492, 0.3098], dtype=torch.float64),\n",
       " tensor([0.2597, 0.3106], dtype=torch.float64),\n",
       " tensor([0.2386, 0.3071], dtype=torch.float64),\n",
       " tensor([0.2646, 0.3107], dtype=torch.float64),\n",
       " tensor([0.2464, 0.3084], dtype=torch.float64),\n",
       " tensor([0.2452, 0.3086], dtype=torch.float64),\n",
       " tensor([0.2620, 0.3110], dtype=torch.float64),\n",
       " tensor([0.2514, 0.3095], dtype=torch.float64),\n",
       " tensor([0.2420, 0.3073], dtype=torch.float64),\n",
       " tensor([0.2575, 0.3093], dtype=torch.float64),\n",
       " tensor([0.2532, 0.3080], dtype=torch.float64),\n",
       " tensor([0.2358, 0.3043], dtype=torch.float64),\n",
       " tensor([0.2445, 0.3054], dtype=torch.float64),\n",
       " tensor([0.2464, 0.3049], dtype=torch.float64),\n",
       " tensor([0.2320, 0.3019], dtype=torch.float64),\n",
       " tensor([0.2327, 0.3009], dtype=torch.float64),\n",
       " tensor([0.2432, 0.3015], dtype=torch.float64),\n",
       " tensor([0.2325, 0.2982], dtype=torch.float64),\n",
       " tensor([0.2219, 0.2950], dtype=torch.float64),\n",
       " tensor([0.2303, 0.2958], dtype=torch.float64),\n",
       " tensor([0.2237, 0.2939], dtype=torch.float64),\n",
       " tensor([0.2115, 0.2910], dtype=torch.float64),\n",
       " tensor([0.2180, 0.2910], dtype=torch.float64),\n",
       " tensor([0.2213, 0.2901], dtype=torch.float64),\n",
       " tensor([0.2133, 0.2872], dtype=torch.float64),\n",
       " tensor([0.2109, 0.2858], dtype=torch.float64),\n",
       " tensor([0.2123, 0.2860], dtype=torch.float64),\n",
       " tensor([0.2059, 0.2847], dtype=torch.float64),\n",
       " tensor([0.2030, 0.2841], dtype=torch.float64),\n",
       " tensor([0.2099, 0.2851], dtype=torch.float64),\n",
       " tensor([0.2098, 0.2849], dtype=torch.float64),\n",
       " tensor([0.2070, 0.2845], dtype=torch.float64),\n",
       " tensor([0.2102, 0.2856], dtype=torch.float64),\n",
       " tensor([0.2074, 0.2858], dtype=torch.float64),\n",
       " tensor([0.2017, 0.2852], dtype=torch.float64),\n",
       " tensor([0.2060, 0.2854], dtype=torch.float64),\n",
       " tensor([0.2047, 0.2844], dtype=torch.float64),\n",
       " tensor([0.2020, 0.2830], dtype=torch.float64),\n",
       " tensor([0.2061, 0.2821], dtype=torch.float64),\n",
       " tensor([0.2032, 0.2805], dtype=torch.float64),\n",
       " tensor([0.1992, 0.2789], dtype=torch.float64),\n",
       " tensor([0.1999, 0.2777], dtype=torch.float64),\n",
       " tensor([0.1969, 0.2762], dtype=torch.float64),\n",
       " tensor([0.1950, 0.2749], dtype=torch.float64),\n",
       " tensor([0.1964, 0.2738], dtype=torch.float64),\n",
       " tensor([0.1902, 0.2719], dtype=torch.float64),\n",
       " tensor([0.1908, 0.2709], dtype=torch.float64),\n",
       " tensor([0.1891, 0.2700], dtype=torch.float64),\n",
       " tensor([0.1872, 0.2696], dtype=torch.float64),\n",
       " tensor([0.1905, 0.2697], dtype=torch.float64),\n",
       " tensor([0.1854, 0.2688], dtype=torch.float64),\n",
       " tensor([0.1874, 0.2682], dtype=torch.float64),\n",
       " tensor([0.1825, 0.2668], dtype=torch.float64),\n",
       " tensor([0.1833, 0.2655], dtype=torch.float64),\n",
       " tensor([0.1794, 0.2638], dtype=torch.float64),\n",
       " tensor([0.1784, 0.2622], dtype=torch.float64),\n",
       " tensor([0.1754, 0.2605], dtype=torch.float64),\n",
       " tensor([0.1750, 0.2591], dtype=torch.float64),\n",
       " tensor([0.1717, 0.2575], dtype=torch.float64),\n",
       " tensor([0.1731, 0.2562], dtype=torch.float64),\n",
       " tensor([0.1647, 0.2539], dtype=torch.float64),\n",
       " tensor([0.1725, 0.2528], dtype=torch.float64),\n",
       " tensor([0.1428, 0.2479], dtype=torch.float64),\n",
       " tensor([0.1707, 0.2478], dtype=torch.float64),\n",
       " tensor([0.1418, 0.2447], dtype=torch.float64),\n",
       " tensor([0.1478, 0.2439], dtype=torch.float64),\n",
       " tensor([0.1631, 0.2422], dtype=torch.float64),\n",
       " tensor([0.1436, 0.2393], dtype=torch.float64),\n",
       " tensor([0.1375, 0.2364], dtype=torch.float64),\n",
       " tensor([0.1520, 0.2345], dtype=torch.float64),\n",
       " tensor([0.1415, 0.2321], dtype=torch.float64),\n",
       " tensor([0.1287, 0.2286], dtype=torch.float64),\n",
       " tensor([0.1458, 0.2283], dtype=torch.float64),\n",
       " tensor([0.1415, 0.2262], dtype=torch.float64),\n",
       " tensor([0.1238, 0.2231], dtype=torch.float64),\n",
       " tensor([0.1329, 0.2225], dtype=torch.float64),\n",
       " tensor([0.1372, 0.2210], dtype=torch.float64),\n",
       " tensor([0.1260, 0.2193], dtype=torch.float64),\n",
       " tensor([0.1245, 0.2183], dtype=torch.float64),\n",
       " tensor([0.1286, 0.2169], dtype=torch.float64),\n",
       " tensor([0.1213, 0.2157], dtype=torch.float64),\n",
       " tensor([0.1194, 0.2144], dtype=torch.float64),\n",
       " tensor([0.1288, 0.2137], dtype=torch.float64),\n",
       " tensor([0.1218, 0.2110], dtype=torch.float64),\n",
       " tensor([0.1081, 0.2075], dtype=torch.float64),\n",
       " tensor([0.1157, 0.2062], dtype=torch.float64),\n",
       " tensor([0.1188, 0.2053], dtype=torch.float64),\n",
       " tensor([0.1096, 0.2042], dtype=torch.float64),\n",
       " tensor([0.1096, 0.2037], dtype=torch.float64),\n",
       " tensor([0.1155, 0.2041], dtype=torch.float64),\n",
       " tensor([0.1094, 0.2027], dtype=torch.float64),\n",
       " tensor([0.1021, 0.2000], dtype=torch.float64),\n",
       " tensor([0.1050, 0.1983], dtype=torch.float64),\n",
       " tensor([0.1046, 0.1967], dtype=torch.float64),\n",
       " tensor([0.1014, 0.1954], dtype=torch.float64),\n",
       " tensor([0.0999, 0.1946], dtype=torch.float64),\n",
       " tensor([0.0964, 0.1934], dtype=torch.float64),\n",
       " tensor([0.0959, 0.1933], dtype=torch.float64),\n",
       " tensor([0.0974, 0.1928], dtype=torch.float64),\n",
       " tensor([0.0976, 0.1921], dtype=torch.float64),\n",
       " tensor([0.0957, 0.1905], dtype=torch.float64),\n",
       " tensor([0.0938, 0.1891], dtype=torch.float64),\n",
       " tensor([0.0921, 0.1884], dtype=torch.float64),\n",
       " tensor([0.0907, 0.1877], dtype=torch.float64),\n",
       " tensor([0.0947, 0.1879], dtype=torch.float64),\n",
       " tensor([0.0945, 0.1874], dtype=torch.float64),\n",
       " tensor([0.0900, 0.1858], dtype=torch.float64),\n",
       " tensor([0.0920, 0.1858], dtype=torch.float64),\n",
       " tensor([0.0958, 0.1858], dtype=torch.float64),\n",
       " tensor([0.0923, 0.1851], dtype=torch.float64),\n",
       " tensor([0.0875, 0.1836], dtype=torch.float64),\n",
       " tensor([0.0880, 0.1829], dtype=torch.float64),\n",
       " tensor([0.0903, 0.1821], dtype=torch.float64),\n",
       " tensor([0.0882, 0.1805], dtype=torch.float64)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the line graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. R^2 score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('r^2 score') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m max_r2_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr2_scores_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m corresponding_rmse \u001b[38;5;241m=\u001b[39m rmse_list[r2_scores_list\u001b[38;5;241m.\u001b[39mindex(max_r2_score)]\n\u001b[1;32m      3\u001b[0m corresponding_num_epochs \u001b[38;5;241m=\u001b[39m num_epochs_list[r2_scores_list\u001b[38;5;241m.\u001b[39mindex(max_r2_score)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
