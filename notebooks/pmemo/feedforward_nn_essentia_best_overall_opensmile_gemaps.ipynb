{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Overall openSMILE GeMAPS Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>...</th>\n",
       "      <th>alphaRatioUV_sma3nz_amean</th>\n",
       "      <th>hammarbergIndexUV_sma3nz_amean</th>\n",
       "      <th>slopeUV0-500_sma3nz_amean</th>\n",
       "      <th>slopeUV500-1500_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.213586</td>\n",
       "      <td>-0.223554</td>\n",
       "      <td>-0.205962</td>\n",
       "      <td>-0.189170</td>\n",
       "      <td>0.094015</td>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.428626</td>\n",
       "      <td>-0.774106</td>\n",
       "      <td>-0.092809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.112443</td>\n",
       "      <td>-0.356517</td>\n",
       "      <td>0.241192</td>\n",
       "      <td>0.626061</td>\n",
       "      <td>0.381042</td>\n",
       "      <td>-0.336055</td>\n",
       "      <td>-0.374205</td>\n",
       "      <td>0.262637</td>\n",
       "      <td>-0.077513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.519912</td>\n",
       "      <td>1.444784</td>\n",
       "      <td>0.436868</td>\n",
       "      <td>0.274619</td>\n",
       "      <td>0.907303</td>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.210257</td>\n",
       "      <td>0.082097</td>\n",
       "      <td>1.628937</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.582425</td>\n",
       "      <td>2.521296</td>\n",
       "      <td>-1.587073</td>\n",
       "      <td>-1.756907</td>\n",
       "      <td>1.246634</td>\n",
       "      <td>-2.081171</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.364584</td>\n",
       "      <td>13.929680</td>\n",
       "      <td>12.635127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4.192724</td>\n",
       "      <td>4.051296</td>\n",
       "      <td>3.004901</td>\n",
       "      <td>2.571136</td>\n",
       "      <td>2.095142</td>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.658255</td>\n",
       "      <td>-1.519185</td>\n",
       "      <td>3.431222</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.393781</td>\n",
       "      <td>2.181989</td>\n",
       "      <td>-1.341486</td>\n",
       "      <td>-0.316982</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>-0.483298</td>\n",
       "      <td>-0.156962</td>\n",
       "      <td>-0.196903</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.506650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.308062</td>\n",
       "      <td>0.278054</td>\n",
       "      <td>-0.185456</td>\n",
       "      <td>-0.169303</td>\n",
       "      <td>-0.301137</td>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.294941</td>\n",
       "      <td>-0.568763</td>\n",
       "      <td>0.097731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.741640</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>-1.876367</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>0.560435</td>\n",
       "      <td>-0.939150</td>\n",
       "      <td>-0.438867</td>\n",
       "      <td>-0.422328</td>\n",
       "      <td>4.102221</td>\n",
       "      <td>3.583107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.285838</td>\n",
       "      <td>-0.283458</td>\n",
       "      <td>-0.268729</td>\n",
       "      <td>-0.246064</td>\n",
       "      <td>-0.124872</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.309671</td>\n",
       "      <td>-1.204256</td>\n",
       "      <td>-0.389019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469653</td>\n",
       "      <td>-0.687136</td>\n",
       "      <td>1.506562</td>\n",
       "      <td>-0.467269</td>\n",
       "      <td>0.347808</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.097217</td>\n",
       "      <td>-0.183003</td>\n",
       "      <td>-0.546643</td>\n",
       "      <td>-0.583730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>-0.375556</td>\n",
       "      <td>-0.349878</td>\n",
       "      <td>-0.246022</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>0.509430</td>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.341794</td>\n",
       "      <td>-1.504569</td>\n",
       "      <td>-0.282898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612326</td>\n",
       "      <td>-0.673763</td>\n",
       "      <td>0.581939</td>\n",
       "      <td>0.317231</td>\n",
       "      <td>-0.272794</td>\n",
       "      <td>0.346534</td>\n",
       "      <td>-0.296380</td>\n",
       "      <td>0.179435</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>-0.110172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>-0.003168</td>\n",
       "      <td>0.049535</td>\n",
       "      <td>-0.012648</td>\n",
       "      <td>1.028387</td>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.498191</td>\n",
       "      <td>-0.337371</td>\n",
       "      <td>0.952869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502963</td>\n",
       "      <td>-0.119092</td>\n",
       "      <td>0.708211</td>\n",
       "      <td>1.645550</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.734247</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-0.392740</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.131591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>-0.431716</td>\n",
       "      <td>-0.421521</td>\n",
       "      <td>-0.304278</td>\n",
       "      <td>-0.273126</td>\n",
       "      <td>-0.286054</td>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.361876</td>\n",
       "      <td>-1.369817</td>\n",
       "      <td>-0.487787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785432</td>\n",
       "      <td>-0.498027</td>\n",
       "      <td>0.274327</td>\n",
       "      <td>1.052788</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.200898</td>\n",
       "      <td>-0.273511</td>\n",
       "      <td>-0.243128</td>\n",
       "      <td>0.080631</td>\n",
       "      <td>1.233979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>-0.387322</td>\n",
       "      <td>-0.389485</td>\n",
       "      <td>-0.311137</td>\n",
       "      <td>-0.281734</td>\n",
       "      <td>-0.391420</td>\n",
       "      <td>-0.296933</td>\n",
       "      <td>-0.228861</td>\n",
       "      <td>0.697357</td>\n",
       "      <td>-0.370472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605529</td>\n",
       "      <td>-0.365492</td>\n",
       "      <td>-0.185980</td>\n",
       "      <td>1.193421</td>\n",
       "      <td>-0.851040</td>\n",
       "      <td>0.757317</td>\n",
       "      <td>-0.403438</td>\n",
       "      <td>-0.260820</td>\n",
       "      <td>0.378455</td>\n",
       "      <td>2.290346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>-0.618565</td>\n",
       "      <td>-0.597394</td>\n",
       "      <td>-0.332018</td>\n",
       "      <td>-0.300291</td>\n",
       "      <td>-0.529375</td>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.458441</td>\n",
       "      <td>-0.476552</td>\n",
       "      <td>-0.619506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880999</td>\n",
       "      <td>-0.831021</td>\n",
       "      <td>0.168912</td>\n",
       "      <td>1.275916</td>\n",
       "      <td>-1.050472</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>-0.394235</td>\n",
       "      <td>-0.342545</td>\n",
       "      <td>0.394655</td>\n",
       "      <td>0.652717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.melbands_kurtosis.dmean  \\\n",
       "0          1                         -0.213586   \n",
       "1          4                          1.519912   \n",
       "2          5                          4.192724   \n",
       "3          6                          0.308062   \n",
       "4          7                         -0.285838   \n",
       "..       ...                               ...   \n",
       "762      993                         -0.375556   \n",
       "763      996                          0.108256   \n",
       "764      997                         -0.431716   \n",
       "765      999                         -0.387322   \n",
       "766     1000                         -0.618565   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dmean2  lowlevel.melbands_kurtosis.dvar  \\\n",
       "0                            -0.223554                        -0.205962   \n",
       "1                             1.444784                         0.436868   \n",
       "2                             4.051296                         3.004901   \n",
       "3                             0.278054                        -0.185456   \n",
       "4                            -0.283458                        -0.268729   \n",
       "..                                 ...                              ...   \n",
       "762                          -0.349878                        -0.246022   \n",
       "763                          -0.003168                         0.049535   \n",
       "764                          -0.421521                        -0.304278   \n",
       "765                          -0.389485                        -0.311137   \n",
       "766                          -0.597394                        -0.332018   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar2  lowlevel.melbands_kurtosis.max  \\\n",
       "0                           -0.189170                        0.094015   \n",
       "1                            0.274619                        0.907303   \n",
       "2                            2.571136                        2.095142   \n",
       "3                           -0.169303                       -0.301137   \n",
       "4                           -0.246064                       -0.124872   \n",
       "..                                ...                             ...   \n",
       "762                         -0.213527                        0.509430   \n",
       "763                         -0.012648                        1.028387   \n",
       "764                         -0.273126                       -0.286054   \n",
       "765                         -0.281734                       -0.391420   \n",
       "766                         -0.300291                       -0.529375   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_kurtosis.median  \\\n",
       "0                          -0.290830                          -0.428626   \n",
       "1                           2.148017                           2.210257   \n",
       "2                           4.061978                           3.658255   \n",
       "3                           0.609811                           1.294941   \n",
       "4                          -0.407720                          -0.309671   \n",
       "..                               ...                                ...   \n",
       "762                        -0.341976                          -0.341794   \n",
       "763                         0.208018                          -0.498191   \n",
       "764                        -0.448361                          -0.361876   \n",
       "765                        -0.296933                          -0.228861   \n",
       "766                        -0.599766                          -0.458441   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.min  lowlevel.melbands_kurtosis.stdev  ...  \\\n",
       "0                         -0.774106                         -0.092809  ...   \n",
       "1                          0.082097                          1.628937  ...   \n",
       "2                         -1.519185                          3.431222  ...   \n",
       "3                         -0.568763                          0.097731  ...   \n",
       "4                         -1.204256                         -0.389019  ...   \n",
       "..                              ...                               ...  ...   \n",
       "762                       -1.504569                         -0.282898  ...   \n",
       "763                       -0.337371                          0.952869  ...   \n",
       "764                       -1.369817                         -0.487787  ...   \n",
       "765                        0.697357                         -0.370472  ...   \n",
       "766                       -0.476552                         -0.619506  ...   \n",
       "\n",
       "     alphaRatioUV_sma3nz_amean  hammarbergIndexUV_sma3nz_amean  \\\n",
       "0                     0.010665                        0.112443   \n",
       "1                    -2.582425                        2.521296   \n",
       "2                    -2.393781                        2.181989   \n",
       "3                    -0.741640                        0.768306   \n",
       "4                     0.469653                       -0.687136   \n",
       "..                         ...                             ...   \n",
       "762                   0.612326                       -0.673763   \n",
       "763                   0.502963                       -0.119092   \n",
       "764                   0.785432                       -0.498027   \n",
       "765                   0.605529                       -0.365492   \n",
       "766                   0.880999                       -0.831021   \n",
       "\n",
       "     slopeUV0-500_sma3nz_amean  slopeUV500-1500_sma3nz_amean  \\\n",
       "0                    -0.356517                      0.241192   \n",
       "1                    -1.587073                     -1.756907   \n",
       "2                    -1.341486                     -0.316982   \n",
       "3                    -1.876367                      0.705660   \n",
       "4                     1.506562                     -0.467269   \n",
       "..                         ...                           ...   \n",
       "762                   0.581939                      0.317231   \n",
       "763                   0.708211                      1.645550   \n",
       "764                   0.274327                      1.052788   \n",
       "765                  -0.185980                      1.193421   \n",
       "766                   0.168912                      1.275916   \n",
       "\n",
       "     loudnessPeaksPerSec  VoicedSegmentsPerSec  MeanVoicedSegmentLengthSec  \\\n",
       "0               0.626061              0.381042                   -0.336055   \n",
       "1               1.246634             -2.081171                    0.077305   \n",
       "2               1.247192             -0.483298                   -0.156962   \n",
       "3               0.560435             -0.939150                   -0.438867   \n",
       "4               0.347808             -0.244452                   -0.097217   \n",
       "..                   ...                   ...                         ...   \n",
       "762            -0.272794              0.346534                   -0.296380   \n",
       "763             0.506390              0.734247                   -0.376792   \n",
       "764            -0.036849              0.200898                   -0.273511   \n",
       "765            -0.851040              0.757317                   -0.403438   \n",
       "766            -1.050472              0.822118                   -0.394235   \n",
       "\n",
       "     StddevVoicedSegmentLengthSec  MeanUnvoicedSegmentLength  \\\n",
       "0                       -0.374205                   0.262637   \n",
       "1                        0.364584                  13.929680   \n",
       "2                       -0.196903                   0.509336   \n",
       "3                       -0.422328                   4.102221   \n",
       "4                       -0.183003                  -0.546643   \n",
       "..                            ...                        ...   \n",
       "762                      0.179435                   0.009848   \n",
       "763                     -0.392740                   0.211598   \n",
       "764                     -0.243128                   0.080631   \n",
       "765                     -0.260820                   0.378455   \n",
       "766                     -0.342545                   0.394655   \n",
       "\n",
       "     StddevUnvoicedSegmentLength  \n",
       "0                      -0.077513  \n",
       "1                      12.635127  \n",
       "2                       0.506650  \n",
       "3                       3.583107  \n",
       "4                      -0.583730  \n",
       "..                           ...  \n",
       "762                    -0.110172  \n",
       "763                     0.131591  \n",
       "764                     1.233979  \n",
       "765                     2.290346  \n",
       "766                     0.652717  \n",
       "\n",
       "[767 rows x 199 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_gemaps = pd.read_csv(get_pmemo_path('processed/features/integrated/standardised_essentia_best_overall_opensmile_gemaps_features.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_overall_opensmile_gemaps = df_essentia_best_overall_opensmile_gemaps[df_essentia_best_overall_opensmile_gemaps.columns[1:]]\n",
    "\n",
    "df_essentia_best_overall_opensmile_gemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 199 columns):\n",
      " #    Column                                          Dtype  \n",
      "---   ------                                          -----  \n",
      " 0    song_id                                         int64  \n",
      " 1    lowlevel.melbands_kurtosis.dmean                float64\n",
      " 2    lowlevel.melbands_kurtosis.dmean2               float64\n",
      " 3    lowlevel.melbands_kurtosis.dvar                 float64\n",
      " 4    lowlevel.melbands_kurtosis.dvar2                float64\n",
      " 5    lowlevel.melbands_kurtosis.max                  float64\n",
      " 6    lowlevel.melbands_kurtosis.mean                 float64\n",
      " 7    lowlevel.melbands_kurtosis.median               float64\n",
      " 8    lowlevel.melbands_kurtosis.min                  float64\n",
      " 9    lowlevel.melbands_kurtosis.stdev                float64\n",
      " 10   lowlevel.melbands_kurtosis.var                  float64\n",
      " 11   lowlevel.melbands_skewness.dmean                float64\n",
      " 12   lowlevel.melbands_skewness.dmean2               float64\n",
      " 13   lowlevel.melbands_skewness.dvar                 float64\n",
      " 14   lowlevel.melbands_skewness.dvar2                float64\n",
      " 15   lowlevel.melbands_skewness.max                  float64\n",
      " 16   lowlevel.melbands_skewness.mean                 float64\n",
      " 17   lowlevel.melbands_skewness.median               float64\n",
      " 18   lowlevel.melbands_skewness.min                  float64\n",
      " 19   lowlevel.melbands_skewness.stdev                float64\n",
      " 20   lowlevel.melbands_skewness.var                  float64\n",
      " 21   lowlevel.spectral_energy.dmean                  float64\n",
      " 22   lowlevel.spectral_energy.dmean2                 float64\n",
      " 23   lowlevel.spectral_energy.dvar                   float64\n",
      " 24   lowlevel.spectral_energy.dvar2                  float64\n",
      " 25   lowlevel.spectral_energy.max                    float64\n",
      " 26   lowlevel.spectral_energy.mean                   float64\n",
      " 27   lowlevel.spectral_energy.median                 float64\n",
      " 28   lowlevel.spectral_energy.min                    float64\n",
      " 29   lowlevel.spectral_energy.stdev                  float64\n",
      " 30   lowlevel.spectral_energy.var                    float64\n",
      " 31   tonal.chords_strength.dmean                     float64\n",
      " 32   tonal.chords_strength.dmean2                    float64\n",
      " 33   tonal.chords_strength.dvar                      float64\n",
      " 34   tonal.chords_strength.dvar2                     float64\n",
      " 35   tonal.chords_strength.max                       float64\n",
      " 36   tonal.chords_strength.mean                      float64\n",
      " 37   tonal.chords_strength.median                    float64\n",
      " 38   tonal.chords_strength.min                       float64\n",
      " 39   tonal.chords_strength.stdev                     float64\n",
      " 40   tonal.chords_strength.var                       float64\n",
      " 41   tonal.hpcp_entropy.dmean                        float64\n",
      " 42   tonal.hpcp_entropy.dmean2                       float64\n",
      " 43   tonal.hpcp_entropy.dvar                         float64\n",
      " 44   tonal.hpcp_entropy.dvar2                        float64\n",
      " 45   tonal.hpcp_entropy.max                          float64\n",
      " 46   tonal.hpcp_entropy.mean                         float64\n",
      " 47   tonal.hpcp_entropy.median                       float64\n",
      " 48   tonal.hpcp_entropy.min                          float64\n",
      " 49   tonal.hpcp_entropy.stdev                        float64\n",
      " 50   tonal.hpcp_entropy.var                          float64\n",
      " 51   tonal.key_edma.strength                         float64\n",
      " 52   tonal.key_temperley.strength                    float64\n",
      " 53   rhythm.beats_loudness_band_ratio.dmean_0        float64\n",
      " 54   rhythm.beats_loudness_band_ratio.dmean_1        float64\n",
      " 55   rhythm.beats_loudness_band_ratio.dmean_2        float64\n",
      " 56   rhythm.beats_loudness_band_ratio.dmean_3        float64\n",
      " 57   rhythm.beats_loudness_band_ratio.dmean_4        float64\n",
      " 58   rhythm.beats_loudness_band_ratio.dmean_5        float64\n",
      " 59   rhythm.beats_loudness_band_ratio.dmean2_0       float64\n",
      " 60   rhythm.beats_loudness_band_ratio.dmean2_1       float64\n",
      " 61   rhythm.beats_loudness_band_ratio.dmean2_2       float64\n",
      " 62   rhythm.beats_loudness_band_ratio.dmean2_3       float64\n",
      " 63   rhythm.beats_loudness_band_ratio.dmean2_4       float64\n",
      " 64   rhythm.beats_loudness_band_ratio.dmean2_5       float64\n",
      " 65   rhythm.beats_loudness_band_ratio.dvar_0         float64\n",
      " 66   rhythm.beats_loudness_band_ratio.dvar_1         float64\n",
      " 67   rhythm.beats_loudness_band_ratio.dvar_2         float64\n",
      " 68   rhythm.beats_loudness_band_ratio.dvar_3         float64\n",
      " 69   rhythm.beats_loudness_band_ratio.dvar_4         float64\n",
      " 70   rhythm.beats_loudness_band_ratio.dvar_5         float64\n",
      " 71   rhythm.beats_loudness_band_ratio.dvar2_0        float64\n",
      " 72   rhythm.beats_loudness_band_ratio.dvar2_1        float64\n",
      " 73   rhythm.beats_loudness_band_ratio.dvar2_2        float64\n",
      " 74   rhythm.beats_loudness_band_ratio.dvar2_3        float64\n",
      " 75   rhythm.beats_loudness_band_ratio.dvar2_4        float64\n",
      " 76   rhythm.beats_loudness_band_ratio.dvar2_5        float64\n",
      " 77   rhythm.beats_loudness_band_ratio.max_0          float64\n",
      " 78   rhythm.beats_loudness_band_ratio.max_1          float64\n",
      " 79   rhythm.beats_loudness_band_ratio.max_2          float64\n",
      " 80   rhythm.beats_loudness_band_ratio.max_3          float64\n",
      " 81   rhythm.beats_loudness_band_ratio.max_4          float64\n",
      " 82   rhythm.beats_loudness_band_ratio.max_5          float64\n",
      " 83   rhythm.beats_loudness_band_ratio.mean_0         float64\n",
      " 84   rhythm.beats_loudness_band_ratio.mean_1         float64\n",
      " 85   rhythm.beats_loudness_band_ratio.mean_2         float64\n",
      " 86   rhythm.beats_loudness_band_ratio.mean_3         float64\n",
      " 87   rhythm.beats_loudness_band_ratio.mean_4         float64\n",
      " 88   rhythm.beats_loudness_band_ratio.mean_5         float64\n",
      " 89   rhythm.beats_loudness_band_ratio.median_0       float64\n",
      " 90   rhythm.beats_loudness_band_ratio.median_1       float64\n",
      " 91   rhythm.beats_loudness_band_ratio.median_2       float64\n",
      " 92   rhythm.beats_loudness_band_ratio.median_3       float64\n",
      " 93   rhythm.beats_loudness_band_ratio.median_4       float64\n",
      " 94   rhythm.beats_loudness_band_ratio.median_5       float64\n",
      " 95   rhythm.beats_loudness_band_ratio.min_0          float64\n",
      " 96   rhythm.beats_loudness_band_ratio.min_1          float64\n",
      " 97   rhythm.beats_loudness_band_ratio.min_2          float64\n",
      " 98   rhythm.beats_loudness_band_ratio.min_3          float64\n",
      " 99   rhythm.beats_loudness_band_ratio.min_4          float64\n",
      " 100  rhythm.beats_loudness_band_ratio.min_5          float64\n",
      " 101  rhythm.beats_loudness_band_ratio.stdev_0        float64\n",
      " 102  rhythm.beats_loudness_band_ratio.stdev_1        float64\n",
      " 103  rhythm.beats_loudness_band_ratio.stdev_2        float64\n",
      " 104  rhythm.beats_loudness_band_ratio.stdev_3        float64\n",
      " 105  rhythm.beats_loudness_band_ratio.stdev_4        float64\n",
      " 106  rhythm.beats_loudness_band_ratio.stdev_5        float64\n",
      " 107  rhythm.beats_loudness_band_ratio.var_0          float64\n",
      " 108  rhythm.beats_loudness_band_ratio.var_1          float64\n",
      " 109  rhythm.beats_loudness_band_ratio.var_2          float64\n",
      " 110  rhythm.beats_loudness_band_ratio.var_3          float64\n",
      " 111  rhythm.beats_loudness_band_ratio.var_4          float64\n",
      " 112  rhythm.beats_loudness_band_ratio.var_5          float64\n",
      " 113  tonal.chords_histogram_0                        float64\n",
      " 114  tonal.chords_histogram_1                        float64\n",
      " 115  tonal.chords_histogram_2                        float64\n",
      " 116  tonal.chords_histogram_3                        float64\n",
      " 117  tonal.chords_histogram_4                        float64\n",
      " 118  tonal.chords_histogram_5                        float64\n",
      " 119  tonal.chords_histogram_6                        float64\n",
      " 120  tonal.chords_histogram_7                        float64\n",
      " 121  tonal.chords_histogram_8                        float64\n",
      " 122  tonal.chords_histogram_9                        float64\n",
      " 123  tonal.chords_histogram_10                       float64\n",
      " 124  tonal.chords_histogram_11                       float64\n",
      " 125  tonal.chords_histogram_12                       float64\n",
      " 126  tonal.chords_histogram_13                       float64\n",
      " 127  tonal.chords_histogram_14                       float64\n",
      " 128  tonal.chords_histogram_15                       float64\n",
      " 129  tonal.chords_histogram_16                       float64\n",
      " 130  tonal.chords_histogram_17                       float64\n",
      " 131  tonal.chords_histogram_18                       float64\n",
      " 132  tonal.chords_histogram_19                       float64\n",
      " 133  tonal.chords_histogram_20                       float64\n",
      " 134  tonal.chords_histogram_21                       float64\n",
      " 135  tonal.chords_histogram_22                       float64\n",
      " 136  tonal.chords_histogram_23                       float64\n",
      " 137  F0semitoneFrom27.5Hz_sma3nz_amean               float64\n",
      " 138  F0semitoneFrom27.5Hz_sma3nz_stddevNorm          float64\n",
      " 139  F0semitoneFrom27.5Hz_sma3nz_percentile20.0      float64\n",
      " 140  F0semitoneFrom27.5Hz_sma3nz_percentile50.0      float64\n",
      " 141  F0semitoneFrom27.5Hz_sma3nz_percentile80.0      float64\n",
      " 142  F0semitoneFrom27.5Hz_sma3nz_pctlrange0-2        float64\n",
      " 143  F0semitoneFrom27.5Hz_sma3nz_meanRisingSlope     float64\n",
      " 144  F0semitoneFrom27.5Hz_sma3nz_stddevRisingSlope   float64\n",
      " 145  F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope    float64\n",
      " 146  F0semitoneFrom27.5Hz_sma3nz_stddevFallingSlope  float64\n",
      " 147  loudness_sma3_amean                             float64\n",
      " 148  loudness_sma3_stddevNorm                        float64\n",
      " 149  loudness_sma3_percentile20.0                    float64\n",
      " 150  loudness_sma3_percentile50.0                    float64\n",
      " 151  loudness_sma3_percentile80.0                    float64\n",
      " 152  loudness_sma3_pctlrange0-2                      float64\n",
      " 153  loudness_sma3_meanRisingSlope                   float64\n",
      " 154  loudness_sma3_stddevRisingSlope                 float64\n",
      " 155  loudness_sma3_meanFallingSlope                  float64\n",
      " 156  loudness_sma3_stddevFallingSlope                float64\n",
      " 157  jitterLocal_sma3nz_amean                        float64\n",
      " 158  jitterLocal_sma3nz_stddevNorm                   float64\n",
      " 159  shimmerLocaldB_sma3nz_amean                     float64\n",
      " 160  shimmerLocaldB_sma3nz_stddevNorm                float64\n",
      " 161  HNRdBACF_sma3nz_amean                           float64\n",
      " 162  HNRdBACF_sma3nz_stddevNorm                      float64\n",
      " 163  logRelF0-H1-H2_sma3nz_amean                     float64\n",
      " 164  logRelF0-H1-H2_sma3nz_stddevNorm                float64\n",
      " 165  logRelF0-H1-A3_sma3nz_amean                     float64\n",
      " 166  logRelF0-H1-A3_sma3nz_stddevNorm                float64\n",
      " 167  F1frequency_sma3nz_amean                        float64\n",
      " 168  F1frequency_sma3nz_stddevNorm                   float64\n",
      " 169  F1bandwidth_sma3nz_amean                        float64\n",
      " 170  F1bandwidth_sma3nz_stddevNorm                   float64\n",
      " 171  F1amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 172  F1amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 173  F2frequency_sma3nz_amean                        float64\n",
      " 174  F2frequency_sma3nz_stddevNorm                   float64\n",
      " 175  F2amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 176  F2amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 177  F3frequency_sma3nz_amean                        float64\n",
      " 178  F3frequency_sma3nz_stddevNorm                   float64\n",
      " 179  F3amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 180  F3amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 181  alphaRatioV_sma3nz_amean                        float64\n",
      " 182  alphaRatioV_sma3nz_stddevNorm                   float64\n",
      " 183  hammarbergIndexV_sma3nz_amean                   float64\n",
      " 184  hammarbergIndexV_sma3nz_stddevNorm              float64\n",
      " 185  slopeV0-500_sma3nz_amean                        float64\n",
      " 186  slopeV0-500_sma3nz_stddevNorm                   float64\n",
      " 187  slopeV500-1500_sma3nz_amean                     float64\n",
      " 188  slopeV500-1500_sma3nz_stddevNorm                float64\n",
      " 189  alphaRatioUV_sma3nz_amean                       float64\n",
      " 190  hammarbergIndexUV_sma3nz_amean                  float64\n",
      " 191  slopeUV0-500_sma3nz_amean                       float64\n",
      " 192  slopeUV500-1500_sma3nz_amean                    float64\n",
      " 193  loudnessPeaksPerSec                             float64\n",
      " 194  VoicedSegmentsPerSec                            float64\n",
      " 195  MeanVoicedSegmentLengthSec                      float64\n",
      " 196  StddevVoicedSegmentLengthSec                    float64\n",
      " 197  MeanUnvoicedSegmentLength                       float64\n",
      " 198  StddevUnvoicedSegmentLength                     float64\n",
      "dtypes: float64(198), int64(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_gemaps.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>lowlevel.melbands_kurtosis.var</th>\n",
       "      <th>...</th>\n",
       "      <th>slopeUV0-500_sma3nz_amean</th>\n",
       "      <th>slopeUV500-1500_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.213586</td>\n",
       "      <td>-0.223554</td>\n",
       "      <td>-0.205962</td>\n",
       "      <td>-0.189170</td>\n",
       "      <td>0.094015</td>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.428626</td>\n",
       "      <td>-0.774106</td>\n",
       "      <td>-0.092809</td>\n",
       "      <td>-0.259203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.356517</td>\n",
       "      <td>0.241192</td>\n",
       "      <td>0.626061</td>\n",
       "      <td>0.381042</td>\n",
       "      <td>-0.336055</td>\n",
       "      <td>-0.374205</td>\n",
       "      <td>0.262637</td>\n",
       "      <td>-0.077513</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.519912</td>\n",
       "      <td>1.444784</td>\n",
       "      <td>0.436868</td>\n",
       "      <td>0.274619</td>\n",
       "      <td>0.907303</td>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.210257</td>\n",
       "      <td>0.082097</td>\n",
       "      <td>1.628937</td>\n",
       "      <td>1.007622</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.587073</td>\n",
       "      <td>-1.756907</td>\n",
       "      <td>1.246634</td>\n",
       "      <td>-2.081171</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.364584</td>\n",
       "      <td>13.929680</td>\n",
       "      <td>12.635127</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.192724</td>\n",
       "      <td>4.051296</td>\n",
       "      <td>3.004901</td>\n",
       "      <td>2.571136</td>\n",
       "      <td>2.095142</td>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.658255</td>\n",
       "      <td>-1.519185</td>\n",
       "      <td>3.431222</td>\n",
       "      <td>3.762238</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.341486</td>\n",
       "      <td>-0.316982</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>-0.483298</td>\n",
       "      <td>-0.156962</td>\n",
       "      <td>-0.196903</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.506650</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.308062</td>\n",
       "      <td>0.278054</td>\n",
       "      <td>-0.185456</td>\n",
       "      <td>-0.169303</td>\n",
       "      <td>-0.301137</td>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.294941</td>\n",
       "      <td>-0.568763</td>\n",
       "      <td>0.097731</td>\n",
       "      <td>-0.184629</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.876367</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>0.560435</td>\n",
       "      <td>-0.939150</td>\n",
       "      <td>-0.438867</td>\n",
       "      <td>-0.422328</td>\n",
       "      <td>4.102221</td>\n",
       "      <td>3.583107</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.285838</td>\n",
       "      <td>-0.283458</td>\n",
       "      <td>-0.268729</td>\n",
       "      <td>-0.246064</td>\n",
       "      <td>-0.124872</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.309671</td>\n",
       "      <td>-1.204256</td>\n",
       "      <td>-0.389019</td>\n",
       "      <td>-0.342705</td>\n",
       "      <td>...</td>\n",
       "      <td>1.506562</td>\n",
       "      <td>-0.467269</td>\n",
       "      <td>0.347808</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.097217</td>\n",
       "      <td>-0.183003</td>\n",
       "      <td>-0.546643</td>\n",
       "      <td>-0.583730</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.375556</td>\n",
       "      <td>-0.349878</td>\n",
       "      <td>-0.246022</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>0.509430</td>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.341794</td>\n",
       "      <td>-1.504569</td>\n",
       "      <td>-0.282898</td>\n",
       "      <td>-0.317327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581939</td>\n",
       "      <td>0.317231</td>\n",
       "      <td>-0.272794</td>\n",
       "      <td>0.346534</td>\n",
       "      <td>-0.296380</td>\n",
       "      <td>0.179435</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>-0.110172</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.108256</td>\n",
       "      <td>-0.003168</td>\n",
       "      <td>0.049535</td>\n",
       "      <td>-0.012648</td>\n",
       "      <td>1.028387</td>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.498191</td>\n",
       "      <td>-0.337371</td>\n",
       "      <td>0.952869</td>\n",
       "      <td>0.351178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708211</td>\n",
       "      <td>1.645550</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.734247</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-0.392740</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.131591</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.431716</td>\n",
       "      <td>-0.421521</td>\n",
       "      <td>-0.304278</td>\n",
       "      <td>-0.273126</td>\n",
       "      <td>-0.286054</td>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.361876</td>\n",
       "      <td>-1.369817</td>\n",
       "      <td>-0.487787</td>\n",
       "      <td>-0.361773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274327</td>\n",
       "      <td>1.052788</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.200898</td>\n",
       "      <td>-0.273511</td>\n",
       "      <td>-0.243128</td>\n",
       "      <td>0.080631</td>\n",
       "      <td>1.233979</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.387322</td>\n",
       "      <td>-0.389485</td>\n",
       "      <td>-0.311137</td>\n",
       "      <td>-0.281734</td>\n",
       "      <td>-0.391420</td>\n",
       "      <td>-0.296933</td>\n",
       "      <td>-0.228861</td>\n",
       "      <td>0.697357</td>\n",
       "      <td>-0.370472</td>\n",
       "      <td>-0.338635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185980</td>\n",
       "      <td>1.193421</td>\n",
       "      <td>-0.851040</td>\n",
       "      <td>0.757317</td>\n",
       "      <td>-0.403438</td>\n",
       "      <td>-0.260820</td>\n",
       "      <td>0.378455</td>\n",
       "      <td>2.290346</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.618565</td>\n",
       "      <td>-0.597394</td>\n",
       "      <td>-0.332018</td>\n",
       "      <td>-0.300291</td>\n",
       "      <td>-0.529375</td>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.458441</td>\n",
       "      <td>-0.476552</td>\n",
       "      <td>-0.619506</td>\n",
       "      <td>-0.380375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168912</td>\n",
       "      <td>1.275916</td>\n",
       "      <td>-1.050472</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>-0.394235</td>\n",
       "      <td>-0.342545</td>\n",
       "      <td>0.394655</td>\n",
       "      <td>0.652717</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.dmean  lowlevel.melbands_kurtosis.dmean2  \\\n",
       "0                           -0.213586                          -0.223554   \n",
       "1                            1.519912                           1.444784   \n",
       "2                            4.192724                           4.051296   \n",
       "3                            0.308062                           0.278054   \n",
       "4                           -0.285838                          -0.283458   \n",
       "..                                ...                                ...   \n",
       "762                         -0.375556                          -0.349878   \n",
       "763                          0.108256                          -0.003168   \n",
       "764                         -0.431716                          -0.421521   \n",
       "765                         -0.387322                          -0.389485   \n",
       "766                         -0.618565                          -0.597394   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar  lowlevel.melbands_kurtosis.dvar2  \\\n",
       "0                          -0.205962                         -0.189170   \n",
       "1                           0.436868                          0.274619   \n",
       "2                           3.004901                          2.571136   \n",
       "3                          -0.185456                         -0.169303   \n",
       "4                          -0.268729                         -0.246064   \n",
       "..                               ...                               ...   \n",
       "762                        -0.246022                         -0.213527   \n",
       "763                         0.049535                         -0.012648   \n",
       "764                        -0.304278                         -0.273126   \n",
       "765                        -0.311137                         -0.281734   \n",
       "766                        -0.332018                         -0.300291   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.max  lowlevel.melbands_kurtosis.mean  \\\n",
       "0                          0.094015                        -0.290830   \n",
       "1                          0.907303                         2.148017   \n",
       "2                          2.095142                         4.061978   \n",
       "3                         -0.301137                         0.609811   \n",
       "4                         -0.124872                        -0.407720   \n",
       "..                              ...                              ...   \n",
       "762                        0.509430                        -0.341976   \n",
       "763                        1.028387                         0.208018   \n",
       "764                       -0.286054                        -0.448361   \n",
       "765                       -0.391420                        -0.296933   \n",
       "766                       -0.529375                        -0.599766   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.median  lowlevel.melbands_kurtosis.min  \\\n",
       "0                            -0.428626                       -0.774106   \n",
       "1                             2.210257                        0.082097   \n",
       "2                             3.658255                       -1.519185   \n",
       "3                             1.294941                       -0.568763   \n",
       "4                            -0.309671                       -1.204256   \n",
       "..                                 ...                             ...   \n",
       "762                          -0.341794                       -1.504569   \n",
       "763                          -0.498191                       -0.337371   \n",
       "764                          -0.361876                       -1.369817   \n",
       "765                          -0.228861                        0.697357   \n",
       "766                          -0.458441                       -0.476552   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.stdev  lowlevel.melbands_kurtosis.var  ...  \\\n",
       "0                           -0.092809                       -0.259203  ...   \n",
       "1                            1.628937                        1.007622  ...   \n",
       "2                            3.431222                        3.762238  ...   \n",
       "3                            0.097731                       -0.184629  ...   \n",
       "4                           -0.389019                       -0.342705  ...   \n",
       "..                                ...                             ...  ...   \n",
       "762                         -0.282898                       -0.317327  ...   \n",
       "763                          0.952869                        0.351178  ...   \n",
       "764                         -0.487787                       -0.361773  ...   \n",
       "765                         -0.370472                       -0.338635  ...   \n",
       "766                         -0.619506                       -0.380375  ...   \n",
       "\n",
       "     slopeUV0-500_sma3nz_amean  slopeUV500-1500_sma3nz_amean  \\\n",
       "0                    -0.356517                      0.241192   \n",
       "1                    -1.587073                     -1.756907   \n",
       "2                    -1.341486                     -0.316982   \n",
       "3                    -1.876367                      0.705660   \n",
       "4                     1.506562                     -0.467269   \n",
       "..                         ...                           ...   \n",
       "762                   0.581939                      0.317231   \n",
       "763                   0.708211                      1.645550   \n",
       "764                   0.274327                      1.052788   \n",
       "765                  -0.185980                      1.193421   \n",
       "766                   0.168912                      1.275916   \n",
       "\n",
       "     loudnessPeaksPerSec  VoicedSegmentsPerSec  MeanVoicedSegmentLengthSec  \\\n",
       "0               0.626061              0.381042                   -0.336055   \n",
       "1               1.246634             -2.081171                    0.077305   \n",
       "2               1.247192             -0.483298                   -0.156962   \n",
       "3               0.560435             -0.939150                   -0.438867   \n",
       "4               0.347808             -0.244452                   -0.097217   \n",
       "..                   ...                   ...                         ...   \n",
       "762            -0.272794              0.346534                   -0.296380   \n",
       "763             0.506390              0.734247                   -0.376792   \n",
       "764            -0.036849              0.200898                   -0.273511   \n",
       "765            -0.851040              0.757317                   -0.403438   \n",
       "766            -1.050472              0.822118                   -0.394235   \n",
       "\n",
       "     StddevVoicedSegmentLengthSec  MeanUnvoicedSegmentLength  \\\n",
       "0                       -0.374205                   0.262637   \n",
       "1                        0.364584                  13.929680   \n",
       "2                       -0.196903                   0.509336   \n",
       "3                       -0.422328                   4.102221   \n",
       "4                       -0.183003                  -0.546643   \n",
       "..                            ...                        ...   \n",
       "762                      0.179435                   0.009848   \n",
       "763                     -0.392740                   0.211598   \n",
       "764                     -0.243128                   0.080631   \n",
       "765                     -0.260820                   0.378455   \n",
       "766                     -0.342545                   0.394655   \n",
       "\n",
       "     StddevUnvoicedSegmentLength  valence_mean_mapped  arousal_mean_mapped  \n",
       "0                      -0.077513                0.150               -0.200  \n",
       "1                      12.635127               -0.425               -0.475  \n",
       "2                       0.506650               -0.600               -0.700  \n",
       "3                       3.583107               -0.300                0.025  \n",
       "4                      -0.583730                0.450                0.400  \n",
       "..                           ...                  ...                  ...  \n",
       "762                    -0.110172                0.525                0.725  \n",
       "763                     0.131591                0.125                0.750  \n",
       "764                     1.233979                0.325                0.425  \n",
       "765                     2.290346                0.550                0.750  \n",
       "766                     0.652717                0.150                0.325  \n",
       "\n",
       "[767 rows x 200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_gemaps_whole = pd.merge(df_essentia_best_overall_opensmile_gemaps, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_overall_opensmile_gemaps_whole = df_essentia_best_overall_opensmile_gemaps_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_overall_opensmile_gemaps_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>lowlevel.melbands_kurtosis.var</th>\n",
       "      <th>...</th>\n",
       "      <th>alphaRatioUV_sma3nz_amean</th>\n",
       "      <th>hammarbergIndexUV_sma3nz_amean</th>\n",
       "      <th>slopeUV0-500_sma3nz_amean</th>\n",
       "      <th>slopeUV500-1500_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.213586</td>\n",
       "      <td>-0.223554</td>\n",
       "      <td>-0.205962</td>\n",
       "      <td>-0.189170</td>\n",
       "      <td>0.094015</td>\n",
       "      <td>-0.290830</td>\n",
       "      <td>-0.428626</td>\n",
       "      <td>-0.774106</td>\n",
       "      <td>-0.092809</td>\n",
       "      <td>-0.259203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.112443</td>\n",
       "      <td>-0.356517</td>\n",
       "      <td>0.241192</td>\n",
       "      <td>0.626061</td>\n",
       "      <td>0.381042</td>\n",
       "      <td>-0.336055</td>\n",
       "      <td>-0.374205</td>\n",
       "      <td>0.262637</td>\n",
       "      <td>-0.077513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.519912</td>\n",
       "      <td>1.444784</td>\n",
       "      <td>0.436868</td>\n",
       "      <td>0.274619</td>\n",
       "      <td>0.907303</td>\n",
       "      <td>2.148017</td>\n",
       "      <td>2.210257</td>\n",
       "      <td>0.082097</td>\n",
       "      <td>1.628937</td>\n",
       "      <td>1.007622</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.582425</td>\n",
       "      <td>2.521296</td>\n",
       "      <td>-1.587073</td>\n",
       "      <td>-1.756907</td>\n",
       "      <td>1.246634</td>\n",
       "      <td>-2.081171</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.364584</td>\n",
       "      <td>13.929680</td>\n",
       "      <td>12.635127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.192724</td>\n",
       "      <td>4.051296</td>\n",
       "      <td>3.004901</td>\n",
       "      <td>2.571136</td>\n",
       "      <td>2.095142</td>\n",
       "      <td>4.061978</td>\n",
       "      <td>3.658255</td>\n",
       "      <td>-1.519185</td>\n",
       "      <td>3.431222</td>\n",
       "      <td>3.762238</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.393781</td>\n",
       "      <td>2.181989</td>\n",
       "      <td>-1.341486</td>\n",
       "      <td>-0.316982</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>-0.483298</td>\n",
       "      <td>-0.156962</td>\n",
       "      <td>-0.196903</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.506650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.308062</td>\n",
       "      <td>0.278054</td>\n",
       "      <td>-0.185456</td>\n",
       "      <td>-0.169303</td>\n",
       "      <td>-0.301137</td>\n",
       "      <td>0.609811</td>\n",
       "      <td>1.294941</td>\n",
       "      <td>-0.568763</td>\n",
       "      <td>0.097731</td>\n",
       "      <td>-0.184629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.741640</td>\n",
       "      <td>0.768306</td>\n",
       "      <td>-1.876367</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>0.560435</td>\n",
       "      <td>-0.939150</td>\n",
       "      <td>-0.438867</td>\n",
       "      <td>-0.422328</td>\n",
       "      <td>4.102221</td>\n",
       "      <td>3.583107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.285838</td>\n",
       "      <td>-0.283458</td>\n",
       "      <td>-0.268729</td>\n",
       "      <td>-0.246064</td>\n",
       "      <td>-0.124872</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.309671</td>\n",
       "      <td>-1.204256</td>\n",
       "      <td>-0.389019</td>\n",
       "      <td>-0.342705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469653</td>\n",
       "      <td>-0.687136</td>\n",
       "      <td>1.506562</td>\n",
       "      <td>-0.467269</td>\n",
       "      <td>0.347808</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.097217</td>\n",
       "      <td>-0.183003</td>\n",
       "      <td>-0.546643</td>\n",
       "      <td>-0.583730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-0.375556</td>\n",
       "      <td>-0.349878</td>\n",
       "      <td>-0.246022</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>0.509430</td>\n",
       "      <td>-0.341976</td>\n",
       "      <td>-0.341794</td>\n",
       "      <td>-1.504569</td>\n",
       "      <td>-0.282898</td>\n",
       "      <td>-0.317327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612326</td>\n",
       "      <td>-0.673763</td>\n",
       "      <td>0.581939</td>\n",
       "      <td>0.317231</td>\n",
       "      <td>-0.272794</td>\n",
       "      <td>0.346534</td>\n",
       "      <td>-0.296380</td>\n",
       "      <td>0.179435</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>-0.110172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.108256</td>\n",
       "      <td>-0.003168</td>\n",
       "      <td>0.049535</td>\n",
       "      <td>-0.012648</td>\n",
       "      <td>1.028387</td>\n",
       "      <td>0.208018</td>\n",
       "      <td>-0.498191</td>\n",
       "      <td>-0.337371</td>\n",
       "      <td>0.952869</td>\n",
       "      <td>0.351178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502963</td>\n",
       "      <td>-0.119092</td>\n",
       "      <td>0.708211</td>\n",
       "      <td>1.645550</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.734247</td>\n",
       "      <td>-0.376792</td>\n",
       "      <td>-0.392740</td>\n",
       "      <td>0.211598</td>\n",
       "      <td>0.131591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-0.431716</td>\n",
       "      <td>-0.421521</td>\n",
       "      <td>-0.304278</td>\n",
       "      <td>-0.273126</td>\n",
       "      <td>-0.286054</td>\n",
       "      <td>-0.448361</td>\n",
       "      <td>-0.361876</td>\n",
       "      <td>-1.369817</td>\n",
       "      <td>-0.487787</td>\n",
       "      <td>-0.361773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785432</td>\n",
       "      <td>-0.498027</td>\n",
       "      <td>0.274327</td>\n",
       "      <td>1.052788</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.200898</td>\n",
       "      <td>-0.273511</td>\n",
       "      <td>-0.243128</td>\n",
       "      <td>0.080631</td>\n",
       "      <td>1.233979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.387322</td>\n",
       "      <td>-0.389485</td>\n",
       "      <td>-0.311137</td>\n",
       "      <td>-0.281734</td>\n",
       "      <td>-0.391420</td>\n",
       "      <td>-0.296933</td>\n",
       "      <td>-0.228861</td>\n",
       "      <td>0.697357</td>\n",
       "      <td>-0.370472</td>\n",
       "      <td>-0.338635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605529</td>\n",
       "      <td>-0.365492</td>\n",
       "      <td>-0.185980</td>\n",
       "      <td>1.193421</td>\n",
       "      <td>-0.851040</td>\n",
       "      <td>0.757317</td>\n",
       "      <td>-0.403438</td>\n",
       "      <td>-0.260820</td>\n",
       "      <td>0.378455</td>\n",
       "      <td>2.290346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.618565</td>\n",
       "      <td>-0.597394</td>\n",
       "      <td>-0.332018</td>\n",
       "      <td>-0.300291</td>\n",
       "      <td>-0.529375</td>\n",
       "      <td>-0.599766</td>\n",
       "      <td>-0.458441</td>\n",
       "      <td>-0.476552</td>\n",
       "      <td>-0.619506</td>\n",
       "      <td>-0.380375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880999</td>\n",
       "      <td>-0.831021</td>\n",
       "      <td>0.168912</td>\n",
       "      <td>1.275916</td>\n",
       "      <td>-1.050472</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>-0.394235</td>\n",
       "      <td>-0.342545</td>\n",
       "      <td>0.394655</td>\n",
       "      <td>0.652717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.dmean  lowlevel.melbands_kurtosis.dmean2  \\\n",
       "0                           -0.213586                          -0.223554   \n",
       "1                            1.519912                           1.444784   \n",
       "2                            4.192724                           4.051296   \n",
       "3                            0.308062                           0.278054   \n",
       "4                           -0.285838                          -0.283458   \n",
       "..                                ...                                ...   \n",
       "762                         -0.375556                          -0.349878   \n",
       "763                          0.108256                          -0.003168   \n",
       "764                         -0.431716                          -0.421521   \n",
       "765                         -0.387322                          -0.389485   \n",
       "766                         -0.618565                          -0.597394   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar  lowlevel.melbands_kurtosis.dvar2  \\\n",
       "0                          -0.205962                         -0.189170   \n",
       "1                           0.436868                          0.274619   \n",
       "2                           3.004901                          2.571136   \n",
       "3                          -0.185456                         -0.169303   \n",
       "4                          -0.268729                         -0.246064   \n",
       "..                               ...                               ...   \n",
       "762                        -0.246022                         -0.213527   \n",
       "763                         0.049535                         -0.012648   \n",
       "764                        -0.304278                         -0.273126   \n",
       "765                        -0.311137                         -0.281734   \n",
       "766                        -0.332018                         -0.300291   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.max  lowlevel.melbands_kurtosis.mean  \\\n",
       "0                          0.094015                        -0.290830   \n",
       "1                          0.907303                         2.148017   \n",
       "2                          2.095142                         4.061978   \n",
       "3                         -0.301137                         0.609811   \n",
       "4                         -0.124872                        -0.407720   \n",
       "..                              ...                              ...   \n",
       "762                        0.509430                        -0.341976   \n",
       "763                        1.028387                         0.208018   \n",
       "764                       -0.286054                        -0.448361   \n",
       "765                       -0.391420                        -0.296933   \n",
       "766                       -0.529375                        -0.599766   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.median  lowlevel.melbands_kurtosis.min  \\\n",
       "0                            -0.428626                       -0.774106   \n",
       "1                             2.210257                        0.082097   \n",
       "2                             3.658255                       -1.519185   \n",
       "3                             1.294941                       -0.568763   \n",
       "4                            -0.309671                       -1.204256   \n",
       "..                                 ...                             ...   \n",
       "762                          -0.341794                       -1.504569   \n",
       "763                          -0.498191                       -0.337371   \n",
       "764                          -0.361876                       -1.369817   \n",
       "765                          -0.228861                        0.697357   \n",
       "766                          -0.458441                       -0.476552   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.stdev  lowlevel.melbands_kurtosis.var  ...  \\\n",
       "0                           -0.092809                       -0.259203  ...   \n",
       "1                            1.628937                        1.007622  ...   \n",
       "2                            3.431222                        3.762238  ...   \n",
       "3                            0.097731                       -0.184629  ...   \n",
       "4                           -0.389019                       -0.342705  ...   \n",
       "..                                ...                             ...  ...   \n",
       "762                         -0.282898                       -0.317327  ...   \n",
       "763                          0.952869                        0.351178  ...   \n",
       "764                         -0.487787                       -0.361773  ...   \n",
       "765                         -0.370472                       -0.338635  ...   \n",
       "766                         -0.619506                       -0.380375  ...   \n",
       "\n",
       "     alphaRatioUV_sma3nz_amean  hammarbergIndexUV_sma3nz_amean  \\\n",
       "0                     0.010665                        0.112443   \n",
       "1                    -2.582425                        2.521296   \n",
       "2                    -2.393781                        2.181989   \n",
       "3                    -0.741640                        0.768306   \n",
       "4                     0.469653                       -0.687136   \n",
       "..                         ...                             ...   \n",
       "762                   0.612326                       -0.673763   \n",
       "763                   0.502963                       -0.119092   \n",
       "764                   0.785432                       -0.498027   \n",
       "765                   0.605529                       -0.365492   \n",
       "766                   0.880999                       -0.831021   \n",
       "\n",
       "     slopeUV0-500_sma3nz_amean  slopeUV500-1500_sma3nz_amean  \\\n",
       "0                    -0.356517                      0.241192   \n",
       "1                    -1.587073                     -1.756907   \n",
       "2                    -1.341486                     -0.316982   \n",
       "3                    -1.876367                      0.705660   \n",
       "4                     1.506562                     -0.467269   \n",
       "..                         ...                           ...   \n",
       "762                   0.581939                      0.317231   \n",
       "763                   0.708211                      1.645550   \n",
       "764                   0.274327                      1.052788   \n",
       "765                  -0.185980                      1.193421   \n",
       "766                   0.168912                      1.275916   \n",
       "\n",
       "     loudnessPeaksPerSec  VoicedSegmentsPerSec  MeanVoicedSegmentLengthSec  \\\n",
       "0               0.626061              0.381042                   -0.336055   \n",
       "1               1.246634             -2.081171                    0.077305   \n",
       "2               1.247192             -0.483298                   -0.156962   \n",
       "3               0.560435             -0.939150                   -0.438867   \n",
       "4               0.347808             -0.244452                   -0.097217   \n",
       "..                   ...                   ...                         ...   \n",
       "762            -0.272794              0.346534                   -0.296380   \n",
       "763             0.506390              0.734247                   -0.376792   \n",
       "764            -0.036849              0.200898                   -0.273511   \n",
       "765            -0.851040              0.757317                   -0.403438   \n",
       "766            -1.050472              0.822118                   -0.394235   \n",
       "\n",
       "     StddevVoicedSegmentLengthSec  MeanUnvoicedSegmentLength  \\\n",
       "0                       -0.374205                   0.262637   \n",
       "1                        0.364584                  13.929680   \n",
       "2                       -0.196903                   0.509336   \n",
       "3                       -0.422328                   4.102221   \n",
       "4                       -0.183003                  -0.546643   \n",
       "..                            ...                        ...   \n",
       "762                      0.179435                   0.009848   \n",
       "763                     -0.392740                   0.211598   \n",
       "764                     -0.243128                   0.080631   \n",
       "765                     -0.260820                   0.378455   \n",
       "766                     -0.342545                   0.394655   \n",
       "\n",
       "     StddevUnvoicedSegmentLength  \n",
       "0                      -0.077513  \n",
       "1                      12.635127  \n",
       "2                       0.506650  \n",
       "3                       3.583107  \n",
       "4                      -0.583730  \n",
       "..                           ...  \n",
       "762                    -0.110172  \n",
       "763                     0.131591  \n",
       "764                     1.233979  \n",
       "765                     2.290346  \n",
       "766                     0.652717  \n",
       "\n",
       "[767 rows x 198 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_overall_opensmile_gemaps.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116d33e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 198])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 198])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  # metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  # metric.update(test_pred, target_test_labels)\n",
    "  # adjusted_r2_score = metric.compute()\n",
    "  # print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2371205306603663\n",
      "Valence RMSE: 0.24559958519576652\n",
      "Arousal RMSE: 0.22832681812037509\n",
      "Test R^2 score: tensor([0.3554, 0.6155], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4854757154277499\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_overall_opensmile_gemaps_standardised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2486,  0.3805],\n",
       "        [-0.0415,  0.0731],\n",
       "        [ 0.3767,  0.4535],\n",
       "        [ 0.2797,  0.3201],\n",
       "        [ 0.1558,  0.2099],\n",
       "        [-0.0285,  0.0716],\n",
       "        [-0.1289, -0.1454],\n",
       "        [ 0.4504,  0.5520],\n",
       "        [-0.2504, -0.3877],\n",
       "        [ 0.4403,  0.5441],\n",
       "        [ 0.4393,  0.5365],\n",
       "        [-0.1977, -0.2218],\n",
       "        [-0.2445, -0.3102],\n",
       "        [-0.2214, -0.3912],\n",
       "        [-0.0762,  0.1000],\n",
       "        [-0.0263,  0.0117],\n",
       "        [ 0.5120,  0.6398],\n",
       "        [-0.0581, -0.0783],\n",
       "        [-0.1625, -0.2457],\n",
       "        [-0.0495,  0.0068],\n",
       "        [ 0.4001,  0.4830],\n",
       "        [-0.0638,  0.0068],\n",
       "        [ 0.4129,  0.4992],\n",
       "        [-0.2966, -0.4026],\n",
       "        [ 0.1297,  0.2512],\n",
       "        [ 0.2815,  0.4393],\n",
       "        [ 0.4393,  0.5367],\n",
       "        [-0.1808, -0.2316],\n",
       "        [-0.1940, -0.1425],\n",
       "        [ 0.3758,  0.4508],\n",
       "        [-0.1896, -0.2358],\n",
       "        [ 0.4492,  0.5503],\n",
       "        [ 0.2702,  0.3500],\n",
       "        [-0.0329,  0.1200],\n",
       "        [ 0.4105,  0.4958],\n",
       "        [ 0.4147,  0.5017],\n",
       "        [ 0.5181,  0.6542],\n",
       "        [ 0.4382,  0.5428],\n",
       "        [ 0.2013,  0.2658],\n",
       "        [ 0.3495,  0.4189],\n",
       "        [ 0.5487,  0.6935],\n",
       "        [ 0.4246,  0.5159],\n",
       "        [ 0.5403,  0.6793],\n",
       "        [ 0.2924,  0.3931],\n",
       "        [ 0.4391,  0.5435],\n",
       "        [ 0.5372,  0.6746],\n",
       "        [ 0.4393,  0.5366],\n",
       "        [-0.0230,  0.0303],\n",
       "        [-0.2685, -0.3844],\n",
       "        [ 0.3061,  0.3496],\n",
       "        [ 0.4279,  0.5234],\n",
       "        [-0.2276, -0.3539],\n",
       "        [ 0.2433,  0.2999],\n",
       "        [-0.0236,  0.0414],\n",
       "        [ 0.0014,  0.0393],\n",
       "        [ 0.4007,  0.5016],\n",
       "        [ 0.4027,  0.4853],\n",
       "        [ 0.4428,  0.5414],\n",
       "        [ 0.3248,  0.3876],\n",
       "        [ 0.3724,  0.4591],\n",
       "        [ 0.2476,  0.3024],\n",
       "        [ 0.4934,  0.6132],\n",
       "        [-0.0410, -0.0251],\n",
       "        [ 0.4662,  0.5818],\n",
       "        [-0.0132,  0.0101],\n",
       "        [-0.0060,  0.1495],\n",
       "        [-0.1843, -0.0628],\n",
       "        [ 0.3876,  0.4637],\n",
       "        [ 0.3158,  0.3655],\n",
       "        [ 0.1918,  0.2791],\n",
       "        [ 0.3147,  0.3926],\n",
       "        [-0.0989,  0.0426],\n",
       "        [ 0.3621,  0.4281],\n",
       "        [ 0.5456,  0.6860],\n",
       "        [-0.1428, -0.0732],\n",
       "        [ 0.5093,  0.6350],\n",
       "        [-0.1270, -0.0885],\n",
       "        [ 0.5215,  0.6603],\n",
       "        [-0.1351, -0.0564],\n",
       "        [ 0.0370,  0.1028],\n",
       "        [-0.0402,  0.0993],\n",
       "        [-0.2580, -0.4267],\n",
       "        [ 0.3457,  0.4074],\n",
       "        [ 0.4770,  0.5966],\n",
       "        [ 0.0184,  0.1139],\n",
       "        [ 0.2836,  0.3671],\n",
       "        [-0.0368,  0.0289],\n",
       "        [-0.2242, -0.3633],\n",
       "        [ 0.6309,  0.8065],\n",
       "        [-0.3159, -0.4328],\n",
       "        [ 0.3278,  0.3802],\n",
       "        [ 0.3734,  0.4472],\n",
       "        [ 0.2304,  0.2842],\n",
       "        [-0.0780, -0.1412],\n",
       "        [-0.2048, -0.1142],\n",
       "        [ 0.4090,  0.4979],\n",
       "        [ 0.3206,  0.4102],\n",
       "        [ 0.3756,  0.4525],\n",
       "        [ 0.4371,  0.5334],\n",
       "        [ 0.1344,  0.1978],\n",
       "        [-0.0266,  0.0980],\n",
       "        [ 0.0952,  0.2079],\n",
       "        [ 0.1065,  0.1557],\n",
       "        [ 0.1071,  0.2051],\n",
       "        [-0.1092, -0.1290],\n",
       "        [ 0.4277,  0.5263],\n",
       "        [ 0.1293,  0.2474],\n",
       "        [-0.0823, -0.0602],\n",
       "        [ 0.5028,  0.6300],\n",
       "        [ 0.5392,  0.6773],\n",
       "        [ 0.3927,  0.4711],\n",
       "        [ 0.5842,  0.7438],\n",
       "        [-0.1459, -0.2405],\n",
       "        [ 0.5036,  0.6285],\n",
       "        [ 0.4507,  0.5524],\n",
       "        [ 0.4302,  0.5247],\n",
       "        [ 0.3765,  0.4481],\n",
       "        [ 0.4830,  0.5983],\n",
       "        [-0.1661, -0.0626],\n",
       "        [ 0.2465,  0.2980],\n",
       "        [-0.1353,  0.0026],\n",
       "        [-0.1843, -0.2850],\n",
       "        [ 0.1195,  0.1764],\n",
       "        [ 0.2358,  0.3052],\n",
       "        [ 0.1937,  0.2631],\n",
       "        [ 0.3702,  0.4570],\n",
       "        [-0.1482, -0.0997],\n",
       "        [ 0.4113,  0.4970],\n",
       "        [-0.1681, -0.1332],\n",
       "        [ 0.2661,  0.3212],\n",
       "        [-0.1798, -0.0207],\n",
       "        [ 0.1261,  0.1886],\n",
       "        [ 0.3470,  0.4113],\n",
       "        [-0.1301, -0.1719],\n",
       "        [ 0.1002,  0.1433],\n",
       "        [-0.0938, -0.0507],\n",
       "        [-0.0821, -0.0303],\n",
       "        [ 0.4672,  0.5792],\n",
       "        [-0.1974, -0.1320],\n",
       "        [ 0.4472,  0.5546],\n",
       "        [ 0.3861,  0.4652],\n",
       "        [-0.1679, -0.2885],\n",
       "        [ 0.4591,  0.5676],\n",
       "        [ 0.3373,  0.3956],\n",
       "        [ 0.2992,  0.3471],\n",
       "        [ 0.3081,  0.4009],\n",
       "        [ 0.4771,  0.5912],\n",
       "        [-0.0031,  0.1318],\n",
       "        [-0.0304,  0.0820],\n",
       "        [-0.0752, -0.1153],\n",
       "        [ 0.3659,  0.4584],\n",
       "        [ 0.4409,  0.5386],\n",
       "        [ 0.3651,  0.4337],\n",
       "        [-0.1602, -0.0536]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3480, 0.6156], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4422214842629849\n",
      "Valence RMSE: 0.3093277932560616\n",
      "Arousal RMSE: 0.5435402456183444\n",
      "Test R^2 score: tensor([-0.0225, -1.1788], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.6006507665810992\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4383214394657176\n",
      "Valence RMSE: 0.30879495441155286\n",
      "Arousal RMSE: 0.5374914368811512\n",
      "Test R^2 score: tensor([-0.0190, -1.1306], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5747787437444661\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.43449167343726264\n",
      "Valence RMSE: 0.30831293285900263\n",
      "Arousal RMSE: 0.531515911337098\n",
      "Test R^2 score: tensor([-0.0158, -1.0835], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5496342440921932\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.43073489477900495\n",
      "Valence RMSE: 0.3078785166630654\n",
      "Arousal RMSE: 0.525619556464475\n",
      "Test R^2 score: tensor([-0.0129, -1.0375], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.525219048721126\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.42705169237207236\n",
      "Valence RMSE: 0.3074893637021925\n",
      "Arousal RMSE: 0.5198043739001463\n",
      "Test R^2 score: tensor([-0.0104, -0.9927], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5015221438635273\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.42343630495812634\n",
      "Valence RMSE: 0.3071394088669456\n",
      "Arousal RMSE: 0.5140641907719754\n",
      "Test R^2 score: tensor([-0.0081, -0.9489], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.47848921205796746\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.41998324537310905\n",
      "Valence RMSE: 0.3068471874993064\n",
      "Arousal RMSE: 0.5085436621491056\n",
      "Test R^2 score: tensor([-0.0061, -0.9073], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.45671351340015\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.41681302696413036\n",
      "Valence RMSE: 0.30663686962543824\n",
      "Arousal RMSE: 0.5034282760039545\n",
      "Test R^2 score: tensor([-0.0048, -0.8691], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4369354193157112\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.41367236671146357\n",
      "Valence RMSE: 0.3064544252465342\n",
      "Arousal RMSE: 0.4983325588481487\n",
      "Test R^2 score: tensor([-0.0036, -0.8315], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4175143403970022\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4105789191445669\n",
      "Valence RMSE: 0.3063122182748122\n",
      "Arousal RMSE: 0.4932777337640585\n",
      "Test R^2 score: tensor([-0.0026, -0.7945], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.39856557123185443\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.40752312401214225\n",
      "Valence RMSE: 0.30619350621290714\n",
      "Arousal RMSE: 0.4882578519207633\n",
      "Test R^2 score: tensor([-0.0019, -0.7582], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3800081514189354\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.40457252825853574\n",
      "Valence RMSE: 0.306094908363852\n",
      "Arousal RMSE: 0.4833878032353862\n",
      "Test R^2 score: tensor([-0.0012, -0.7233], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.362236582420832\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4017387845305259\n",
      "Valence RMSE: 0.3060136180796033\n",
      "Arousal RMSE: 0.4786896359249481\n",
      "Test R^2 score: tensor([-6.8077e-04, -6.8993e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3453033137116177\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.39907892201438483\n",
      "Valence RMSE: 0.3059567262454692\n",
      "Arousal RMSE: 0.4742556838430953\n",
      "Test R^2 score: tensor([-3.0873e-04, -6.5876e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3295365336744944\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3964238441847784\n",
      "Valence RMSE: 0.30591080285574296\n",
      "Arousal RMSE: 0.4698109291753754\n",
      "Test R^2 score: tensor([-8.4615e-06, -6.2782e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.31391320448709903\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3937721885694121\n",
      "Valence RMSE: 0.3058657357944354\n",
      "Arousal RMSE: 0.46535924257332417\n",
      "Test R^2 score: tensor([ 2.8616e-04, -5.9712e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2984146046428549\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.39112572842093357\n",
      "Valence RMSE: 0.3057878387369944\n",
      "Arousal RMSE: 0.4609256648811928\n",
      "Test R^2 score: tensor([ 0.0008, -0.5668], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.28301645401896464\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3884872759293186\n",
      "Valence RMSE: 0.3057316420016457\n",
      "Arousal RMSE: 0.45647879490392823\n",
      "Test R^2 score: tensor([ 0.0012, -0.5367], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2677894789437875\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.38584668622617313\n",
      "Valence RMSE: 0.3056740497149148\n",
      "Arousal RMSE: 0.4520162672673646\n",
      "Test R^2 score: tensor([ 0.0015, -0.5068], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2526516190927607\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.38320088581066186\n",
      "Valence RMSE: 0.305610635504679\n",
      "Arousal RMSE: 0.4475354480245985\n",
      "Test R^2 score: tensor([ 0.0020, -0.4771], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2375812739392899\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3805488363792825\n",
      "Valence RMSE: 0.3055342981399211\n",
      "Arousal RMSE: 0.4430390799911421\n",
      "Test R^2 score: tensor([ 0.0025, -0.4476], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.22256604523905432\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3779574208640199\n",
      "Valence RMSE: 0.3054771258087805\n",
      "Arousal RMSE: 0.4386198235145903\n",
      "Test R^2 score: tensor([ 0.0028, -0.4188], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.20801195837692682\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3753609377786752\n",
      "Valence RMSE: 0.3053978804696081\n",
      "Arousal RMSE: 0.4341932770378232\n",
      "Test R^2 score: tensor([ 0.0033, -0.3904], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1935065569191195\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3728524985441851\n",
      "Valence RMSE: 0.3051420572017805\n",
      "Arousal RMSE: 0.43003057596867283\n",
      "Test R^2 score: tensor([ 0.0050, -0.3638], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1794063064631508\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37040279724312736\n",
      "Valence RMSE: 0.3049959745944773\n",
      "Arousal RMSE: 0.42588017081361224\n",
      "Test R^2 score: tensor([ 0.0060, -0.3376], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.16583076834570798\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36792202974263993\n",
      "Valence RMSE: 0.3048282908708409\n",
      "Arousal RMSE: 0.4216787320041764\n",
      "Test R^2 score: tensor([ 0.0071, -0.3114], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1521534153242532\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3654018344210609\n",
      "Valence RMSE: 0.3046270535095111\n",
      "Arousal RMSE: 0.41741988388991036\n",
      "Test R^2 score: tensor([ 0.0084, -0.2850], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.13832057107402368\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3628977728048471\n",
      "Valence RMSE: 0.3042864656037775\n",
      "Arousal RMSE: 0.4132787604798943\n",
      "Test R^2 score: tensor([ 0.0106, -0.2596], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12452747312173723\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36056237582559203\n",
      "Valence RMSE: 0.3038422001994142\n",
      "Arousal RMSE: 0.4095001478631846\n",
      "Test R^2 score: tensor([ 0.0135, -0.2367], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.11161971949922828\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3581822254182623\n",
      "Valence RMSE: 0.3034593594292262\n",
      "Arousal RMSE: 0.40558775916681883\n",
      "Test R^2 score: tensor([ 0.0160, -0.2132], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0986183230252603\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3557189578631103\n",
      "Valence RMSE: 0.3030244862710928\n",
      "Arousal RMSE: 0.40155711260862353\n",
      "Test R^2 score: tensor([ 0.0188, -0.1892], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0852126102004731\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35303739021264346\n",
      "Valence RMSE: 0.3026367635764157\n",
      "Arousal RMSE: 0.3970916608395362\n",
      "Test R^2 score: tensor([ 0.0213, -0.1629], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07080716460267683\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3501763237252091\n",
      "Valence RMSE: 0.30214049989895464\n",
      "Arousal RMSE: 0.39237486376705466\n",
      "Test R^2 score: tensor([ 0.0245, -0.1354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.055472315543300454\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3471080670518871\n",
      "Valence RMSE: 0.30142902963392526\n",
      "Arousal RMSE: 0.38743846029910994\n",
      "Test R^2 score: tensor([ 0.0291, -0.1070], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03898306772820359\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3438521888447178\n",
      "Valence RMSE: 0.3005604802238415\n",
      "Arousal RMSE: 0.38227222404226635\n",
      "Test R^2 score: tensor([ 0.0347, -0.0777], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.021526157964638637\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3403738601607181\n",
      "Valence RMSE: 0.2995519731560077\n",
      "Arousal RMSE: 0.37679881202009985\n",
      "Test R^2 score: tensor([ 0.0411, -0.0471], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0029720861780538343\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33663750618879346\n",
      "Valence RMSE: 0.2983948091358885\n",
      "Arousal RMSE: 0.37095843301746967\n",
      "Test R^2 score: tensor([ 0.0485, -0.0149], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01682875365757619\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33257443137053\n",
      "Valence RMSE: 0.2970237250013286\n",
      "Arousal RMSE: 0.3646757622727317\n",
      "Test R^2 score: tensor([0.0573, 0.0192], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0382332000282703\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32810478679040667\n",
      "Valence RMSE: 0.2954051910411889\n",
      "Arousal RMSE: 0.357828555785414\n",
      "Test R^2 score: tensor([0.0675, 0.0557], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.06159887267844444\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3232554260010928\n",
      "Valence RMSE: 0.2936142051797123\n",
      "Arousal RMSE: 0.3503981155699639\n",
      "Test R^2 score: tensor([0.0788, 0.0945], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08664044883371425\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.318008759134198\n",
      "Valence RMSE: 0.2916588705855535\n",
      "Arousal RMSE: 0.3423364499741505\n",
      "Test R^2 score: tensor([0.0910, 0.1357], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.11334806429604682\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31244189429010405\n",
      "Valence RMSE: 0.28954624369494114\n",
      "Arousal RMSE: 0.33377065086272467\n",
      "Test R^2 score: tensor([0.1041, 0.1784], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14126421386586724\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3066772905803641\n",
      "Valence RMSE: 0.2871765905928597\n",
      "Arousal RMSE: 0.3250100412770152\n",
      "Test R^2 score: tensor([0.1187, 0.2210], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1698477407952224\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.300845487363591\n",
      "Valence RMSE: 0.28457009856878507\n",
      "Arousal RMSE: 0.316284481969358\n",
      "Test R^2 score: tensor([0.1346, 0.2622], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19844399974795496\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2953002742638735\n",
      "Valence RMSE: 0.28198423984752763\n",
      "Arousal RMSE: 0.3080412187325745\n",
      "Test R^2 score: tensor([0.1503, 0.3002], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22524917516289977\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2905557654496539\n",
      "Valence RMSE: 0.2794930321861682\n",
      "Arousal RMSE: 0.3012124675896568\n",
      "Test R^2 score: tensor([0.1653, 0.3309], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24806425528670079\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28752465641817376\n",
      "Valence RMSE: 0.2775974840559443\n",
      "Arousal RMSE: 0.2971203341115978\n",
      "Test R^2 score: tensor([0.1765, 0.3489], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26273504270096565\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2871164106401977\n",
      "Valence RMSE: 0.2765737547123787\n",
      "Arousal RMSE: 0.2972854263533923\n",
      "Test R^2 score: tensor([0.1826, 0.3482], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26540437396521777\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.288650196599024\n",
      "Valence RMSE: 0.27612953397585\n",
      "Arousal RMSE: 0.30064988351833827\n",
      "Test R^2 score: tensor([0.1852, 0.3334], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25929799137874815\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28856692166256587\n",
      "Valence RMSE: 0.27456614293971104\n",
      "Arousal RMSE: 0.30191914431984773\n",
      "Test R^2 score: tensor([0.1944, 0.3277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.261077801819154\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28445931449934914\n",
      "Valence RMSE: 0.27092893772684806\n",
      "Arousal RMSE: 0.2973747028801894\n",
      "Test R^2 score: tensor([0.2156, 0.3478], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.281721387973646\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2776371921542916\n",
      "Valence RMSE: 0.26613291017429785\n",
      "Arousal RMSE: 0.2886833820239696\n",
      "Test R^2 score: tensor([0.2431, 0.3854], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31426627108068494\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2711788718800367\n",
      "Valence RMSE: 0.26197866500933564\n",
      "Arousal RMSE: 0.2800770254558289\n",
      "Test R^2 score: tensor([0.2666, 0.4215], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34403832033163284\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26705318248162374\n",
      "Valence RMSE: 0.2597663562882789\n",
      "Arousal RMSE: 0.2741463928047163\n",
      "Test R^2 score: tensor([0.2789, 0.4457], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36232588076625777\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26552398625097035\n",
      "Valence RMSE: 0.25944472964563436\n",
      "Arousal RMSE: 0.27146713762132363\n",
      "Test R^2 score: tensor([0.2807, 0.4565], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36860861445189663\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2649798884251899\n",
      "Valence RMSE: 0.2596809954412196\n",
      "Arousal RMSE: 0.27017487512037347\n",
      "Test R^2 score: tensor([0.2794, 0.4617], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37053431621942556\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26401878160763037\n",
      "Valence RMSE: 0.25952660687171514\n",
      "Arousal RMSE: 0.26843579196673734\n",
      "Test R^2 score: tensor([0.2803, 0.4686], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37441662893812216\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26234667277060136\n",
      "Valence RMSE: 0.25892489512296574\n",
      "Arousal RMSE: 0.26572439126503083\n",
      "Test R^2 score: tensor([0.2836, 0.4793], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3814240768123193\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26026223030766055\n",
      "Valence RMSE: 0.2582977751263832\n",
      "Arousal RMSE: 0.26221196848007505\n",
      "Test R^2 score: tensor([0.2871, 0.4929], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38999495617833074\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25815983907493717\n",
      "Valence RMSE: 0.25801822058337126\n",
      "Arousal RMSE: 0.25830137992156704\n",
      "Test R^2 score: tensor([0.2886, 0.5079], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39827206901131923\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2564655075424413\n",
      "Valence RMSE: 0.25838759858089655\n",
      "Arousal RMSE: 0.25452890212626444\n",
      "Test R^2 score: tensor([0.2866, 0.5222], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4043868483561567\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2552581240093686\n",
      "Valence RMSE: 0.2591601527477577\n",
      "Arousal RMSE: 0.25129551323754923\n",
      "Test R^2 score: tensor([0.2823, 0.5343], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4082815148953235\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2541308417119876\n",
      "Valence RMSE: 0.2596785418637798\n",
      "Arousal RMSE: 0.2484593011219096\n",
      "Test R^2 score: tensor([0.2794, 0.5447], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41207113619840613\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25261628332144653\n",
      "Valence RMSE: 0.25916670625874105\n",
      "Arousal RMSE: 0.2458914223092748\n",
      "Test R^2 score: tensor([0.2823, 0.5541], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41817104530097216\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2505192619759534\n",
      "Valence RMSE: 0.25721582835204393\n",
      "Arousal RMSE: 0.24363870564244236\n",
      "Test R^2 score: tensor([0.2930, 0.5622], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42762001729080334\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2479326471183873\n",
      "Valence RMSE: 0.2539948671143176\n",
      "Arousal RMSE: 0.24171843639623694\n",
      "Test R^2 score: tensor([0.3106, 0.5691], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43985451530653435\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24541656079318\n",
      "Valence RMSE: 0.25024296966285703\n",
      "Arousal RMSE: 0.2404933112529736\n",
      "Test R^2 score: tensor([0.3308, 0.5735], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4521410877855313\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24368716910348562\n",
      "Valence RMSE: 0.2469327907988855\n",
      "Arousal RMSE: 0.24039773210164742\n",
      "Test R^2 score: tensor([0.3484, 0.5738], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46110374578015006\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2426579935488251\n",
      "Valence RMSE: 0.24439610518127852\n",
      "Arousal RMSE: 0.2409073420186789\n",
      "Test R^2 score: tensor([0.3617, 0.5720], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4668585033020438\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24196804596932003\n",
      "Valence RMSE: 0.24262082597448284\n",
      "Arousal RMSE: 0.24131350012770034\n",
      "Test R^2 score: tensor([0.3710, 0.5705], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4707557792433853\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24128081630600182\n",
      "Valence RMSE: 0.24134373372834877\n",
      "Arousal RMSE: 0.24121788247275755\n",
      "Test R^2 score: tensor([0.3776, 0.5709], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47422823959329174\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24057072722389733\n",
      "Valence RMSE: 0.24036150477500992\n",
      "Arousal RMSE: 0.2407797678717265\n",
      "Test R^2 score: tensor([0.3826, 0.5724], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4775349316947955\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24022711957374765\n",
      "Valence RMSE: 0.24001328952996154\n",
      "Arousal RMSE: 0.2404407594531471\n",
      "Test R^2 score: tensor([0.3844, 0.5736], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4790302407581246\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24053701470733957\n",
      "Valence RMSE: 0.2405656599584619\n",
      "Arousal RMSE: 0.2405083660444839\n",
      "Test R^2 score: tensor([0.3816, 0.5734], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4774920048526089\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24134211092531005\n",
      "Valence RMSE: 0.24199063480946814\n",
      "Arousal RMSE: 0.24069183965452573\n",
      "Test R^2 score: tensor([0.3742, 0.5727], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4734924456558082\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.242291338640345\n",
      "Valence RMSE: 0.24396259334894577\n",
      "Arousal RMSE: 0.24060847575826913\n",
      "Test R^2 score: tensor([0.3640, 0.5730], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46852032388634635\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2430849679147601\n",
      "Valence RMSE: 0.24605727790450624\n",
      "Arousal RMSE: 0.24007586143230747\n",
      "Test R^2 score: tensor([0.3530, 0.5749], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4639801510107663\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2436867461696578\n",
      "Valence RMSE: 0.2478296514769214\n",
      "Arousal RMSE: 0.23947217868958834\n",
      "Test R^2 score: tensor([0.3437, 0.5771], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4603706630095585\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24402797569618165\n",
      "Valence RMSE: 0.24901573815487332\n",
      "Arousal RMSE: 0.2389361169767685\n",
      "Test R^2 score: tensor([0.3374, 0.5790], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4581677097681314\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24395610812559892\n",
      "Valence RMSE: 0.249425405112475\n",
      "Arousal RMSE: 0.23836134893907518\n",
      "Test R^2 score: tensor([0.3352, 0.5810], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4580883052929333\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24340040849253838\n",
      "Valence RMSE: 0.24906889323771242\n",
      "Arousal RMSE: 0.2375967258402567\n",
      "Test R^2 score: tensor([0.3371, 0.5837], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4603798351827045\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24251524362329527\n",
      "Valence RMSE: 0.24821200059521675\n",
      "Arousal RMSE: 0.23668140936679552\n",
      "Test R^2 score: tensor([0.3416, 0.5869], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46425735751653907\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24159426568493064\n",
      "Valence RMSE: 0.24726062426183756\n",
      "Arousal RMSE: 0.23579177702652845\n",
      "Test R^2 score: tensor([0.3467, 0.5900], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4683258880470131\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24080771477958463\n",
      "Valence RMSE: 0.2465189029971314\n",
      "Arousal RMSE: 0.2349577439877701\n",
      "Test R^2 score: tensor([0.3506, 0.5929], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4717305200910944\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2403365516421979\n",
      "Valence RMSE: 0.24612738998638126\n",
      "Arousal RMSE: 0.23440269624946186\n",
      "Test R^2 score: tensor([0.3527, 0.5948], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47372171429335835\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24025513773920879\n",
      "Valence RMSE: 0.24615861101345735\n",
      "Arousal RMSE: 0.23420290485836795\n",
      "Test R^2 score: tensor([0.3525, 0.5955], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4739848285663006\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24049806842132632\n",
      "Valence RMSE: 0.24639513488410691\n",
      "Arousal RMSE: 0.23445272302581743\n",
      "Test R^2 score: tensor([0.3512, 0.5946], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47293064157567066\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24077290837992146\n",
      "Valence RMSE: 0.24672970580011422\n",
      "Arousal RMSE: 0.23466495071748825\n",
      "Test R^2 score: tensor([0.3495, 0.5939], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4716820032021659\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24060261785200168\n",
      "Valence RMSE: 0.2469722955303207\n",
      "Arousal RMSE: 0.23405966050337815\n",
      "Test R^2 score: tensor([0.3482, 0.5960], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4720882806742422\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24026887440984562\n",
      "Valence RMSE: 0.24705708654270556\n",
      "Arousal RMSE: 0.23328321844783098\n",
      "Test R^2 score: tensor([0.3478, 0.5986], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4732025229707596\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23983922599630172\n",
      "Valence RMSE: 0.24649495959445444\n",
      "Arousal RMSE: 0.2329934409968196\n",
      "Test R^2 score: tensor([0.3507, 0.5996], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47518311438372435\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23940614900997567\n",
      "Valence RMSE: 0.2454946802357555\n",
      "Arousal RMSE: 0.2331586806094028\n",
      "Test R^2 score: tensor([0.3560, 0.5991], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47752850785682627\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23895475154881568\n",
      "Valence RMSE: 0.2448465342984242\n",
      "Arousal RMSE: 0.2329139781497947\n",
      "Test R^2 score: tensor([0.3594, 0.5999], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4796471303474345\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23848873607004065\n",
      "Valence RMSE: 0.2447552026214624\n",
      "Arousal RMSE: 0.23205310869345813\n",
      "Test R^2 score: tensor([0.3599, 0.6029], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4813620574857491\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23836183907878822\n",
      "Valence RMSE: 0.24512375577859824\n",
      "Arousal RMSE: 0.23140241358084057\n",
      "Test R^2 score: tensor([0.3579, 0.6051], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4815094270603986\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23853273307327305\n",
      "Valence RMSE: 0.2455778663075029\n",
      "Arousal RMSE: 0.2312730876575675\n",
      "Test R^2 score: tensor([0.3555, 0.6055], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48053947841566996\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23893879129043105\n",
      "Valence RMSE: 0.24624432889596934\n",
      "Arousal RMSE: 0.23140272784312205\n",
      "Test R^2 score: tensor([0.3520, 0.6051], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4785669684623934\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23917281342673796\n",
      "Valence RMSE: 0.24737360954654172\n",
      "Arousal RMSE: 0.23068065949454353\n",
      "Test R^2 score: tensor([0.3461, 0.6076], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47681895950047887\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23938753507095634\n",
      "Valence RMSE: 0.24842684607952412\n",
      "Arousal RMSE: 0.2299932304257608\n",
      "Test R^2 score: tensor([0.3405, 0.6099], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4751966312934613\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23959528243159023\n",
      "Valence RMSE: 0.2489338438714894\n",
      "Arousal RMSE: 0.2298776633392464\n",
      "Test R^2 score: tensor([0.3378, 0.6103], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47404531433165054\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23970309149032729\n",
      "Valence RMSE: 0.24903749453414453\n",
      "Arousal RMSE: 0.22999015295485228\n",
      "Test R^2 score: tensor([0.3373, 0.6099], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47357878116762886\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2396082263375532\n",
      "Valence RMSE: 0.24941729768814638\n",
      "Arousal RMSE: 0.22938006860056254\n",
      "Test R^2 score: tensor([0.3352, 0.6120], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47360070834461215\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23959111993596655\n",
      "Valence RMSE: 0.2494627557572707\n",
      "Arousal RMSE: 0.22929488218084967\n",
      "Test R^2 score: tensor([0.3350, 0.6123], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47362361987110435\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23966333513240365\n",
      "Valence RMSE: 0.24908849288043922\n",
      "Arousal RMSE: 0.2298520200653565\n",
      "Test R^2 score: tensor([0.3370, 0.6104], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.473677276290508\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2395613351548361\n",
      "Valence RMSE: 0.2490510942660448\n",
      "Arousal RMSE: 0.2296798185457771\n",
      "Test R^2 score: tensor([0.3372, 0.6110], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4740686126025625\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2394883568267614\n",
      "Valence RMSE: 0.24903490458647193\n",
      "Arousal RMSE: 0.2295451206381246\n",
      "Test R^2 score: tensor([0.3373, 0.6114], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4743397927378627\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23923971924183043\n",
      "Valence RMSE: 0.24841071390439806\n",
      "Arousal RMSE: 0.22970285967598883\n",
      "Test R^2 score: tensor([0.3406, 0.6109], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4757316689333666\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23888944867405004\n",
      "Valence RMSE: 0.2478623438373401\n",
      "Arousal RMSE: 0.22956610351496445\n",
      "Test R^2 score: tensor([0.3435, 0.6113], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47741731877914567\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23860890292126952\n",
      "Valence RMSE: 0.2480017558982923\n",
      "Arousal RMSE: 0.22883082436146462\n",
      "Test R^2 score: tensor([0.3428, 0.6138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47829082500973547\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23816870587998212\n",
      "Valence RMSE: 0.2474204384985269\n",
      "Arousal RMSE: 0.2285427564686475\n",
      "Test R^2 score: tensor([0.3458, 0.6148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4803154344354243\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.237842245154292\n",
      "Valence RMSE: 0.2470190998531697\n",
      "Arousal RMSE: 0.22829680564519175\n",
      "Test R^2 score: tensor([0.3480, 0.6156], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4817900097297661\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2377393722518685\n",
      "Valence RMSE: 0.24741257793646254\n",
      "Arousal RMSE: 0.2276555172102334\n",
      "Test R^2 score: tensor([0.3459, 0.6178], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48182875004136655\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2373412802700884\n",
      "Valence RMSE: 0.24645587091221113\n",
      "Arousal RMSE: 0.227862393416274\n",
      "Test R^2 score: tensor([0.3509, 0.6171], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4840057511316761\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23713682394480726\n",
      "Valence RMSE: 0.24562845734584787\n",
      "Arousal RMSE: 0.22832960273070163\n",
      "Test R^2 score: tensor([0.3553, 0.6155], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48539524783466365\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2371205306603663\n",
      "Valence RMSE: 0.24559958519576652\n",
      "Arousal RMSE: 0.22832681812037509\n",
      "Test R^2 score: tensor([0.3554, 0.6155], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4854757154277499\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2371807744214886\n",
      "Valence RMSE: 0.24430078061885718\n",
      "Arousal RMSE: 0.22984030999668256\n",
      "Test R^2 score: tensor([0.3622, 0.6104], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48631835945739543\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23728998777454846\n",
      "Valence RMSE: 0.2447302140599723\n",
      "Arousal RMSE: 0.22960879539392828\n",
      "Test R^2 score: tensor([0.3600, 0.6112], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48558853304778166\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23768683771232568\n",
      "Valence RMSE: 0.2443682485312405\n",
      "Arousal RMSE: 0.2308120983682285\n",
      "Test R^2 score: tensor([0.3619, 0.6071], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4844914792848676\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23811493506242334\n",
      "Valence RMSE: 0.2444783195054852\n",
      "Arousal RMSE: 0.23157676025745344\n",
      "Test R^2 score: tensor([0.3613, 0.6045], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.482900198559101\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23849504940658992\n",
      "Valence RMSE: 0.24498227148368995\n",
      "Arousal RMSE: 0.23182636571709253\n",
      "Test R^2 score: tensor([0.3587, 0.6036], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4811557496448941\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23877923821920174\n",
      "Valence RMSE: 0.2440293940966253\n",
      "Arousal RMSE: 0.2334110195040468\n",
      "Test R^2 score: tensor([0.3636, 0.5982], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4809268530665337\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23919838342165653\n",
      "Valence RMSE: 0.2456466165573039\n",
      "Arousal RMSE: 0.23257143641689224\n",
      "Test R^2 score: tensor([0.3552, 0.6011], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4781383181528492\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23974372976902542\n",
      "Valence RMSE: 0.24345513556670617\n",
      "Arousal RMSE: 0.23597395808292848\n",
      "Test R^2 score: tensor([0.3666, 0.5893], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47798652440693407\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23937860647942452\n",
      "Valence RMSE: 0.24536707134430255\n",
      "Arousal RMSE: 0.23323643536116379\n",
      "Test R^2 score: tensor([0.3567, 0.5988], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.477729459193303\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23937377008125138\n",
      "Valence RMSE: 0.24487603505434544\n",
      "Arousal RMSE: 0.23374201817791895\n",
      "Test R^2 score: tensor([0.3592, 0.5971], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47814506088163383\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24009393960981942\n",
      "Valence RMSE: 0.24363788357563756\n",
      "Arousal RMSE: 0.23649689503566704\n",
      "Test R^2 score: tensor([0.3657, 0.5875], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4765998423555981\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24029005686116037\n",
      "Valence RMSE: 0.24541866024032402\n",
      "Arousal RMSE: 0.23504957787352815\n",
      "Test R^2 score: tensor([0.3564, 0.5925], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47446325412822377\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2404575288698233\n",
      "Valence RMSE: 0.2448992892367422\n",
      "Arousal RMSE: 0.23593216082531504\n",
      "Test R^2 score: tensor([0.3591, 0.5895], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4742910694451333\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24115896560048133\n",
      "Valence RMSE: 0.2441063244678993\n",
      "Arousal RMSE: 0.23817513668257007\n",
      "Test R^2 score: tensor([0.3632, 0.5816], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47244158133309433\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2416468668476514\n",
      "Valence RMSE: 0.2458235025818474\n",
      "Arousal RMSE: 0.23739676091506726\n",
      "Test R^2 score: tensor([0.3543, 0.5844], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4693115519615425\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24178229998324577\n",
      "Valence RMSE: 0.2449977060663496\n",
      "Arousal RMSE: 0.23852355269994413\n",
      "Test R^2 score: tensor([0.3586, 0.5804], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46949970366207006\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24227297330755806\n",
      "Valence RMSE: 0.2449004509485755\n",
      "Arousal RMSE: 0.23961668622146046\n",
      "Test R^2 score: tensor([0.3591, 0.5766], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4678269317763289\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24318924085006724\n",
      "Valence RMSE: 0.24645854896598327\n",
      "Arousal RMSE: 0.23987537883669108\n",
      "Test R^2 score: tensor([0.3509, 0.5756], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46327902026919976\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24343368851403865\n",
      "Valence RMSE: 0.24508694120096505\n",
      "Arousal RMSE: 0.24176913090767957\n",
      "Test R^2 score: tensor([0.3581, 0.5689], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4635178958659823\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.243816924676681\n",
      "Valence RMSE: 0.24669654266805102\n",
      "Arousal RMSE: 0.240902887805894\n",
      "Test R^2 score: tensor([0.3497, 0.5720], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4608302961866308\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24404392499065594\n",
      "Valence RMSE: 0.24655435920219315\n",
      "Arousal RMSE: 0.24150739659084094\n",
      "Test R^2 score: tensor([0.3504, 0.5698], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4601296637726017\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24444375004000182\n",
      "Valence RMSE: 0.24593174815392643\n",
      "Arousal RMSE: 0.24294663841508787\n",
      "Test R^2 score: tensor([0.3537, 0.5647], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45919688699949013\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24505442483206138\n",
      "Valence RMSE: 0.24737058474252716\n",
      "Arousal RMSE: 0.24271616358142503\n",
      "Test R^2 score: tensor([0.3461, 0.5655], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45581728537022165\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2449760507311674\n",
      "Valence RMSE: 0.24596893689932883\n",
      "Arousal RMSE: 0.24397912399279778\n",
      "Test R^2 score: tensor([0.3535, 0.5610], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45724528932982184\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2452112742069058\n",
      "Valence RMSE: 0.24687214636327035\n",
      "Arousal RMSE: 0.24353907560459512\n",
      "Test R^2 score: tensor([0.3487, 0.5626], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45565800009411833\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24568523016868876\n",
      "Valence RMSE: 0.247273610089743\n",
      "Arousal RMSE: 0.24408651416919558\n",
      "Test R^2 score: tensor([0.3466, 0.5606], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45361369557222353\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24572765590529072\n",
      "Valence RMSE: 0.24629539889696275\n",
      "Arousal RMSE: 0.24515859812701632\n",
      "Test R^2 score: tensor([0.3518, 0.5567], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45425924139561763\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24630289884464457\n",
      "Valence RMSE: 0.24773590037634738\n",
      "Arousal RMSE: 0.2448615111104049\n",
      "Test R^2 score: tensor([0.3442, 0.5578], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45099370835938163\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2457999065724686\n",
      "Valence RMSE: 0.24653402065729677\n",
      "Arousal RMSE: 0.24506359338060837\n",
      "Test R^2 score: tensor([0.3505, 0.5571], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4538026457455506\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24591470329930468\n",
      "Valence RMSE: 0.2468664689089103\n",
      "Arousal RMSE: 0.24495923972370603\n",
      "Test R^2 score: tensor([0.3488, 0.5575], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4531147967740767\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24634946874810562\n",
      "Valence RMSE: 0.2477850160140778\n",
      "Arousal RMSE: 0.24490550696922056\n",
      "Test R^2 score: tensor([0.3439, 0.5577], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4507842149159103\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2461155526870771\n",
      "Valence RMSE: 0.24688282917405552\n",
      "Arousal RMSE: 0.24534587668828872\n",
      "Test R^2 score: tensor([0.3487, 0.5561], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45237260105253996\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24678083446041602\n",
      "Valence RMSE: 0.24823032606787537\n",
      "Arousal RMSE: 0.24532277866964325\n",
      "Test R^2 score: tensor([0.3415, 0.5562], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4488497479656676\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24684931940556704\n",
      "Valence RMSE: 0.24768452309170536\n",
      "Arousal RMSE: 0.24601128023490665\n",
      "Test R^2 score: tensor([0.3444, 0.5537], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.449048527377186\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2471855326623098\n",
      "Valence RMSE: 0.247897372849372\n",
      "Arousal RMSE: 0.2464716366024276\n",
      "Test R^2 score: tensor([0.3433, 0.5520], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44764890856568557\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24767366644786398\n",
      "Valence RMSE: 0.24856014101437532\n",
      "Arousal RMSE: 0.2467840075903814\n",
      "Test R^2 score: tensor([0.3398, 0.5508], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44532271027329534\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24792931213975095\n",
      "Valence RMSE: 0.24823481838396766\n",
      "Arousal RMSE: 0.24762342897645753\n",
      "Test R^2 score: tensor([0.3415, 0.5478], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44465587284132335\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24884258421312905\n",
      "Valence RMSE: 0.24977110858473406\n",
      "Arousal RMSE: 0.24791058217044595\n",
      "Test R^2 score: tensor([0.3333, 0.5467], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.440043345444833\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24874780664310508\n",
      "Valence RMSE: 0.24880606068714536\n",
      "Arousal RMSE: 0.24868953895340212\n",
      "Test R^2 score: tensor([0.3385, 0.5439], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44118769999820123\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24940882353678412\n",
      "Valence RMSE: 0.2504183784247589\n",
      "Arousal RMSE: 0.24839516553894458\n",
      "Test R^2 score: tensor([0.3299, 0.5450], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4374266688506741\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2489348294107525\n",
      "Valence RMSE: 0.2492332429663589\n",
      "Arousal RMSE: 0.24863605769878358\n",
      "Test R^2 score: tensor([0.3362, 0.5441], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.440149038730953\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24921669248640393\n",
      "Valence RMSE: 0.2504967418725832\n",
      "Arousal RMSE: 0.24793003436241062\n",
      "Test R^2 score: tensor([0.3295, 0.5467], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43806821623044245\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24908437591697663\n",
      "Valence RMSE: 0.24998566935761024\n",
      "Arousal RMSE: 0.24817980934743067\n",
      "Test R^2 score: tensor([0.3322, 0.5458], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4389779231533567\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2494811766901764\n",
      "Valence RMSE: 0.25072903255360496\n",
      "Arousal RMSE: 0.24822704784161487\n",
      "Test R^2 score: tensor([0.3282, 0.5456], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4369027252762514\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24976204697524257\n",
      "Valence RMSE: 0.25105890259764857\n",
      "Arousal RMSE: 0.2484584223667835\n",
      "Test R^2 score: tensor([0.3265, 0.5447], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4355945620311618\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2498960935983635\n",
      "Valence RMSE: 0.25095063378277627\n",
      "Arousal RMSE: 0.24883708444576147\n",
      "Test R^2 score: tensor([0.3270, 0.5433], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43519058589102116\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2504796840458431\n",
      "Valence RMSE: 0.252117742140428\n",
      "Arousal RMSE: 0.24883084281781354\n",
      "Test R^2 score: tensor([0.3208, 0.5434], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4320649843234866\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25048205419485764\n",
      "Valence RMSE: 0.25137025881748926\n",
      "Arousal RMSE: 0.24959068878742177\n",
      "Test R^2 score: tensor([0.3248, 0.5406], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4326792757592734\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25207625666404376\n",
      "Valence RMSE: 0.2542944765501049\n",
      "Arousal RMSE: 0.24983834282115688\n",
      "Test R^2 score: tensor([0.3090, 0.5397], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4243226539876946\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25167413920831644\n",
      "Valence RMSE: 0.25188111453817497\n",
      "Arousal RMSE: 0.2514669935230086\n",
      "Test R^2 score: tensor([0.3220, 0.5336], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42783891813866487\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25209341104927535\n",
      "Valence RMSE: 0.2546049938516804\n",
      "Arousal RMSE: 0.24955655249803327\n",
      "Test R^2 score: tensor([0.3073, 0.5407], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4239972633892937\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25078541591852843\n",
      "Valence RMSE: 0.25273333997400704\n",
      "Arousal RMSE: 0.24882224285710788\n",
      "Test R^2 score: tensor([0.3174, 0.5434], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43042024478267515\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2510431853849769\n",
      "Valence RMSE: 0.25262136825112314\n",
      "Arousal RMSE: 0.24945501830871664\n",
      "Test R^2 score: tensor([0.3180, 0.5411], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42955992651011327\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25255305496558283\n",
      "Valence RMSE: 0.2556377755675622\n",
      "Arousal RMSE: 0.24943018832484198\n",
      "Test R^2 score: tensor([0.3017, 0.5412], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4214141836490698\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25141275115402373\n",
      "Valence RMSE: 0.25363750319348066\n",
      "Arousal RMSE: 0.24916813572254218\n",
      "Test R^2 score: tensor([0.3126, 0.5421], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42733882868394424\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2517983442568595\n",
      "Valence RMSE: 0.25434245677209283\n",
      "Arousal RMSE: 0.24922826289192793\n",
      "Test R^2 score: tensor([0.3087, 0.5419], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4253149897881755\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25341314708759766\n",
      "Valence RMSE: 0.2566300722799666\n",
      "Arousal RMSE: 0.2501548565094428\n",
      "Test R^2 score: tensor([0.2962, 0.5385], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41736323715270995\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2525216154877735\n",
      "Valence RMSE: 0.25496091368401735\n",
      "Arousal RMSE: 0.25005852329109074\n",
      "Test R^2 score: tensor([0.3054, 0.5388], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4221034437821718\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25292516156317696\n",
      "Valence RMSE: 0.25608929340935166\n",
      "Arousal RMSE: 0.2497209412616758\n",
      "Test R^2 score: tensor([0.2992, 0.5401], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41964450321974567\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2541001809698229\n",
      "Valence RMSE: 0.2578557648391982\n",
      "Arousal RMSE: 0.2502882507769503\n",
      "Test R^2 score: tensor([0.2895, 0.5380], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4137477799815449\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2536120623580507\n",
      "Valence RMSE: 0.25639347160916437\n",
      "Arousal RMSE: 0.2507998087383805\n",
      "Test R^2 score: tensor([0.2975, 0.5361], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4168203835093417\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25433330738008697\n",
      "Valence RMSE: 0.25788175445546646\n",
      "Arousal RMSE: 0.25073464699709547\n",
      "Test R^2 score: tensor([0.2894, 0.5364], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41285144134824175\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25487772380184476\n",
      "Valence RMSE: 0.2586000232361124\n",
      "Arousal RMSE: 0.2511002512207048\n",
      "Test R^2 score: tensor([0.2854, 0.5350], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4101927863812438\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25447866752201\n",
      "Valence RMSE: 0.25758616495993386\n",
      "Arousal RMSE: 0.2513327516834787\n",
      "Test R^2 score: tensor([0.2910, 0.5341], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41255822659473557\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2553892627371255\n",
      "Valence RMSE: 0.2595862893127662\n",
      "Arousal RMSE: 0.2511221006674917\n",
      "Test R^2 score: tensor([0.2799, 0.5349], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4074216849204632\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25556995816061445\n",
      "Valence RMSE: 0.25964326217555805\n",
      "Arousal RMSE: 0.25143067321882356\n",
      "Test R^2 score: tensor([0.2796, 0.5338], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40669179658168986\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2558115790196614\n",
      "Valence RMSE: 0.2593352692599499\n",
      "Arousal RMSE: 0.25223866880185963\n",
      "Test R^2 score: tensor([0.2813, 0.5308], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4060451624081053\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2568673942541656\n",
      "Valence RMSE: 0.26113298928696327\n",
      "Arousal RMSE: 0.25252975739089567\n",
      "Test R^2 score: tensor([0.2713, 0.5297], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40050415425223723\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2565704186928987\n",
      "Valence RMSE: 0.2604180638242354\n",
      "Arousal RMSE: 0.25266418727341466\n",
      "Test R^2 score: tensor([0.2753, 0.5292], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40224596580584765\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2569873159346692\n",
      "Valence RMSE: 0.26099741104267726\n",
      "Arousal RMSE: 0.2529136463926587\n",
      "Test R^2 score: tensor([0.2721, 0.5283], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4001668847091356\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2576757837892881\n",
      "Valence RMSE: 0.26237408143093066\n",
      "Arousal RMSE: 0.25289021431467706\n",
      "Test R^2 score: tensor([0.2644, 0.5283], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3963609136773356\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2572882524465444\n",
      "Valence RMSE: 0.26130532394471667\n",
      "Arousal RMSE: 0.25320745915580806\n",
      "Test R^2 score: tensor([0.2704, 0.5272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39875925958981157\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25789844145297564\n",
      "Valence RMSE: 0.2626120295846446\n",
      "Arousal RMSE: 0.2530970843869632\n",
      "Test R^2 score: tensor([0.2630, 0.5276], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39530748878986527\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25831272476879036\n",
      "Valence RMSE: 0.2630571626996785\n",
      "Arousal RMSE: 0.2534794995800421\n",
      "Test R^2 score: tensor([0.2605, 0.5261], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3933429206423769\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25854732785260565\n",
      "Valence RMSE: 0.2631224298484584\n",
      "Arousal RMSE: 0.25388979575809767\n",
      "Test R^2 score: tensor([0.2602, 0.5246], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39239180172204785\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25918379555063326\n",
      "Valence RMSE: 0.2647818036793184\n",
      "Arousal RMSE: 0.2534621790176826\n",
      "Test R^2 score: tensor([0.2508, 0.5262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3885114035040863\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2592738825706935\n",
      "Valence RMSE: 0.2642679955845672\n",
      "Arousal RMSE: 0.25418166510643264\n",
      "Test R^2 score: tensor([0.2537, 0.5235], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38861696292813097\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2599854203735482\n",
      "Valence RMSE: 0.26576533283657905\n",
      "Arousal RMSE: 0.25407405510182174\n",
      "Test R^2 score: tensor([0.2452, 0.5239], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38457824338563684\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26042225995183516\n",
      "Valence RMSE: 0.2663271241480806\n",
      "Arousal RMSE: 0.2543803646114521\n",
      "Test R^2 score: tensor([0.2420, 0.5228], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38240678731203204\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26065143194591495\n",
      "Valence RMSE: 0.26663088037778393\n",
      "Arousal RMSE: 0.25453155321075566\n",
      "Test R^2 score: tensor([0.2403, 0.5222], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38125809408117145\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.261082207293963\n",
      "Valence RMSE: 0.26783404839290426\n",
      "Arousal RMSE: 0.2541510583342968\n",
      "Test R^2 score: tensor([0.2334, 0.5236], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3785359966961303\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2616569186119734\n",
      "Valence RMSE: 0.2677858151479127\n",
      "Arousal RMSE: 0.2553809768181523\n",
      "Test R^2 score: tensor([0.2337, 0.5190], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3763631508740313\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26231797537721857\n",
      "Valence RMSE: 0.26944451528538305\n",
      "Arousal RMSE: 0.25499234026658735\n",
      "Test R^2 score: tensor([0.2242, 0.5205], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3723334049213654\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2623618260947364\n",
      "Valence RMSE: 0.2688915896377314\n",
      "Arousal RMSE: 0.25566534494456483\n",
      "Test R^2 score: tensor([0.2274, 0.5179], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37265650280022233\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2626518134440388\n",
      "Valence RMSE: 0.2700988832928955\n",
      "Arousal RMSE: 0.25498733979320454\n",
      "Test R^2 score: tensor([0.2204, 0.5205], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3704564141336935\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26319899700397026\n",
      "Valence RMSE: 0.2708177444781479\n",
      "Arousal RMSE: 0.25535303664448655\n",
      "Test R^2 score: tensor([0.2163, 0.5191], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3676906323816227\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2636995203174524\n",
      "Valence RMSE: 0.2716389056917606\n",
      "Arousal RMSE: 0.25551355922121205\n",
      "Test R^2 score: tensor([0.2115, 0.5185], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36500823526744874\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2641780704533387\n",
      "Valence RMSE: 0.272655937076796\n",
      "Arousal RMSE: 0.2554189613041143\n",
      "Test R^2 score: tensor([0.2056, 0.5189], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3622287729552591\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26470407688636327\n",
      "Valence RMSE: 0.27292314497475423\n",
      "Arousal RMSE: 0.2562214931999513\n",
      "Test R^2 score: tensor([0.2040, 0.5158], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35993574843277326\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26494701395531745\n",
      "Valence RMSE: 0.27416302879181853\n",
      "Arousal RMSE: 0.2553986571056607\n",
      "Test R^2 score: tensor([0.1968, 0.5189], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35786382978698505\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26549791715538124\n",
      "Valence RMSE: 0.27372116413641984\n",
      "Arousal RMSE: 0.2570116968768045\n",
      "Test R^2 score: tensor([0.1994, 0.5128], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3561094739379914\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26590095503717415\n",
      "Valence RMSE: 0.27555758677974435\n",
      "Arousal RMSE: 0.2558801519217274\n",
      "Test R^2 score: tensor([0.1886, 0.5171], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35286002760822627\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26647445597872493\n",
      "Valence RMSE: 0.27448976116573104\n",
      "Arousal RMSE: 0.2582104614331006\n",
      "Test R^2 score: tensor([0.1949, 0.5083], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3515806909908685\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2663467445001141\n",
      "Valence RMSE: 0.2763884964077735\n",
      "Arousal RMSE: 0.2559112652172004\n",
      "Test R^2 score: tensor([0.1837, 0.5170], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3503509239871542\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26641002942814984\n",
      "Valence RMSE: 0.2752532173544884\n",
      "Arousal RMSE: 0.257263044170394\n",
      "Test R^2 score: tensor([0.1904, 0.5119], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35113906347430746\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26640142213744517\n",
      "Valence RMSE: 0.27650199470557685\n",
      "Arousal RMSE: 0.2559024860323629\n",
      "Test R^2 score: tensor([0.1830, 0.5170], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3500322092044488\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26680011946536075\n",
      "Valence RMSE: 0.27713625432788414\n",
      "Arousal RMSE: 0.2560470738566873\n",
      "Test R^2 score: tensor([0.1793, 0.5165], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3478830641421526\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2672567766226362\n",
      "Valence RMSE: 0.2769215029598876\n",
      "Arousal RMSE: 0.2572291789433701\n",
      "Test R^2 score: tensor([0.1805, 0.5120], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34628143242227355\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26731095186170734\n",
      "Valence RMSE: 0.2783325205531112\n",
      "Arousal RMSE: 0.2558149682348083\n",
      "Test R^2 score: tensor([0.1722, 0.5174], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34477080597327714\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2679953122712061\n",
      "Valence RMSE: 0.27802470958565423\n",
      "Arousal RMSE: 0.25757568918377205\n",
      "Test R^2 score: tensor([0.1740, 0.5107], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34235255600720016\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2677970135538858\n",
      "Valence RMSE: 0.27887801964744724\n",
      "Arousal RMSE: 0.25623725547679094\n",
      "Test R^2 score: tensor([0.1689, 0.5158], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3423494082198634\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2678723978247567\n",
      "Valence RMSE: 0.2790818190399501\n",
      "Arousal RMSE: 0.2561729519565287\n",
      "Test R^2 score: tensor([0.1677, 0.5160], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34186334887255515\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2681002005074143\n",
      "Valence RMSE: 0.27923010451853564\n",
      "Arousal RMSE: 0.2564877848062148\n",
      "Test R^2 score: tensor([0.1668, 0.5148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3408258372670813\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26787926351306274\n",
      "Valence RMSE: 0.2799705592485097\n",
      "Arousal RMSE: 0.2552157628256501\n",
      "Test R^2 score: tensor([0.1624, 0.5196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3410136791448836\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26843661277180036\n",
      "Valence RMSE: 0.2797404330207586\n",
      "Arousal RMSE: 0.25663538393244495\n",
      "Test R^2 score: tensor([0.1638, 0.5143], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3390224276212479\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2678439495397562\n",
      "Valence RMSE: 0.28033519507262067\n",
      "Arousal RMSE: 0.25474092920790453\n",
      "Test R^2 score: tensor([0.1602, 0.5214], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3408149698849252\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26778854222356446\n",
      "Valence RMSE: 0.2802351811669872\n",
      "Arousal RMSE: 0.2547344694554496\n",
      "Test R^2 score: tensor([0.1608, 0.5214], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34112665931976205\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2676657420596962\n",
      "Valence RMSE: 0.2807621006027286\n",
      "Arousal RMSE: 0.2538947455342053\n",
      "Test R^2 score: tensor([0.1577, 0.5246], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3411222230202972\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26797865204307636\n",
      "Valence RMSE: 0.28123667689549414\n",
      "Arousal RMSE: 0.25402961927780754\n",
      "Test R^2 score: tensor([0.1548, 0.5241], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.339444575034015\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2685145930644739\n",
      "Valence RMSE: 0.2814251010497632\n",
      "Arousal RMSE: 0.25495114409684577\n",
      "Test R^2 score: tensor([0.1537, 0.5206], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33714854713805\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26809059177209443\n",
      "Valence RMSE: 0.2818223176918313\n",
      "Arousal RMSE: 0.2536164664295835\n",
      "Test R^2 score: tensor([0.1513, 0.5256], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3384561187086159\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2686486701784177\n",
      "Valence RMSE: 0.2818876138537092\n",
      "Arousal RMSE: 0.25472257287709427\n",
      "Test R^2 score: tensor([0.1509, 0.5215], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3361860683692023\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26830307562808514\n",
      "Valence RMSE: 0.28251894748287426\n",
      "Arousal RMSE: 0.2532905941722844\n",
      "Test R^2 score: tensor([0.1471, 0.5269], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33696471695341124\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26865568833561904\n",
      "Valence RMSE: 0.2821887133439437\n",
      "Arousal RMSE: 0.25440378890940296\n",
      "Test R^2 score: tensor([0.1491, 0.5227], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33587708184519377\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26805408108624723\n",
      "Valence RMSE: 0.2824793070252718\n",
      "Arousal RMSE: 0.2528070843083853\n",
      "Test R^2 score: tensor([0.1473, 0.5287], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33798672077726755\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26838980754059494\n",
      "Valence RMSE: 0.2827065175363694\n",
      "Arousal RMSE: 0.2532650835109602\n",
      "Test R^2 score: tensor([0.1459, 0.5269], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33644590863233736\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26845044132589796\n",
      "Valence RMSE: 0.28293860933820053\n",
      "Arousal RMSE: 0.2531343956121015\n",
      "Test R^2 score: tensor([0.1445, 0.5274], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3359885113098108\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26856416454973586\n",
      "Valence RMSE: 0.283166829079579\n",
      "Arousal RMSE: 0.25312046118323117\n",
      "Test R^2 score: tensor([0.1432, 0.5275], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33532422938548384\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2689911937879691\n",
      "Valence RMSE: 0.28347042889277674\n",
      "Arousal RMSE: 0.25368689484145357\n",
      "Test R^2 score: tensor([0.1413, 0.5254], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3333464954725366\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.268308212574091\n",
      "Valence RMSE: 0.28340584321761453\n",
      "Arousal RMSE: 0.2523087828426115\n",
      "Test R^2 score: tensor([0.1417, 0.5305], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33611346175995294\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26908597591444267\n",
      "Valence RMSE: 0.2835116070496537\n",
      "Arousal RMSE: 0.25384186718462887\n",
      "Test R^2 score: tensor([0.1411, 0.5248], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33293172027382273\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26839944152822703\n",
      "Valence RMSE: 0.28327845374512844\n",
      "Arousal RMSE: 0.2526456769254076\n",
      "Test R^2 score: tensor([0.1425, 0.5293], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3358718699933901\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26883916359395305\n",
      "Valence RMSE: 0.2831009684534807\n",
      "Arousal RMSE: 0.25377713337590885\n",
      "Test R^2 score: tensor([0.1436, 0.5250], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33429605984362737\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26813264385687247\n",
      "Valence RMSE: 0.2832745939708163\n",
      "Arousal RMSE: 0.25208279158645874\n",
      "Test R^2 score: tensor([0.1425, 0.5314], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33693118245392273\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2685952510894333\n",
      "Valence RMSE: 0.28381316750823715\n",
      "Arousal RMSE: 0.2524616877162409\n",
      "Test R^2 score: tensor([0.1392, 0.5299], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3345944005723109\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26884681877296\n",
      "Valence RMSE: 0.2842681548185051\n",
      "Arousal RMSE: 0.2524853264741618\n",
      "Test R^2 score: tensor([0.1365, 0.5299], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3331693851039354\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2690275005060781\n",
      "Valence RMSE: 0.28472369420081656\n",
      "Arousal RMSE: 0.25235690998610605\n",
      "Test R^2 score: tensor([0.1337, 0.5303], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33202355226418073\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2700166744945157\n",
      "Valence RMSE: 0.2855291006309448\n",
      "Arousal RMSE: 0.2535569792039618\n",
      "Test R^2 score: tensor([0.1288, 0.5259], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3273308118245591\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26923323688850137\n",
      "Valence RMSE: 0.28513478440857215\n",
      "Arousal RMSE: 0.25233158028910463\n",
      "Test R^2 score: tensor([0.1312, 0.5304], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.330819026292793\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26979612305519923\n",
      "Valence RMSE: 0.2851964205407107\n",
      "Arousal RMSE: 0.2534618269917552\n",
      "Test R^2 score: tensor([0.1308, 0.5262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32852317797824565\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2694093246843324\n",
      "Valence RMSE: 0.28501671158357317\n",
      "Arousal RMSE: 0.25284034996776583\n",
      "Test R^2 score: tensor([0.1319, 0.5285], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33023097110119337\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26945476734617824\n",
      "Valence RMSE: 0.2852167225842465\n",
      "Arousal RMSE: 0.25271162309926903\n",
      "Test R^2 score: tensor([0.1307, 0.5290], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32986156194963906\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2695708718849351\n",
      "Valence RMSE: 0.2854651272000714\n",
      "Arousal RMSE: 0.25267879034508467\n",
      "Test R^2 score: tensor([0.1292, 0.5291], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32916532786818925\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2697611707589658\n",
      "Valence RMSE: 0.28575322871530845\n",
      "Arousal RMSE: 0.2527593139278401\n",
      "Test R^2 score: tensor([0.1274, 0.5288], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3281359593897341\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27074712026719555\n",
      "Valence RMSE: 0.28679442872555533\n",
      "Arousal RMSE: 0.25368673973611794\n",
      "Test R^2 score: tensor([0.1211, 0.5254], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3232188332095632\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2699949759714196\n",
      "Valence RMSE: 0.28636930965906066\n",
      "Arousal RMSE: 0.2525612650130814\n",
      "Test R^2 score: tensor([0.1237, 0.5296], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32662173305514053\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2709522836172691\n",
      "Valence RMSE: 0.2874783244403483\n",
      "Arousal RMSE: 0.25335053379023276\n",
      "Test R^2 score: tensor([0.1169, 0.5266], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32174901525329885\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2697242959403481\n",
      "Valence RMSE: 0.2863210017469563\n",
      "Arousal RMSE: 0.2520370520372994\n",
      "Test R^2 score: tensor([0.1240, 0.5315], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.327744950237561\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2704592774895808\n",
      "Valence RMSE: 0.2869860264374946\n",
      "Arousal RMSE: 0.25285462659399727\n",
      "Test R^2 score: tensor([0.1199, 0.5285], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3241857234780843\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2702491922452426\n",
      "Valence RMSE: 0.2869569598895186\n",
      "Arousal RMSE: 0.2524380220754738\n",
      "Test R^2 score: tensor([0.1201, 0.5300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32505109903718116\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2700040384107752\n",
      "Valence RMSE: 0.286967350193815\n",
      "Arousal RMSE: 0.2519009754625715\n",
      "Test R^2 score: tensor([0.1200, 0.5320], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3260180043785495\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27107123178443593\n",
      "Valence RMSE: 0.28807291324111495\n",
      "Arousal RMSE: 0.25292928272353093\n",
      "Test R^2 score: tensor([0.1132, 0.5282], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3207069902841717\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2709956550513998\n",
      "Valence RMSE: 0.2879285399349018\n",
      "Arousal RMSE: 0.25293170225266387\n",
      "Test R^2 score: tensor([0.1141, 0.5282], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32114679565029997\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2720127161008012\n",
      "Valence RMSE: 0.2890254359011427\n",
      "Arousal RMSE: 0.25386242897133193\n",
      "Test R^2 score: tensor([0.1073, 0.5247], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3160261059728714\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2712201328503905\n",
      "Valence RMSE: 0.28824040402189843\n",
      "Arousal RMSE: 0.25305768199376927\n",
      "Test R^2 score: tensor([0.1122, 0.5277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31995167748179537\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2720654945754197\n",
      "Valence RMSE: 0.289145136730331\n",
      "Arousal RMSE: 0.25383923373334866\n",
      "Test R^2 score: tensor([0.1066, 0.5248], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31569975599230876\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27145360613409847\n",
      "Valence RMSE: 0.28856025684824005\n",
      "Arousal RMSE: 0.25319379679227966\n",
      "Test R^2 score: tensor([0.1102, 0.5272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3187118448937474\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27190723303633174\n",
      "Valence RMSE: 0.2892404054601535\n",
      "Arousal RMSE: 0.25339114941961577\n",
      "Test R^2 score: tensor([0.1060, 0.5265], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3162434440865569\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27224837194633783\n",
      "Valence RMSE: 0.28966481817853645\n",
      "Arousal RMSE: 0.25363880847468584\n",
      "Test R^2 score: tensor([0.1034, 0.5255], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3144676616289014\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2717874387693121\n",
      "Valence RMSE: 0.2890808078364606\n",
      "Arousal RMSE: 0.2533162258644014\n",
      "Test R^2 score: tensor([0.1070, 0.5268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31687658736123187\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.273327421095406\n",
      "Valence RMSE: 0.2907670436430202\n",
      "Arousal RMSE: 0.2546964557594565\n",
      "Test R^2 score: tensor([0.0965, 0.5216], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3090668622984246\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27192894066603884\n",
      "Valence RMSE: 0.2891957412443364\n",
      "Arousal RMSE: 0.2534886995304662\n",
      "Test R^2 score: tensor([0.1063, 0.5261], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31619915046379143\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2728289176693301\n",
      "Valence RMSE: 0.29022330874125185\n",
      "Arousal RMSE: 0.25424725700883416\n",
      "Test R^2 score: tensor([0.0999, 0.5233], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31159775695993525\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27199519213097256\n",
      "Valence RMSE: 0.28911887971848277\n",
      "Arousal RMSE: 0.2537184314847048\n",
      "Test R^2 score: tensor([0.1068, 0.5253], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31600697565269004\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2722305598538416\n",
      "Valence RMSE: 0.28958847333340104\n",
      "Arousal RMSE: 0.2536877441838728\n",
      "Test R^2 score: tensor([0.1039, 0.5254], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31461239773963856\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.272563181475776\n",
      "Valence RMSE: 0.29013720883755684\n",
      "Arousal RMSE: 0.2537750496805105\n",
      "Test R^2 score: tensor([0.1005, 0.5250], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31274933683872896\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2726759837158318\n",
      "Valence RMSE: 0.2904159432206104\n",
      "Arousal RMSE: 0.25369856939698415\n",
      "Test R^2 score: tensor([0.0987, 0.5253], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3120278515898499\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2730651647093358\n",
      "Valence RMSE: 0.2908727985308553\n",
      "Arousal RMSE: 0.2540121718152534\n",
      "Test R^2 score: tensor([0.0959, 0.5242], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31002182432002273\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27304627679588594\n",
      "Valence RMSE: 0.29060991551711435\n",
      "Arousal RMSE: 0.2542723255632173\n",
      "Test R^2 score: tensor([0.0975, 0.5232], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3103509602806758\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.273650839961785\n",
      "Valence RMSE: 0.29139826169327565\n",
      "Arousal RMSE: 0.25466962423052775\n",
      "Test R^2 score: tensor([0.0926, 0.5217], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3071538548470283\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27227697133917855\n",
      "Valence RMSE: 0.2899378290851548\n",
      "Arousal RMSE: 0.25338814792460057\n",
      "Test R^2 score: tensor([0.1017, 0.5265], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31409084687884037\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2729988731337254\n",
      "Valence RMSE: 0.29094115290397327\n",
      "Arousal RMSE: 0.25379128237879767\n",
      "Test R^2 score: tensor([0.0955, 0.5250], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31022295501823516\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27292432403709854\n",
      "Valence RMSE: 0.29087668624616014\n",
      "Arousal RMSE: 0.2537048022814453\n",
      "Test R^2 score: tensor([0.0959, 0.5253], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31058519619407265\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27334743096129294\n",
      "Valence RMSE: 0.29152874092980036\n",
      "Arousal RMSE: 0.2538673457500254\n",
      "Test R^2 score: tensor([0.0918, 0.5247], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30825191635317145\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2741925589943946\n",
      "Valence RMSE: 0.29248585837965496\n",
      "Arousal RMSE: 0.2545881801335355\n",
      "Test R^2 score: tensor([0.0858, 0.5220], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3039138380599537\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27352316758423767\n",
      "Valence RMSE: 0.29149164167175146\n",
      "Arousal RMSE: 0.2542881618285388\n",
      "Test R^2 score: tensor([0.0920, 0.5231], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30757895250374173\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27451588505144575\n",
      "Valence RMSE: 0.2924955373093886\n",
      "Arousal RMSE: 0.2552729968979274\n",
      "Test R^2 score: tensor([0.0858, 0.5194], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3025960603259067\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2735843058684782\n",
      "Valence RMSE: 0.2915407137292938\n",
      "Arousal RMSE: 0.2543634350162931\n",
      "Test R^2 score: tensor([0.0917, 0.5228], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30728490090273713\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2734163919955946\n",
      "Valence RMSE: 0.2913517420490857\n",
      "Arousal RMSE: 0.2542188215469898\n",
      "Test R^2 score: tensor([0.0929, 0.5234], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30814463788575397\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2741032391071514\n",
      "Valence RMSE: 0.2922287828221987\n",
      "Arousal RMSE: 0.2546910086130263\n",
      "Test R^2 score: tensor([0.0874, 0.5216], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3045238661147791\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2741059187431584\n",
      "Valence RMSE: 0.292511856214397\n",
      "Arousal RMSE: 0.2543716245064882\n",
      "Test R^2 score: tensor([0.0857, 0.5228], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30423900533689313\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2745204385199295\n",
      "Valence RMSE: 0.29294156902837\n",
      "Arousal RMSE: 0.25477083715673887\n",
      "Test R^2 score: tensor([0.0830, 0.5213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30214533103294594\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27486931196763753\n",
      "Valence RMSE: 0.29319636718196795\n",
      "Arousal RMSE: 0.2552296369828973\n",
      "Test R^2 score: tensor([0.0814, 0.5196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30048454639103156\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2750808171724647\n",
      "Valence RMSE: 0.2932525051066361\n",
      "Arousal RMSE: 0.2556205785949641\n",
      "Test R^2 score: tensor([0.0810, 0.5181], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.299572208350412\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27451647045662786\n",
      "Valence RMSE: 0.29295673645120074\n",
      "Arousal RMSE: 0.2547448442496747\n",
      "Test R^2 score: tensor([0.0829, 0.5214], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3021466864434021\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27512214349645836\n",
      "Valence RMSE: 0.29345919049179126\n",
      "Arousal RMSE: 0.25547229047408293\n",
      "Test R^2 score: tensor([0.0797, 0.5187], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.29920376294054235\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.274356850505315\n",
      "Valence RMSE: 0.2927869801758971\n",
      "Arousal RMSE: 0.2545960468622212\n",
      "Test R^2 score: tensor([0.0840, 0.5220], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30295742805816095\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27459724114617934\n",
      "Valence RMSE: 0.2932213644416222\n",
      "Arousal RMSE: 0.2546144558448701\n",
      "Test R^2 score: tensor([0.0812, 0.5219], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3015627893332439\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.274799710467525\n",
      "Valence RMSE: 0.2935974209915316\n",
      "Arousal RMSE: 0.2546179807735355\n",
      "Test R^2 score: tensor([0.0789, 0.5219], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3003770958594664\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27493732933994974\n",
      "Valence RMSE: 0.293659452043546\n",
      "Arousal RMSE: 0.25484347422418385\n",
      "Test R^2 score: tensor([0.0785, 0.5210], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2997588422420159\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Epoch 296, Loss: 0.2713225177215148\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2753917407815863\n",
      "Valence RMSE: 0.29418046179479634\n",
      "Arousal RMSE: 0.2552235837057876\n",
      "Test R^2 score: tensor([0.0752, 0.5196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2974075107169611\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Epoch 296, Loss: 0.2713225177215148\n",
      "Epoch 297, Loss: 0.27169561058348596\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27494528267304336\n",
      "Valence RMSE: 0.29414062913769023\n",
      "Arousal RMSE: 0.2543051458755862\n",
      "Test R^2 score: tensor([0.0755, 0.5231], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2992583517971049\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Epoch 296, Loss: 0.2713225177215148\n",
      "Epoch 297, Loss: 0.27169561058348596\n",
      "Epoch 298, Loss: 0.27282493641615857\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2762468095827062\n",
      "Valence RMSE: 0.2956069443468792\n",
      "Arousal RMSE: 0.25542344070807\n",
      "Test R^2 score: tensor([0.0662, 0.5188], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.29253601104643967\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Epoch 296, Loss: 0.2713225177215148\n",
      "Epoch 297, Loss: 0.27169561058348596\n",
      "Epoch 298, Loss: 0.27282493641615857\n",
      "Epoch 299, Loss: 0.27324168141817856\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2746728988086852\n",
      "Valence RMSE: 0.2940338106188876\n",
      "Arousal RMSE: 0.2538395573839282\n",
      "Test R^2 score: tensor([0.0761, 0.5248], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3004664477281099\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.671629971913036\n",
      "Epoch 2, Loss: 0.6687333265347284\n",
      "Epoch 3, Loss: 0.6658749249979059\n",
      "Epoch 4, Loss: 0.6630580036644803\n",
      "Epoch 5, Loss: 0.6602850342583926\n",
      "Epoch 6, Loss: 0.6575572753298399\n",
      "Epoch 7, Loss: 0.6548699673224264\n",
      "Epoch 8, Loss: 0.6522958475216714\n",
      "Epoch 9, Loss: 0.6499293756631087\n",
      "Epoch 10, Loss: 0.6475795217235044\n",
      "Epoch 11, Loss: 0.6452608314131224\n",
      "Epoch 12, Loss: 0.6429649510508926\n",
      "Epoch 13, Loss: 0.6407438958718576\n",
      "Epoch 14, Loss: 0.638606333504788\n",
      "Epoch 15, Loss: 0.6365967616138789\n",
      "Epoch 16, Loss: 0.6345868842020792\n",
      "Epoch 17, Loss: 0.6325761801185782\n",
      "Epoch 18, Loss: 0.6305610494635719\n",
      "Epoch 19, Loss: 0.628548798951053\n",
      "Epoch 20, Loss: 0.6265312272636504\n",
      "Epoch 21, Loss: 0.6245059206218456\n",
      "Epoch 22, Loss: 0.6224720374878084\n",
      "Epoch 23, Loss: 0.6204837946915848\n",
      "Epoch 24, Loss: 0.6184832816890455\n",
      "Epoch 25, Loss: 0.6165304187020964\n",
      "Epoch 26, Loss: 0.6146142892530745\n",
      "Epoch 27, Loss: 0.6126657787108325\n",
      "Epoch 28, Loss: 0.610677744872515\n",
      "Epoch 29, Loss: 0.6086940628843261\n",
      "Epoch 30, Loss: 0.606809055505222\n",
      "Epoch 31, Loss: 0.6048778727116946\n",
      "Epoch 32, Loss: 0.6028579782957656\n",
      "Epoch 33, Loss: 0.6006418341699931\n",
      "Epoch 34, Loss: 0.5982635255122346\n",
      "Epoch 35, Loss: 0.5956974967315445\n",
      "Epoch 36, Loss: 0.5929354300202975\n",
      "Epoch 37, Loss: 0.5899402463687178\n",
      "Epoch 38, Loss: 0.58666204795681\n",
      "Epoch 39, Loss: 0.5830371536207073\n",
      "Epoch 40, Loss: 0.5789954360030954\n",
      "Epoch 41, Loss: 0.574540625426459\n",
      "Epoch 42, Loss: 0.5696385725964667\n",
      "Epoch 43, Loss: 0.5643405736095243\n",
      "Epoch 44, Loss: 0.5586918831369638\n",
      "Epoch 45, Loss: 0.5527174444941838\n",
      "Epoch 46, Loss: 0.546666050911572\n",
      "Epoch 47, Loss: 0.5410831009965017\n",
      "Epoch 48, Loss: 0.5368719512533147\n",
      "Epoch 49, Loss: 0.5350787252811825\n",
      "Epoch 50, Loss: 0.5355749789264459\n",
      "Epoch 51, Loss: 0.5356808339755569\n",
      "Epoch 52, Loss: 0.5331061697761551\n",
      "Epoch 53, Loss: 0.5283992517977867\n",
      "Epoch 54, Loss: 0.5234583380807281\n",
      "Epoch 55, Loss: 0.5197665319946577\n",
      "Epoch 56, Loss: 0.5177322446135447\n",
      "Epoch 57, Loss: 0.5163715622140386\n",
      "Epoch 58, Loss: 0.5143331180639222\n",
      "Epoch 59, Loss: 0.5112214037538747\n",
      "Epoch 60, Loss: 0.5073858963425294\n",
      "Epoch 61, Loss: 0.5035094835662967\n",
      "Epoch 62, Loss: 0.5001065915454233\n",
      "Epoch 63, Loss: 0.497320595554927\n",
      "Epoch 64, Loss: 0.49493468041477306\n",
      "Epoch 65, Loss: 0.49240561602339156\n",
      "Epoch 66, Loss: 0.489308413631865\n",
      "Epoch 67, Loss: 0.48572667016190807\n",
      "Epoch 68, Loss: 0.48220696855602035\n",
      "Epoch 69, Loss: 0.47932768445219004\n",
      "Epoch 70, Loss: 0.4769795265982273\n",
      "Epoch 71, Loss: 0.4746323411196473\n",
      "Epoch 72, Loss: 0.4719036510434612\n",
      "Epoch 73, Loss: 0.4689687695211225\n",
      "Epoch 74, Loss: 0.4662430322704289\n",
      "Epoch 75, Loss: 0.46406173641771375\n",
      "Epoch 76, Loss: 0.4622979373543843\n",
      "Epoch 77, Loss: 0.46083289107024966\n",
      "Epoch 78, Loss: 0.4593422650980771\n",
      "Epoch 79, Loss: 0.4577396803608826\n",
      "Epoch 80, Loss: 0.4561649491453515\n",
      "Epoch 81, Loss: 0.45465674148817226\n",
      "Epoch 82, Loss: 0.45310696204226475\n",
      "Epoch 83, Loss: 0.45158845899472005\n",
      "Epoch 84, Loss: 0.4500622500967342\n",
      "Epoch 85, Loss: 0.4484717917539525\n",
      "Epoch 86, Loss: 0.44689366228309163\n",
      "Epoch 87, Loss: 0.4451394330400119\n",
      "Epoch 88, Loss: 0.44352505650856205\n",
      "Epoch 89, Loss: 0.442258282580105\n",
      "Epoch 90, Loss: 0.4410068595562509\n",
      "Epoch 91, Loss: 0.4400531825794107\n",
      "Epoch 92, Loss: 0.438908234914635\n",
      "Epoch 93, Loss: 0.4377090942174056\n",
      "Epoch 94, Loss: 0.4366650959555693\n",
      "Epoch 95, Loss: 0.43557429548087356\n",
      "Epoch 96, Loss: 0.43458347943536063\n",
      "Epoch 97, Loss: 0.4332692246362345\n",
      "Epoch 98, Loss: 0.4321044914977982\n",
      "Epoch 99, Loss: 0.4308217250943664\n",
      "Epoch 100, Loss: 0.4295953304966253\n",
      "Epoch 101, Loss: 0.4282565652607937\n",
      "Epoch 102, Loss: 0.4270418910722195\n",
      "Epoch 103, Loss: 0.42581005174007763\n",
      "Epoch 104, Loss: 0.42462838559377364\n",
      "Epoch 105, Loss: 0.42344210224536233\n",
      "Epoch 106, Loss: 0.4222172832078141\n",
      "Epoch 107, Loss: 0.4210763188082426\n",
      "Epoch 108, Loss: 0.4198115714049991\n",
      "Epoch 109, Loss: 0.41860063864011754\n",
      "Epoch 110, Loss: 0.4174115754330628\n",
      "Epoch 111, Loss: 0.4161611490530598\n",
      "Epoch 112, Loss: 0.41502782469850324\n",
      "Epoch 113, Loss: 0.4138652223591688\n",
      "Epoch 114, Loss: 0.4125727177594084\n",
      "Epoch 115, Loss: 0.41134192433626254\n",
      "Epoch 116, Loss: 0.41017566816594414\n",
      "Epoch 117, Loss: 0.4089684498773364\n",
      "Epoch 118, Loss: 0.40762116046864794\n",
      "Epoch 119, Loss: 0.406319740618174\n",
      "Epoch 120, Loss: 0.405049439079434\n",
      "Epoch 121, Loss: 0.40383634275878155\n",
      "Epoch 122, Loss: 0.40274685045783204\n",
      "Epoch 123, Loss: 0.40199027228091466\n",
      "Epoch 124, Loss: 0.40161116115949225\n",
      "Epoch 125, Loss: 0.3991063058538977\n",
      "Epoch 126, Loss: 0.397414371096433\n",
      "Epoch 127, Loss: 0.3972026913886879\n",
      "Epoch 128, Loss: 0.3953288297333687\n",
      "Epoch 129, Loss: 0.39364689203574044\n",
      "Epoch 130, Loss: 0.39313012785364565\n",
      "Epoch 131, Loss: 0.3917560605576611\n",
      "Epoch 132, Loss: 0.39011815567637476\n",
      "Epoch 133, Loss: 0.3890992886412017\n",
      "Epoch 134, Loss: 0.388476010912412\n",
      "Epoch 135, Loss: 0.3874661707724198\n",
      "Epoch 136, Loss: 0.38573786093301465\n",
      "Epoch 137, Loss: 0.38444118085081114\n",
      "Epoch 138, Loss: 0.38383941708319347\n",
      "Epoch 139, Loss: 0.38321691676838865\n",
      "Epoch 140, Loss: 0.3818776920055147\n",
      "Epoch 141, Loss: 0.38022946817191977\n",
      "Epoch 142, Loss: 0.3792889618760218\n",
      "Epoch 143, Loss: 0.37877755093221926\n",
      "Epoch 144, Loss: 0.37759850157949587\n",
      "Epoch 145, Loss: 0.37623420903979954\n",
      "Epoch 146, Loss: 0.3751046908790819\n",
      "Epoch 147, Loss: 0.3743532086559549\n",
      "Epoch 148, Loss: 0.3735313465198388\n",
      "Epoch 149, Loss: 0.3723138527787168\n",
      "Epoch 150, Loss: 0.37112605305667157\n",
      "Epoch 151, Loss: 0.3700942180823526\n",
      "Epoch 152, Loss: 0.36921057467262297\n",
      "Epoch 153, Loss: 0.3683897181904784\n",
      "Epoch 154, Loss: 0.36758135916800205\n",
      "Epoch 155, Loss: 0.36677554538772844\n",
      "Epoch 156, Loss: 0.36589423154301387\n",
      "Epoch 157, Loss: 0.36494912008453045\n",
      "Epoch 158, Loss: 0.3637990645126596\n",
      "Epoch 159, Loss: 0.3626506007525755\n",
      "Epoch 160, Loss: 0.36159538488766013\n",
      "Epoch 161, Loss: 0.36068172887938793\n",
      "Epoch 162, Loss: 0.35987752526507955\n",
      "Epoch 163, Loss: 0.3592837051861154\n",
      "Epoch 164, Loss: 0.3590812519180186\n",
      "Epoch 165, Loss: 0.3597252838678716\n",
      "Epoch 166, Loss: 0.36092166205332654\n",
      "Epoch 167, Loss: 0.3579554093338091\n",
      "Epoch 168, Loss: 0.35496451697523573\n",
      "Epoch 169, Loss: 0.35542565388439246\n",
      "Epoch 170, Loss: 0.35568304205021073\n",
      "Epoch 171, Loss: 0.3534016399084544\n",
      "Epoch 172, Loss: 0.3521998773581385\n",
      "Epoch 173, Loss: 0.35276286452644834\n",
      "Epoch 174, Loss: 0.35154196361324086\n",
      "Epoch 175, Loss: 0.34985628413125197\n",
      "Epoch 176, Loss: 0.3501716758976316\n",
      "Epoch 177, Loss: 0.3497411596736572\n",
      "Epoch 178, Loss: 0.34789210259453374\n",
      "Epoch 179, Loss: 0.3475836774537322\n",
      "Epoch 180, Loss: 0.34773151783057554\n",
      "Epoch 181, Loss: 0.3462202009054785\n",
      "Epoch 182, Loss: 0.34516596684176937\n",
      "Epoch 183, Loss: 0.3451332308759372\n",
      "Epoch 184, Loss: 0.3444686335366464\n",
      "Epoch 185, Loss: 0.3432300059183344\n",
      "Epoch 186, Loss: 0.34251380934337794\n",
      "Epoch 187, Loss: 0.34227875664098106\n",
      "Epoch 188, Loss: 0.34159760252029736\n",
      "Epoch 189, Loss: 0.34052755904457543\n",
      "Epoch 190, Loss: 0.3397397287709503\n",
      "Epoch 191, Loss: 0.3392707224900265\n",
      "Epoch 192, Loss: 0.33872212099941884\n",
      "Epoch 193, Loss: 0.33792491143528774\n",
      "Epoch 194, Loss: 0.3369791161577913\n",
      "Epoch 195, Loss: 0.3361945942784245\n",
      "Epoch 196, Loss: 0.3355866973658766\n",
      "Epoch 197, Loss: 0.33509415849431134\n",
      "Epoch 198, Loss: 0.33456910684861596\n",
      "Epoch 199, Loss: 0.33390867479628306\n",
      "Epoch 200, Loss: 0.33308136874349564\n",
      "Epoch 201, Loss: 0.33216885351993036\n",
      "Epoch 202, Loss: 0.3313323238028498\n",
      "Epoch 203, Loss: 0.3306029509455653\n",
      "Epoch 204, Loss: 0.3300205044986877\n",
      "Epoch 205, Loss: 0.3295137783861438\n",
      "Epoch 206, Loss: 0.3290754472009486\n",
      "Epoch 207, Loss: 0.32886016323473444\n",
      "Epoch 208, Loss: 0.3288036322268924\n",
      "Epoch 209, Loss: 0.3289514419968497\n",
      "Epoch 210, Loss: 0.3281350474862877\n",
      "Epoch 211, Loss: 0.32652927850271757\n",
      "Epoch 212, Loss: 0.32474199838155815\n",
      "Epoch 213, Loss: 0.32416596040346407\n",
      "Epoch 214, Loss: 0.3244162383170198\n",
      "Epoch 215, Loss: 0.3241939305376847\n",
      "Epoch 216, Loss: 0.32318678320571176\n",
      "Epoch 217, Loss: 0.321748642516959\n",
      "Epoch 218, Loss: 0.32089643054039696\n",
      "Epoch 219, Loss: 0.32069291808302647\n",
      "Epoch 220, Loss: 0.3206276854835761\n",
      "Epoch 221, Loss: 0.32022046883759187\n",
      "Epoch 222, Loss: 0.31928553752217653\n",
      "Epoch 223, Loss: 0.3180973285076888\n",
      "Epoch 224, Loss: 0.31715938039451286\n",
      "Epoch 225, Loss: 0.3166396963509195\n",
      "Epoch 226, Loss: 0.31639950356531926\n",
      "Epoch 227, Loss: 0.3162379700366688\n",
      "Epoch 228, Loss: 0.3159298548164888\n",
      "Epoch 229, Loss: 0.31552135682631943\n",
      "Epoch 230, Loss: 0.3147105590758946\n",
      "Epoch 231, Loss: 0.31351783219336904\n",
      "Epoch 232, Loss: 0.3123704045512187\n",
      "Epoch 233, Loss: 0.3115554003925337\n",
      "Epoch 234, Loss: 0.31110849518943945\n",
      "Epoch 235, Loss: 0.31090281738769704\n",
      "Epoch 236, Loss: 0.31098887340230585\n",
      "Epoch 237, Loss: 0.31127926097196484\n",
      "Epoch 238, Loss: 0.3113072401481986\n",
      "Epoch 239, Loss: 0.3104659400893418\n",
      "Epoch 240, Loss: 0.3086421421592903\n",
      "Epoch 241, Loss: 0.30698585514401633\n",
      "Epoch 242, Loss: 0.3061961092535718\n",
      "Epoch 243, Loss: 0.3062506948190709\n",
      "Epoch 244, Loss: 0.3063395291467355\n",
      "Epoch 245, Loss: 0.30593669786046107\n",
      "Epoch 246, Loss: 0.3050208224747817\n",
      "Epoch 247, Loss: 0.3036636487446724\n",
      "Epoch 248, Loss: 0.30255695189156795\n",
      "Epoch 249, Loss: 0.3020827016604605\n",
      "Epoch 250, Loss: 0.3020740937869815\n",
      "Epoch 251, Loss: 0.3022624490531133\n",
      "Epoch 252, Loss: 0.30206866749426936\n",
      "Epoch 253, Loss: 0.3015553928189993\n",
      "Epoch 254, Loss: 0.30028179116884635\n",
      "Epoch 255, Loss: 0.29879665832399965\n",
      "Epoch 256, Loss: 0.297768390487141\n",
      "Epoch 257, Loss: 0.29742294489690896\n",
      "Epoch 258, Loss: 0.2974981368631637\n",
      "Epoch 259, Loss: 0.2975792186914257\n",
      "Epoch 260, Loss: 0.29730617432102857\n",
      "Epoch 261, Loss: 0.2964056973610544\n",
      "Epoch 262, Loss: 0.29504114227794226\n",
      "Epoch 263, Loss: 0.29371456581834454\n",
      "Epoch 264, Loss: 0.2927785255850154\n",
      "Epoch 265, Loss: 0.29241332491850186\n",
      "Epoch 266, Loss: 0.2925327617278972\n",
      "Epoch 267, Loss: 0.2928607996104221\n",
      "Epoch 268, Loss: 0.2929958458132377\n",
      "Epoch 269, Loss: 0.292214223078254\n",
      "Epoch 270, Loss: 0.2906440436368935\n",
      "Epoch 271, Loss: 0.2887823964395379\n",
      "Epoch 272, Loss: 0.2875527312806871\n",
      "Epoch 273, Loss: 0.2872561125208705\n",
      "Epoch 274, Loss: 0.28748254463860845\n",
      "Epoch 275, Loss: 0.2878285049850965\n",
      "Epoch 276, Loss: 0.2877793183598978\n",
      "Epoch 277, Loss: 0.2869351565590005\n",
      "Epoch 278, Loss: 0.2849166202923548\n",
      "Epoch 279, Loss: 0.28313170223008394\n",
      "Epoch 280, Loss: 0.28236418803667673\n",
      "Epoch 281, Loss: 0.2824600995842279\n",
      "Epoch 282, Loss: 0.2829653300642664\n",
      "Epoch 283, Loss: 0.28279452609277483\n",
      "Epoch 284, Loss: 0.2819736627977065\n",
      "Epoch 285, Loss: 0.2802662865128542\n",
      "Epoch 286, Loss: 0.27853655529159027\n",
      "Epoch 287, Loss: 0.2774626114504905\n",
      "Epoch 288, Loss: 0.2771784330001127\n",
      "Epoch 289, Loss: 0.2773544954179399\n",
      "Epoch 290, Loss: 0.2777176069946737\n",
      "Epoch 291, Loss: 0.2781470445404604\n",
      "Epoch 292, Loss: 0.2768556974971842\n",
      "Epoch 293, Loss: 0.2747289134239171\n",
      "Epoch 294, Loss: 0.27262728408808146\n",
      "Epoch 295, Loss: 0.27158060358105\n",
      "Epoch 296, Loss: 0.2713225177215148\n",
      "Epoch 297, Loss: 0.27169561058348596\n",
      "Epoch 298, Loss: 0.27282493641615857\n",
      "Epoch 299, Loss: 0.27324168141817856\n",
      "Epoch 300, Loss: 0.2719807786259438\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27537524122866514\n",
      "Valence RMSE: 0.2948440489919777\n",
      "Arousal RMSE: 0.2544209773927993\n",
      "Test R^2 score: tensor([0.0710, 0.5226], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.29682744350590173\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship between the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2371205306603663\n",
      "Corresponding R^2 SCore: 0.4854757154277499\n",
      "Corresponding num_epochs: 115\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.48631835945739543\n",
      "Corresponding RMSE: 0.2371807744214886\n",
      "Corresponding num_epochs: 116\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjsuted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.38441982377791284\n",
      "Corresponding RMSE: 0.24022711957374765\n",
      "Corresponding num_epochs: 73\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACBCElEQVR4nO3dd1hT1/8H8HfYoEzZigLi3mKljjoZjlpnna2jjrZqbd17gG0draPuLkdbrX4dta0bd6vUTbWKKIriYKgIiAgEOL8/7i/RyLrRhDDer+fJk+SOk889SciHc849VyGEECAiIiKiPBkZOgAiIiKi4ozJEhEREVEBmCwRERERFYDJEhEREVEBmCwRERERFYDJEhEREVEBmCwRERERFYDJEhEREVEBmCwRERERFYDJEpGWPD098fbbbxs6DNKCQqHAnDlz1M/Xr18PhUKBW7duGSymV+Xp6YnBgwcbOgzKR2pqKpydnbFx40ZDh1Kkbt26BYVCgfXr16uXTZkyBX5+foYLSoeYLBGVcW3atIFCoSj09mKy8TpWrVql8QdVrqSkJFhYWEChUCAiIkInsejLnj17dFZfr+rl98/GxgatW7fG7t27C9137969MDU1haWlJf7+++98tzt06BA++OADVK9eHVZWVvD29sawYcMQGxsrO84///wTrVu3hrOzs7qM3r17Y9++fbLLKE6++eYbWFtbo2/fvnmunzRpEhQKBfr06VPEkRW9zz77DP/++y/++OMPQ4fy2hS8NhyRdjw9PVG3bl3s2rXL0KHoRGhoKOLj49XPz5w5g2XLlmHatGmoVauWenn9+vVRv3791369unXrwtHREUePHtVqv++//x5jxoyBnZ0dhg4dis8//1z2vgqFArNnz1YnMNnZ2VAqlTA3N4dCodAqDjlGjx6NlStXQh9/Xj09PdGmTZtCE06FQoGAgAAMHDgQQgjcvn0bq1evRmxsLPbu3YugoKA89zt37hzatGmDKlWq4NmzZ0hKSsKJEydQs2bNXNs2adIEiYmJePfdd1GtWjXcvHkTK1asgJWVFcLDw+Hq6lpgjF9//TUmTpyI1q1bo2vXrrCyskJUVBQOHjyIBg0avFJSbUhKpRIVK1bE2LFjMXXq1FzrhRCoXLkyTExMEB8fj/j4eFhbWxsgUt27desWvLy8sG7dOo2Wzz59+iA2NhbHjx83XHC6IIhIK1WqVBGdO3c2dBh6s3XrVgFAHDlyRC/l16lTR7Ru3Vrr/Vq1aiV69Oghxo4dK7y8vLTaF4CYPXu21q/5qkaNGiX09ee1SpUqYtCgQYVuB0CMGjVKY9mVK1cEANGxY8c894mOjhaurq6ibt26IiEhQdy+fVt4e3sLT09PERcXl2v7Y8eOiezs7FzLAIjp06cXGJ9SqRQ2NjYiICAgz/Xx8fEF7q9L2dnZ4tmzZ69dzo4dOwQAERUVlef6w4cPCwDi8OHDwtTUVKxfv15Wuc+ePctVz8VNdHS0ACDWrVunsXzbtm1CoVCIGzduGCYwHWE3XCk2Z84cKBQKREVFYfDgwbCzs4OtrS2GDBmCtLQ09XZ59TWrvNz9oirz2rVreO+992BrawsnJyfMnDkTQgjcuXMHXbt2hY2NDVxdXbFo0aJXin3v3r146623UK5cOVhbW6Nz5864fPmyxjaDBw9G+fLlcfPmTQQFBaFcuXJwd3dHSEhIrv/onz59ivHjx8PDwwPm5uaoUaMGvv766zz/8//ll1/QtGlTWFlZwd7eHq1atcKBAwdybff333+jadOmsLCwgLe3N3766SeN9UqlEsHBwahWrRosLCxQoUIFtGzZEqGhofke99mzZ6FQKLBhw4Zc6/bv3w+FQqFu0Xry5Ak+++wzeHp6wtzcHM7OzggICMD58+fzr9jXIOc9iYuLw5AhQ1CpUiWYm5vDzc0NXbt2VY8N8vT0xOXLl3Hs2DF191CbNm0Kfe2YmBj89ddf6Nu3L/r27Yvo6GicPHky13YZGRkYO3YsnJycYG1tjXfeeQd3797NtV1eY5by62p8eYxQYe/r4MGDsXLlSnWZqptKTk4Oli5dijp16sDCwgIuLi748MMP8fjxY43XFULg888/R6VKlWBlZYW2bdvmqm9t1apVC46Ojrhx40audYmJiejYsSOcnJxw+PBhODk5oXLlyjh69CiMjIzQuXNnPH36VGOfVq1awcjIKNcyBweHQrtKHz58iJSUFLRo0SLP9c7OzhrP09PTMWfOHFSvXh0WFhZwc3NDjx49NI5F7vdcoVBg9OjR2LhxI+rUqQNzc3N1t9+9e/fwwQcfwMXFBebm5qhTpw7Wrl1b4LGo7Ny5E56enqhatWqe6zdu3IjatWujbdu28Pf3z3Nc09GjR6FQKLB582bMmDEDFStWhJWVFVJSUgAAW7duha+vLywtLeHo6Ij33nsP9+7d0yijTZs2eX6vBg8eDE9PT41lmzdvhq+vL6ytrWFjY4N69erhm2++Ua9PTEzEhAkTUK9ePZQvXx42Njbo2LEj/v33X1l14u/vDwD4/fffZW1fXDFZKgN69+6NJ0+eYN68eejduzfWr1+P4ODg1yqzT58+yMnJwfz58+Hn54fPP/8cS5cuRUBAACpWrIgFCxbAx8cHEyZM0Lr59eeff0bnzp1Rvnx5LFiwADNnzsSVK1fQsmXLXANys7Oz0aFDB7i4uGDhwoXw9fXF7NmzMXv2bPU2Qgi88847WLJkCTp06IDFixejRo0amDhxIsaNG6dRXnBwMN5//32YmpoiJCQEwcHB8PDwwOHDhzW2i4qKQq9evRAQEIBFixbB3t4egwcP1vgxmzNnDoKDg9G2bVusWLEC06dPR+XKlQtMZpo0aQJvb2/873//y7Vuy5YtsLe3V3effPTRR1i9ejV69uyJVatWYcKECbC0tNTLeB6570nPnj3x22+/YciQIVi1ahXGjBmDJ0+eICYmBgCwdOlSVKpUCTVr1sTPP/+Mn3/+GdOnTy/09X/99VeUK1cOb7/9Npo2bYqqVavm+UMzbNgwLF26FIGBgZg/fz5MTU3RuXNnndUDUPj7+uGHHyIgIAAA1Mf4888/q/f/8MMPMXHiRLRo0QLffPMNhgwZgo0bNyIoKAhKpVK93axZszBz5kw0aNAAX331Fby9vREYGJgrYdFGcnIyHj9+DHt7e43lGRkZ6Nq1K8zMzNSJkoqHhweOHj2KpKQkvPvuu8jKyirwNVJTU5GamgpHR8cCt3N2doalpSX+/PNPJCYmFrhtdnY23n77bQQHB8PX1xeLFi3Cp59+iuTkZPz3338AtPueA8Dhw4cxduxY9OnTB9988w08PT0RHx+PN998EwcPHsTo0aPxzTffwMfHB0OHDsXSpUsLjBEATp48icaNG+e5LiMjA9u3b0e/fv0AAP369cPhw4cRFxeX5/Zz587F7t27MWHCBHz55ZcwMzPD+vXr0bt3bxgbG2PevHkYPnw4duzYgZYtWyIpKanQ+F4WGhqKfv36wd7eHgsWLMD8+fPRpk0bnDhxQr3NzZs3sXPnTrz99ttYvHgxJk6ciEuXLqF169a4f/9+oa9ha2uLqlWrapRZIhmwVYv0bPbs2QKA+OCDDzSWd+/eXVSoUEH9PL/mUyFyd1+oyhwxYoR6WVZWlqhUqZJQKBRi/vz56uWPHz8WlpaWsroMVJ48eSLs7OzE8OHDNZbHxcUJW1tbjeWDBg0SAMQnn3yiXpaTkyM6d+4szMzMxIMHD4QQQuzcuVMAEJ9//rlGmb169RIKhULdZH79+nVhZGQkunfvnqvJOycnR/24SpUqAoA4fvy4ellCQoIwNzcX48ePVy9r0KDBK3XXTZ06VZiamorExET1soyMDGFnZ6fxXtra2ubqZtGFl7vh5L4njx8/FgDEV199VWD5r9INV69ePTFgwAD182nTpglHR0ehVCrVy8LDwwUAMXLkSI19+/fvn+tzvG7dOgFAREdHq5e9vI3Ky91ect7X/Lrh/vrrLwFAbNy4UWP5vn37NJYnJCQIMzMz0blzZ43P3rRp0wQA2d1wQ4cOFQ8ePBAJCQni7NmzokOHDrLeo9cxd+5cAUAcOnSo0G1nzZolAIhy5cqJjh07ii+++EKcO3cu13Zr164VAMTixYtzrVPVj9zvuRBS3RgZGYnLly9rbDt06FDh5uYmHj58qLG8b9++wtbWVqSlpeV7LEqlUigUCo2/AS/atm2bACCuX78uhBAiJSVFWFhYiCVLlmhsd+TIEQFAeHt7a7xeZmamcHZ2FnXr1tXoMty1a5cAIGbNmqVe1rp16zy/Y4MGDRJVqlRRP//000+FjY2NyMrKyve40tPTc/09jI6OFubm5iIkJERjWX6/I4GBgaJWrVr5vkZJwJalMuCjjz7SeP7WW2/h0aNH6mbdVzFs2DD1Y2NjYzRp0gRCCAwdOlS93M7ODjVq1MDNmzdllxsaGoqkpCT069cPDx8+VN+MjY3h5+eHI0eO5Npn9OjR6seq5vXMzEwcPHgQgHRmkrGxMcaMGaOx3/jx4yGEwN69ewFITeg5OTmYNWtWrq6FlwcB165dG2+99Zb6uZOTU65jtbOzw+XLl3H9+nXZxw9IrXZKpRI7duxQLztw4ACSkpI0zqCxs7PDqVOnZP139zrkvieWlpYwMzPD0aNHc3UpvY6LFy/i0qVL6v/IAahj2b9/v3rZnj17ACDX+/zZZ5/pLBbg1d9XQOpCsbW1RUBAgEZd+vr6onz58uq6PHjwIDIzM/HJJ59ofPa0PZYff/wRTk5OcHZ2RpMmTXDo0CFMmjQpz5YWXTh+/DiCg4PRu3dvtGvXrtDtg4ODsWnTJjRq1Aj79+/H9OnT4evri8aNG2u0kG7fvh2Ojo745JNPcpWhqh+533OV1q1bo3bt2urnQghs374dXbp0gRBC4/0JCgpCcnJyga3CiYmJEELkarVT2bhxI5o0aQIfHx8AUHdl5zfFwKBBg2Bpaal+fvbsWSQkJGDkyJGwsLBQL+/cuTNq1qwp6yzHl9nZ2eHp06cFDg0wNzdX/z3Mzs7Go0ePUL58edSoUUN2l7+9vT0ePnyodXzFCZOlMqBy5coaz1Vf5tf5QXu5TFtbW1hYWORqere1tdXqdVQ/QO3atYOTk5PG7cCBA0hISNDY3sjICN7e3hrLqlevDgDq7qHbt2/D3d0911knqjO9bt++DQC4ceMGjIyMNP6A5ufl4weken3xWENCQpCUlITq1aujXr16mDhxIi5evFho2Q0aNEDNmjWxZcsW9bItW7bA0dFR4wdo4cKF+O+//+Dh4YGmTZtizpw5WiWmcsl9T8zNzbFgwQLs3bsXLi4uaNWqFRYuXJhvN4Ncv/zyC8qVKwdvb29ERUUhKioKFhYW8PT01PihuX37NoyMjHKNF6lRo8Zrvf7LXvV9BaS6TE5OhrOzc666TE1NVdel6jNZrVo1jf2dnJzy/THOS9euXREaGordu3erxxumpaXl+mdAF65evYru3bujbt26+OGHH2Tv169fP/z11194/PgxDhw4gP79++PChQvo0qUL0tPTAUjfzRo1asDExCTfcuR+z1W8vLw0nj948ABJSUn47rvvcr03Q4YMAYBcf3/yIvIYB5mUlIQ9e/agdevW6s9wVFQUWrRogbNnz+LatWu59nk5PlX8eX2ea9asmev45Bg5ciSqV6+Ojh07olKlSvjggw9yTdmQk5ODJUuWoFq1ajA3N4ejoyOcnJxw8eJFJCcny3odIYRezjotSvl/8qjUMDY2znO56kud34c4OztbqzILex05cnJyAEhjPfI67bigP5ZFSc6xtmrVCjdu3MDvv/+OAwcO4IcffsCSJUuwZs0ajZa5vPTp0wdffPEFHj58CGtra/zxxx/o16+fxvH37t0bb731Fn777TccOHAAX331FRYsWIAdO3agY8eOujlQaPeefPbZZ+jSpQt27tyJ/fv3Y+bMmZg3bx4OHz6MRo0aaf3aQgj8+uuvePr0aZ5JbEJCAlJTU1G+fHmty5br5e/B67yvOTk5BU5Y+OJYIV2oVKmSeoBtp06d4OjoiNGjR6Nt27bo0aOHzl7nzp07CAwMhK2tLfbs2fNKp8Pb2NggICAAAQEBMDU1xYYNG3Dq1Cm0bt1aZ3G+6MVWG+D55/y9997DoEGD8tynoKkzHBwcoFAo8vzncOvWrcjIyMCiRYvyPOll48aNucaRvhyfNhQKRZ5/d1/+LDs7OyM8PBz79+/H3r17sXfvXqxbtw4DBw5Un2Ty5ZdfYubMmfjggw8wd+5cODg4wMjICJ999pm6zgrz+PHjQsewFXfF45eHDEr1n+rLAwRf5T+V16VqFXB2dlb/kS9ITk4Obt68qW5NAqD+L0111keVKlVw8OBBPHnyROOP+NWrV9XrVa+dk5ODK1euoGHDhro4HDg4OGDIkCEYMmQIUlNT0apVK8yZM0dWshQcHIzt27fDxcUFKSkpeU5y5+bmhpEjR2LkyJFISEhA48aN8cUXX+g0WdL2PalatSrGjx+P8ePH4/r162jYsCEWLVqEX375BUD+yXlejh07hrt37yIkJERjzidA+gM8YsQI7Ny5E++99x6qVKmCnJwcdSuESmRkpKzXsre3z/UdyMzMzHOCxcLe1/yOsWrVqjh48CBatGhR4I+h6jN5/fp1jZbTBw8evFaL8IcffoglS5ZgxowZ6N69u07+23/06BECAwORkZGBQ4cOwc3N7bXLbNKkCTZs2KCu+6pVq+LUqVNQKpUwNTXNcx+53/P8qM6gzM7OlvU5f5mJiQmqVq2K6OjoXOs2btyIunXrapx4ovLtt99i06ZNhZ50o4o/MjIyVxdnZGSkxvHZ29vn2cqc1990MzMzdOnSBV26dEFOTg5GjhyJb7/9FjNnzoSPjw+2bduGtm3b4scff9TYLykpSXYCFB0djQYNGsjatrhiNxzBxsYGjo6Ouc5aW7VqVZHHEhQUBBsbG3z55ZcaZwapPHjwINeyFStWqB8LIbBixQqYmpqiffv2AKT/qLOzszW2A4AlS5ZAoVCoE4tu3brByMgIISEhuf5j0qZ1TOXRo0caz8uXLw8fHx9kZGQUum+tWrVQr149bNmyBVu2bIGbmxtatWqlXp+dnZ2rCdzZ2Rnu7u4a5T98+BBXr17VmCpCW3Lfk7S0NHW3iUrVqlVhbW2tEVO5cuVkn7mj6oKbOHEievXqpXEbPnw4qlWrpm6lUb2Py5Yt0yhDzllMqlhf/g589913uf4bl/O+litXDkDuf0B69+6N7OxszJ07N9frZ2Vlqbf39/eHqakpli9frvHZk3ss+TExMcH48eMRERGhk1O5nz59ik6dOuHevXvYs2dPrm7DgqSlpSEsLCzPdarxRaqkt2fPnnj48GGu7zDw/Lsp93ueH2NjY/Ts2RPbt29Xn2H3orz+9rysWbNmOHv2rMayO3fu4Pjx4+jdu3euz3CvXr0wZMgQREVF4dSpUwWW3aRJEzg7O2PNmjUan7W9e/ciIiJC46zPqlWr4urVqxox//vvv7nOSHv5s2xkZKRuPVO9hrGxca6/f1u3bs01XUF+kpOTcePGDTRv3lzW9sUVW5YIgDRge/78+Rg2bBiaNGmC48eP59mPrm82NjZYvXo13n//fTRu3Bh9+/aFk5MTYmJisHv3brRo0ULjj6GFhQX27duHQYMGwc/PD3v37sXu3bsxbdo0dZdGly5d0LZtW0yfPh23bt1CgwYNcODAAfz+++/47LPP1C0nPj4+mD59OubOnYu33noLPXr0gLm5Oc6cOQN3d3fMmzdPq2OpXbs22rRpA19fXzg4OODs2bPYtm2bxoD0gvTp0wezZs2ChYUFhg4dqjHO5MmTJ6hUqRJ69eqFBg0aoHz58jh48CDOnDmj0cy/YsUKBAcH48iRI7LmM8qL3Pfk2rVraN++PXr37o3atWvDxMQEv/32G+Lj4zVaxXx9fbF69Wp8/vnn8PHxgbOzc56DgVWnWgcEBGgMaH3RO++8g2+++QYJCQlo2LAh+vXrh1WrViE5ORnNmzfHoUOHEBUVJes4hw0bho8++gg9e/ZEQEAA/v33X+zfvz/Xf89y3ldfX18A0mDzoKAgGBsbo2/fvmjdujU+/PBDzJs3D+Hh4QgMDISpqSmuX7+OrVu34ptvvkGvXr3g5OSECRMmYN68eXj77bfRqVMnXLhwAXv37n3t7ozBgwdj1qxZWLBgAbp16/ZaZQ0YMACnT5/GBx98gIiICI1B2eXLly+w/LS0NDRv3hxvvvkmOnToAA8PDyQlJWHnzp3466+/0K1bN3XX7cCBA/HTTz9h3LhxOH36NN566y08ffoUBw8exMiRI9G1a1fZ3/OCzJ8/H0eOHIGfnx+GDx+O2rVrIzExEefPn8fBgwcLneKga9eu+Pnnn3Ht2jV1a/emTZvU0xrkpVOnTjAxMcHGjRsLvI6aqakpFixYgCFDhqB169bo168f4uPj1dMejB07Vr3tBx98gMWLFyMoKAhDhw5FQkIC1qxZgzp16mic2DNs2DAkJiaiXbt2qFSpEm7fvo3ly5ejYcOG6pbct99+GyEhIRgyZAiaN2+OS5cuYePGjbnGiubn4MGDEEKga9eusrYvtor25DsqSqrT/FWn0Kvkddp0WlqaGDp0qLC1tRXW1taid+/eIiEhId+pA14uc9CgQaJcuXK5YmjdurWoU6eO1rEfOXJEBAUFCVtbW2FhYSGqVq0qBg8eLM6ePZvrNW/cuCECAwOFlZWVcHFxEbNnz851quuTJ0/E2LFjhbu7uzA1NRXVqlUTX331lcZp2Spr164VjRo1Eubm5sLe3l60bt1ahIaGqtfnN4P3y6frfv7556Jp06bCzs5OWFpaipo1a4ovvvhCZGZmyqqD69evCwACgPj777811mVkZIiJEyeKBg0aCGtra1GuXDnRoEEDsWrVKo3tVO+XNrNx5zeDd2HvycOHD8WoUaNEzZo1Rbly5YStra3w8/MT//vf/zTKiYuLE507dxbW1tYCQL7TCGzfvl0AED/++GO+sR49elQAEN98840QQprpeMyYMaJChQqiXLlyokuXLuLOnTuypg7Izs4WkydPFo6OjsLKykoEBQWJqKioXFMHyHlfs7KyxCeffCKcnJyEQqHINY3Ad999J3x9fYWlpaWwtrYW9erVE5MmTRL379/XiCc4OFi4ubkJS0tL0aZNG/Hff/+91gzeKnPmzNHJLO2qaTTyur14inpelEql+P7770W3bt1ElSpVhLm5ubCyshKNGjUSX331lcjIyNDYPi0tTUyfPl14eXkJU1NT4erqKnr16qUxM7Tc73lBdRMfHy9GjRolPDw81K/Tvn178d133xVaHxkZGcLR0VHMnTtXvaxevXqicuXKBe7Xpk0b4ezsLJRKpXrqgK1bt+a57ZYtW9R/nxwcHMSAAQPE3bt3c233yy+/CG9vb2FmZiYaNmwo9u/fn2vqgG3btonAwEDh7OwszMzMROXKlcWHH34oYmNj1dukp6eL8ePHqz+HLVq0EGFhYbn+3uU3dUCfPn1Ey5YtCzz+koDXhqMSa/Dgwdi2bRtSU1MNHQqVMD/++COGDRuGO3fuoFKlSoYOh0qRuXPnYt26dbh+/Xq+J4KUFXFxcfDy8sLmzZtLfMsSxywRUZkTGxsLhUIBBwcHQ4dCpczYsWORmpqKzZs3GzoUg1u6dCnq1atX4hMlgGOWqAg9ePCgwOkIzMzM+ONFehUfH49t27ZhzZo1aNasGaysrAwdEpUy5cuXlzUfU1kwf/58Q4egM0yWqMi88cYbBU5H0Lp1axw9erToAqIyJyIiAhMnTkTTpk3x/fffGzocIiohOGaJisyJEyfw7NmzfNfb29urzyIiIiIqLpgsERERERWAA7yJiIiICsAxSzqQk5OD+/fvw9rausRfLJCIiKisEELgyZMncHd3L/AC00yWdOD+/fvw8PAwdBhERET0Cgqbc43Jkg6oLtp4584d2NjY6KRMpVKJAwcOqC+HQPljXcnHutIO60s+1pV2WF/y6bOuUlJS4OHhoXHx5bwwWdIBVdebjY2NTpMlKysr2NjY8ItUCNaVfKwr7bC+5GNdaYf1JV9R1FVhQ2g4wJuIiIioAEyWiIiIiArAZImIiIioAEyWiIiIiArAZImIiIioAEyWiIiIiArAZImIiIioAEyWiIiIiArAZImIiIioAEyWiIiIiArAZImIiIioAEyWiIiIiArAC+kSkdbS0oDwcMDKCqhcGXBwMHRERET6w2SJqIzKzgauXQOePJGSnapVAYVCWp6aCpQrBxgZAbGxwJ49wMWLwJ070u2//4DMzOdlVaoEWFsDXl7A/PlA3brS8kIu5E1EVCIwWSIq5WJjgd27pdvNmyZ4+LA9KlQwQUwMkJz8fDs7OyAnB0hJeb5MoQCEyLtcV1dpXXw8cPeutCwiAti/HzA2lsobNw748EPpMRFRScVkiagUEAK4cAHYuhUIDQVu3ZK6yqysgEePXtxSAaA87t+XnpUvD1SoAMTFAUlJeZcLAE2bAm3bAlWqAB4eQI0agI+PlEwlJwNXrgDPngErVgC//Sa1TiUkAFOmADNmAG++KbU6DR4MtGsHPH4M2NpKLVdERMUdkyWiYiozU+oOs7N7nlQkJAB//SXdTpx4nhDdvw91AvSiZ8+k+6ZNgbffBho1ysLFi2Fo0KAZXF1N0LCh1AqUng5ERgKWloC9vZREpaZKSY+dHWBhkX+ctrZAs2bS43btgOhoKYn66y9g4UKpy+7vv6Xbzz8DtWpJLVADBwILFgCTJ0uv0bEjEBTErjsiKn6YLBEVE2lpwM6d0u3vv6XuMwAwMZG6vMzMgJs389/f3Bx45x3p1qiRlEQ9fQo4O0s3AFAqBbKzExEYKGBq+nxfCwugQQPN8iwtX+04vLyke09P4P33pSTs3Dng6FHg+++lRAkAfvoJOHz4eRfesmXSrW5dKcHq3Bnw9n61GIiIdInJEpEB5eRILUQ//QRs2SINtn5ZVtbzhEKhkJKJt96Sbs7OUguQi4vUYmNjU7Txy1GjhnTr3x8YNEjqsrt2Dfj6a+m4KlSQWpQ2bQLGjHm+35gxwEcfSV17xsaGi5+IiMkSURHLygLOnwf+/BP45RdpfJGKlxfw3nuAvz9Qp47UHfbggdTFlpoqtRjZ2xss9NfWooV0Uyql1qbz56UxTi1bSi1ZP/4oJUZvvAGcOgWsWQP88490Bl7dusC0aUBAALvqiKhoMVki0kJOjjToOb+WDiFy/5BnZgJnzwLHjkm3EyekxEfF2hro1Usa/NyyZe5Bz5UqSbfSxNQUOHhQGitlZSUtW70aaNVKSgjr1ZMGq/fvL83nBDyvvzfeALp1k7Z5+20mTkSkf0yWiAqRng6sXCl1G8XFST/09eoBvr5A9erSIOrr16VxNhERUnJkZvb89vTp84HWKnZ2QOvWQN++0hgjVcJQlhgZaR63qak06Fvl3XcBR0dprFPr1sCuXVJL05kz0g0AeveWxlpdvSqdcdezp9QlSUSkS0yWiAoQEyO1Xly69HyZUil1H50/n/9+6enSTaVCBanVpHVr6VavHsfhyNG2rXQDpDPtpkwB1q6Vxj1t3gz873/SDZDOtJs2DZg0SUpS33xTaqkjInpdTJaI8nHpEhAYKLUmOTtLM1O//bbUhXb+vNS1FhMjtY54ekpjalTjjDIzpVtGhtRi4uPDOYV0wdlZSpgAYNgwYOhQwN1dSop27wb+/ReYPv359m+8Ib1/Tk5S0vXxx9IZdpmZ0tmDKjk5Unceu/SIKC9MlojycPky0L69NLi6Xj2pC6hyZWmdk5M0ELtnT8PGWNa1aiV1f6qEhACrVgH79kkJ6q5dz7vr7tyREtwlS6TLuDx5IiW1xsZSF2lmpvS+1qkjTdVQtaqUGDdokP8M5kRUdjBZInrJnTvSGVcPHgCNG0sDkUvyGWhlhbEx8Mkn0g2Q5nc6eVJq1bt3T5qeYe/e59MzvDjIHpDe76NHpccHDwLffgsAprCx6YAmTYzRu7c095O7O1sJicoaJktEL3jyRGpRiI2VWhlCQ5kolVSq+Z1U+vaVJvXMzJS68x49klqNLCyk261bUktVdjYQFgYcOgRERQmkpJjj8GFpAk1A6nZ9+22pOy8uThqEfvmyNNVBhQrSRYnNzaWWKhcXaUJRFxephdLPL3ei9eABcPq0tF7VepmSIn0Wy5XjdfWIigMmS0T/TwhpDMzFi9IP3O7d0g8flR4vzgj+8nvr7CxdFgZ4flZecnIWfvjhBLKy3sKvvxrjv/+kmdZVg8pflphY8OtXqiQl4a1aSRNujholTZGQnS2tt7eXTiB4sdWreXMpOXv6FIiKkpa1bAm0aSNNRJqdLQ1oJyL9YbJE9P+WLZN+uExNgR07pIvGUtlmZQX4+CSjU6ccTJ5sjKwsad6nbdukhMbNTWoZcnGRum6fPpUuLPzsmbQ8Lg6Ij5daKo8fl2Ysv3sX2L8f+Pzz51NKeHtLLVuPHz9/bWNjKRE6eVK6vWjLFs3nDg5Sd2OVKlJcOTnSuKugIOmaey9e2oZenxDS2a6vekmg/Mp88cQDpVJKjitXlloYybCYLBFB6kaZNEl6vGjR8wvDEr3IxARo0kS6aSs9XUp6/vsPmD0bSEqSWrN27pQ+b48fS0mVqamUhJUvLz3/8Ufp8jDW1tKJBenp0uScJ09KrVyA1KJ1+rR0e9GqVVIXo7u71FpasaLUumVmJv3Qe3tLZwAaG0tnc9asKR3jw4dSK5dJKfqFUE0Ym50ttR7XrSuNT9ywQUp2q1SRktf0dCn5dHGRpqJ4/Bj46ivgyBEpAX7nHWDCBOl99PCQ6jIzUyq/a1cpaV21Smr969pV6tJ99Egq+9kz6f2tVAno3l2BjRtrYuNGYzRvLo2Ri4yUpshQKqWTE549k96H99+XumObNZNuhw5Jn5FGjaTyhZDeYzc36bOQmfm8CzojQ/PMTzmSkqTXdnXlGaIqpeirQPRqsrKk2bMzM4FOnYDRow0dEZVGFhbSD2G7dtIP7q+/Av36SdNOANKP4svj49zcgBkzcpc1Y4b0g/r4sZTo3L0L3LgB3L4tXR9QoZCmUdiyRWrZunmz4Iswq5iZSUnZo0fP5wbLypJ+qCtWlH4879+XviujR0vJllIp7ScEoFQabuR7QoI0Oen9+9JlhO7fl8aN9egh1cX69VLycuuWdAkdLy/pOFNSCi+7TZvnj1etev74zh3N7VaufP74+nVg3br8y1y0yASAlNFs3fp8+aFDzx+bmkrv8bJlz5cpFM/P0FS1PublnXekOjh7VppzTAgpwVNtryrD0VFKrKpXl+7j4qSzRjMzpc9SdraUbLm6Si2VlStLMXXvLu3/++9SEmlnJ427i42VkuxGjaSzScuXl7aLi5M+Kx4ewIUL0j8Aquk6bGyet4xeuSKdYNGqlVS/ixYBLi7GcHGpgqZNpe+EISiE4ImxryslJQW2trZITk6GjY6uZKpUKrFnzx506tQJpmxDL9Dr1tXy5dJFW21tpRamihX1EGQxwc+Vdkp6fWVlSXOBxcZKP5z37km3rCxpAHl0tDTgPC1Nmlcsrws550fVOpWaKl3UOSZG4OZNBSpXFqhcWQFra6l1ok4dabb7P/6QfkynTZOmd6haVXp++LCUmNWvL3VP3rsntZL4+0uPDxwA+vSRkriICClGW1sp0ZkxQ+qi6tYN+PTT3DPly9G0qZSkPnwoHY+FhVQvN25I/zyZm0vXL6xRQ/qhP3hQmlZk9WppHyGkuoiPl6avuHMH+OwzqSUqKkqahNbTU+rStbCQEobjx4GdOwUqV05Br17lERZmjDfekP5pO3hQatl6802gWjUpGTl2TDruHTuk5K5qVSn5ePpUSh7s7KS6SkmRYsnOzj+JkuvFpOx1yqhRQ2ql++svqXu4YkUp1ryYmEifzfycPJmFZs1028Yj9/e7xCVLK1euxFdffYW4uDg0aNAAy5cvR1PVqMyXrF+/HkOGDNFYZm5ujvQXplYWQmD27Nn4/vvvkZSUhBYtWmD16tWoVq2a7JiYLBnW69RVcrL0h+fRI+k/xo8/1lOQxQQ/V9opS/WVkyO1uiQlSa0Mp05JLRGWltKyu3elpMvZWWo1CQ19tdcpV076kbeyAj74AFixQvpRrVtXc6b8atWkBO/pU2kgu42NFFNBKlWSEo0uXaSB8VeuAN9/LyUQkydLF68GgOBgKTETQmohy6u7UdV9JYTUKlelirRdbGzB3VN5XR8yLxkZSuzbp91nKzVVSsq8vaUENy7ueVcqINWVhYV03NOmSUnahx9KXbbm5lJiaGmpOQFrbKzU/XftmnSfkiL9HQwMlFoqzcykuoiMlLqM09KkROz336VEu2tX6f25d09KZitWlLoyz5+Xyn6RkZH0OTMze35maE6O1HV444a0n7GxlCiGhUnrPvsMsLHJxq5dD3HihAMsLHT7PZT7+12iuuG2bNmCcePGYc2aNfDz88PSpUsRFBSEyMhIODs757mPjY0NIiMj1c8VL32KFy5ciGXLlmHDhg3w8vLCzJkzERQUhCtXrsDCwkKvx0OGN3++lCjVrAkMH27oaIgMx8hI82zB9u2lW16EkLq2jIykH+FDhwBHxyykph6Ct3d7PHhggqdPpeRi717pR7RFC+mSNMnJz1uzVqx4Xt6lS9IPfZs2UteRasJRMzNpf0BarxpUn5YmjeW5fVuaH2vYMKm158XEp1MnaXyRynvvPX9cs2bB9aEa56NQSP9QqRTWDSR3jM+rzNVVvvzzbq1y5TTjUi0DpBY7VWIISIlofipXlhKXvNSq9fxx7drPu94AqVVNoZBaBfMTFyd1uUVHS58lBwdpLFbTplL334tycqSEy9JSWhcVJSV/DRoASmUOGjf+B8bGnfJ/MT0rUcnS4sWLMXz4cHVr0Zo1a7B7926sXbsWU1TXQHiJQqGAq6trnuuEEFi6dClmzJiBrl27AgB++uknuLi4YOfOnejbt69+DoSKhTt3gKVLpccLFpSuwaxE+qRQAA0bPn9eowagVArs2ZOOFi2Extl3LyYoEyZIiU1goPTDe+qU1HLQpQuwZ480nYKPj5RQrV4tdd8EBUlTLNjYAPPmSV10L545JoT0o2yosSxl1cvJTl5cXaWzMV/UKZ98x8hI6uZU8fF59dj0ocT8PGRmZuLcuXOYOnWqepmRkRH8/f0RFhaW736pqamoUqUKcnJy0LhxY3z55ZeoU6cOACA6OhpxcXHw9/dXb29raws/Pz+EhYXlmyxlZGQgIyND/Tzl/0cIKpVKKJXK1zpOFVU5uiqvNHvVupo2zRjp6UZ4660cdOiQjbJQ1fxcaYf1JZ+cunJ3B/r3lx4fOiS1GNWvLyVfb72lKkfqohs//vl+L85rpSreyOj5Y0dHlLjvLz9b8umzruSWWWKSpYcPHyI7OxsuLi4ay11cXHD16tU896lRowbWrl2L+vXrIzk5GV9//TWaN2+Oy5cvo1KlSoiLi1OX8XKZqnV5mTdvHoKDg3MtP3DgAKysrLQ9tAKFvurAgDJIm7q6edMWGze2BgC8885f2Ls3SU9RFU/8XGmH9SWftnWV32DfsoKfLfn0UVdpqvk3ClFikqVX0axZMzR7YcKc5s2bo1atWvj2228xd+7cVy536tSpGDdunPp5SkoKPDw8EBgYqNMB3qGhoQgICCj1A0tfl7Z1JQTQrp0xhFCgT58cfPpp8yKIsnjg50o7rC/5WFfaYX3Jp8+6SpEzdwRKULLk6OgIY2NjxMfHayyPj4/Pd0zSy0xNTdGoUSNE/f81A1T7xcfHw+2FDu/4+Hg0fLFD/iXm5uYwz2OWL1NTU52/kfoos7SSW1cbNwInTkhN/V9/bQRT07J3VVR+rrTD+pKPdaUd1pd8+vqNlaPE/EqYmZnB19cXh16YsSsnJweHDh3SaD0qSHZ2Ni5duqROjLy8vODq6qpRZkpKCk6dOiW7TCpZ7t8Hxo6VHk+fLp1qTEREVJAS07IEAOPGjcOgQYPQpEkTNG3aFEuXLsXTp0/VZ8cNHDgQFStWxLx58wAAISEhePPNN+Hj44OkpCR89dVXuH37NoYNGwZAOlPus88+w+eff45q1aqppw5wd3dHt27dDHWYpCdZWdLg0gcPpEGlL/SkEhER5atEJUt9+vTBgwcPMGvWLMTFxaFhw4bYt2+feoB2TEwMjF6YvOLx48cYPnw44uLiYG9vD19fX5w8eRK1a9dWbzNp0iQ8ffoUI0aMQFJSElq2bIl9+/ZxjqVSJjVVSpSOHZPmKdm6VZqzhYiIqDAlKlkCgNGjR2N0PhfvOnr0qMbzJUuWYMmSJQWWp1AoEBISgpCQEF2FSMVMWJg0Yd2VK9K8LJs2STMUExERyVHikiUibWzdKl1XSghpgrvffpOu2k1ERCRXiRngTaStqChg6FApUerTR7pILhMlIiLSFluWqNQaNEi6UvdbbwG//MLLmRAR0athyxKVSlevSlfaNjWVxigxUSIiolfFZIlKpa1bpfuAAM6lREREr4fJEpVKqmTp3XcNGwcREZV8TJao1Ll6Fbh0SeqC69rV0NEQEVFJx2SJSp3du6X79u0Be3vDxkJERCUfkyUqdcLCpPu2bQ0bBxERlQ5MlqjU+ecf6f7NNw0bBxERlQ5MlqhUuXsXuHcPMDYGfH0NHQ0REZUGTJaoVFG1KjVoAJQrZ9hYiIiodGCyRKUKu+CIiEjXmCxRqcJkiYiIdI3JEpUamZnAuXPSYyZLRESkK0yWqNS4dEmB9HTAwQHw8TF0NEREVFowWaJS49QpBQCpVUmhMHAwRERUajBZolLjxWSJiIhIV5gsUanBZImIiPSByRKVCklJZrh5UwGFAmja1NDREBFRacJkiUqUI0eAChUANzegRw/pDDgAuHZNumJurVqAra0BAyQiolLnlZIlpVKJO3fuIDIyEomJibqOiShfM2YAiYlAXBzw22/At99Ky0+dcgMAtGxpwOCIiKhUkp0sPXnyBKtXr0br1q1hY2MDT09P1KpVC05OTqhSpQqGDx+OM2fO6DNWKuP++Qc4eRIwMwOmT5eWhYQAMTHAX39VBAAMGmTAAImIqFSSlSwtXrwYnp6eWLduHfz9/bFz506Eh4fj2rVrCAsLw+zZs5GVlYXAwEB06NAB169f13fcVAYtXizdDxgAzJkDVK8OPHwI+PubIDPTBHXqCDRrZtAQiYioFDKRs9GZM2dw/Phx1KlTJ8/1TZs2xQcffIA1a9Zg3bp1+Ouvv1CtWjWdBkplW0qK1O0GAJ99BpiYAMuWAW+/Ddy6JZ0FN2xYDhQKY8MFSUREpZKsZOnXX3+VVZi5uTk++uij1wqIKC/HjwNZWUDVqkD9+tKyoCAgLAwYOlQgMTEVAwZYAGCyREREusWz4ahEOHhQuvf311zepAlw9mwWli8/DDu7Ig+LiIjKAFktSz169JBd4I4dO145GKL8HDok3bdvn/d6Xt6EiIj0RVayZMuJa8iA4uKA//6THrdta9hYiIio7JGVLK1bt07fcRDl6/Bh6b5hQ8DR0aChEBFRGcQxS1Ts/f23dM9WJSIiMgRZLUsv27ZtG/73v/8hJiYGmarrTfy/8+fP6yQwIpXTp6V7XiCXiIgMQeuWpWXLlmHIkCFwcXHBhQsX0LRpU1SoUAE3b95Ex44d9REjlWHp6cC//0qP/fwMGwsREZVNWidLq1atwnfffYfly5fDzMwMkyZNQmhoKMaMGYPk5GR9xEhl2IUL0vxKzs5A5cqGjoaIiMoirZOlmJgYNG/eHABgaWmJJ0+eAADef/992ZNXEsml6oJr2pTTAxARkWFonSy5uroiMTERAFC5cmX8888/AIDo6GgIIXQbXR5WrlwJT09PWFhYwM/PD6dVv6Z5+P777/HWW2/B3t4e9vb28Pf3z7X94MGDoVAoNG4dOnTQ92GQTKq3i11wRERkKFonS+3atcMff/wBABgyZAjGjh2LgIAA9OnTB927d9d5gC/asmULxo0bh9mzZ+P8+fNo0KABgoKCkJCQkOf2R48eRb9+/XDkyBGEhYXBw8MDgYGBuHfvnsZ2HTp0QGxsrPrGFrLi49Qp6b5pU8PGQUREZZfWZ8N99913yMnJAQCMGjUKFSpUwMmTJ/HOO+/gww8/1HmAL1q8eDGGDx+OIUOGAADWrFmD3bt3Y+3atZgyZUqu7Tdu3Kjx/IcffsD27dtx6NAhDBw4UL3c3Nwcrq6ueo2dtPf4MXDjhvS4SRPDxkJERGWX1smSkZERjIyeN0j17dsXffv21WlQecnMzMS5c+cwdepUjVj8/f0RFhYmq4y0tDQolUo4ODhoLD969CicnZ1hb2+Pdu3a4fPPP0eFChV0Gj9pLzxcuvf0BF56y4iIiIqM1snSvn37UL58ebRs2RKANIbo+++/R+3atbFy5UrY29vrPEgAePjwIbKzs+Hi4qKx3MXFBVevXpVVxuTJk+Hu7g7/F67G2qFDB/To0QNeXl64ceMGpk2bho4dOyIsLAzGxnlfwT4jIwMZGRnq5ykpKQAApVIJpVKp7aHlSVWOrsoric6eNQJgjAYNcqBUZue7HetKPtaVdlhf8rGutMP6kk+fdSW3TIXQclR2vXr1sGDBAnTq1AmXLl1CkyZNMH78eBw5cgQ1a9bU26VR7t+/j4oVK+LkyZNo1qyZevmkSZNw7NgxnFINbsnH/PnzsXDhQhw9ehT169fPd7ubN2+iatWqOHjwINrnc9XWOXPmIDg4ONfyTZs2wcrKSuYRUWGWLGmMY8c80K9fBPr0uWbocIiIqJRJS0tD//79kZycDBsbm3y307plKTo6GrVr1wYAbN++HV26dMGXX36J8+fPo1OnTq8ecSEcHR1hbGyM+Ph4jeXx8fGFjjf6+uuvMX/+fBw8eLDARAkAvL294ejoiKioqHyTpalTp2LcuHHq5ykpKerB4wVVtjaUSiVCQ0MREBAAU1NTnZRZ0kybJn08e/euhk6dfPLdjnUlH+tKO6wv+VhX2mF9yafPulL1DBVG62TJzMwMaWlpAICDBw+qB0o7ODjIftFXYWZmBl9fXxw6dAjdunUDAOTk5ODQoUMYPXp0vvstXLgQX3zxBfbv348mMkYJ3717F48ePYKbm1u+25ibm8Pc3DzXclNTU52/kfoosyR49gyIjJQev/GGCeRUQVmtq1fButIO60s+1pV2WF/y6es3Vg6tk6WWLVti3LhxaNGiBU6fPo0tW7YAAK5du4ZKlSppW5xWxo0bh0GDBqFJkyZo2rQpli5diqdPn6rPjhs4cCAqVqyIefPmAQAWLFiAWbNmYdOmTfD09ERcXBwAoHz58ihfvjxSU1MRHByMnj17wtXVFTdu3MCkSZPg4+ODoKAgvR4LFezSJSA7G3ByAtzdDR0NERGVZVrPs7RixQqYmJhg27ZtWL16NSpWrAgA2Lt3r94nc+zTpw++/vprzJo1Cw0bNkR4eDj27dunHvQdExOD2NhY9farV69GZmYmevXqBTc3N/Xt66+/BgAYGxvj4sWLeOedd1C9enUMHToUvr6++Ouvv/JsOaKic+GCdN+oEWfuJiIiw9K6Zaly5crYtWtXruVLlizRSUCFGT16dL7dbkePHtV4fuvWrQLLsrS0xP79+3UUGenSi8kSERGRIWmdLMXExBS4vjKvdko6oJpjqWFDQ0ZBRET0CsmSp6cnFAX0i2Rn5z8fDpEc2dnAxYvSY7YsERGRoWmdLF1Q9Y/8P6VSiQsXLmDx4sX44osvdBYYlV2RkdLZcOXKAdWqGToaIiIq67ROlho0aJBrWZMmTeDu7o6vvvoKPXr00ElgVHap8vEGDQAjrU9BICIi0i2d/RTVqFEDZ86c0VVxVIZxcDcRERUnWrcsvTzxpBACsbGxmDNnDqqxz4R0gMkSEREVJ1onS3Z2drkGeAsh4OHhgc2bN+ssMCqbhGCyRERExYvWydKRI0c0nhsZGcHJyQk+Pj4wMdG6OCINMTHA48eAiQlQp46hoyEiInqFZKl169b6iIMIwPNWpTp1AE6iTkRExcErNQXduHEDS5cuRUREBACgdu3a+PTTT1G1alWdBkdlD7vgiIiouNH6bLj9+/ejdu3aOH36NOrXr4/69evj1KlTqFOnDkJDQ/URI5UhTJaIiKi40bplacqUKRg7dizmz5+fa/nkyZMREBCgs+Co7GGyRERExY3WLUsREREYOnRoruUffPABrly5opOgqGx6+BC4e1d6nMfcp0RERAahdbLk5OSEcNVVTl8QHh4OZ2dnXcREZZTqY+XjA9jYGDQUIiIiNa274YYPH44RI0bg5s2baN68OQDgxIkTWLBgAcaNG6fzAKnsYBccEREVR1onSzNnzoS1tTUWLVqEqVOnAgDc3d0xZ84cjBkzRucBUtlx7px0z2SJiIiKE62SpaysLGzatAn9+/fH2LFj8eTJEwCAtbW1XoKjsuXsWem+SRPDxkFERPQircYsmZiY4KOPPkJ6ejoAKUliokS68PgxcOOG9NjX17CxEBERvUjrAd5NmzbFBdXgEiIdOX9euvf2BhwcDBsLERHRi7QeszRy5EiMHz8ed+/eha+vL8qVK6exvn79+joLjsoOVRccW5WIiKi40TpZ6tu3LwBoDOZWKBQQQkChUCA7O1t30VGZoRrczfFKRERU3GidLEVHR+sjDirj2LJERETFldbJUpUqVfJcnpOTgz179uS7nig/jx4Bqhy8cWPDxkJERPQyrZOll0VFRWHt2rVYv349Hjx4AKVSqYu4qAxRDe728QHs7Q0bCxER0cu0PhsOAJ49e4affvoJrVq1Qo0aNXDy5EnMmjULd1UX9iLSArvgiIioONOqZenMmTP44YcfsHnzZlStWhUDBgzAyZMnsWrVKtSuXVtfMVIpx8koiYioOJOdLNWvXx8pKSno378/Tp48iTp16gAApkyZorfgqGxQnQnHliUiIiqOZHfDRUZGolWrVmjbti1bkUhnHjwAbt+WHnNwNxERFUeyk6WbN2+iRo0a+Pjjj1GpUiVMmDABFy5cgEKh0Gd8VMqpWpWqVwdsbQ0bCxERUV5kJ0sVK1bE9OnTERUVhZ9//hlxcXFo0aIFsrKysH79ely7dk2fcVIpxcHdRERU3L3S2XDt2rXDL7/8gtjYWKxYsQKHDx9GzZo1eakT0to//0j3TZsaNg4iIqL8vFKypGJra4uRI0fi7NmzOH/+PNq0aaOjsKgsyMkBTp6UHrdoYdhYiIiI8vNaydKLGjZsiGXLlumqOCoDIiOBx48BS0ugYUNDR0NERJQ3WclShw4d8I+qv6QAT548wYIFC7By5crXDoxKvxMnpPumTQFTU8PGQkRElB9Z8yy9++676NmzJ2xtbdGlSxc0adIE7u7usLCwwOPHj3HlyhX8/fff2LNnDzp37oyvvvpK33FTKcAuOCIiKglkJUtDhw7Fe++9h61bt2LLli347rvvkJycDABQKBSoXbs2goKCcObMGdSqVUuvAVPpoWpZat7csHEQEREVRPaYJXNzc7z33nv4888/8fjxYzx+/Bj3799Heno6Ll26hK+//rpIEqWVK1fC09MTFhYW8PPzw+nTpwvcfuvWrahZsyYsLCxQr1497NmzR2O9EAKzZs2Cm5sbLC0t4e/vj+vXr+vzEAjAw4eAaraJZs0MGwsREVFBXnmAt62tLVxdXWFahINNtmzZgnHjxmH27Nk4f/48GjRogKCgICQkJOS5/cmTJ9GvXz8MHToUFy5cQLdu3dCtWzf8999/6m0WLlyIZcuWYc2aNTh16hTKlSuHoKAgpKenF9VhlUmqLrhatQAHB8PGQkREVBCdnQ1XFBYvXozhw4djyJAhqF27NtasWQMrKyusXbs2z+2/+eYbdOjQARMnTkStWrUwd+5cNG7cGCtWrAAgtSotXboUM2bMQNeuXVG/fn389NNPuH//Pnbu3FmER1b2cLwSERGVFLIvpGtomZmZOHfuHKZOnapeZmRkBH9/f4SFheW5T1hYGMaNG6exLCgoSJ0IRUdHIy4uDv7+/ur1tra28PPzQ1hYGPr27ZtnuRkZGcjIyFA/T0lJAQAolUoolcpXOr6XqcrRVXnFzd9/GwMwgp9fFpRK8Vpllfa60iXWlXZYX/KxrrTD+pJPn3Ult8wSkyw9fPgQ2dnZcHFx0Vju4uKCq1ev5rlPXFxcntvHxcWp16uW5bdNXubNm4fg4OBcyw8cOAArK6vCD0YLoaGhOi2vOFAqFTh9ujMAIDPzKPbseaqTcktjXekL60o7rC/5WFfaYX3Jp4+6SktLk7VdiUmWipOpU6dqtFilpKTAw8MDgYGBsLGx0clrKJVKhIaGIiAgoEjHhRWFU6cUUCqN4egoMGxYa7zutZhLc13pGutKO6wv+VhX2mF9yafPulL1DBXmlZKlpKQkbNu2DTdu3MDEiRPh4OCA8+fPw8XFBRUrVnyVIgvl6OgIY2NjxMfHayyPj4+Hq6trnvu4uroWuL3qPj4+Hm5ubhrbNCxgSmlzc3OYm5vnWm5qaqrzN1IfZRqa6gTG5s0VMDPT3bGVxrrSF9aVdlhf8rGutMP6kk9fv7FyaD3A++LFi6hevToWLFiAr7/+GklJSQCAHTt2aIwn0jUzMzP4+vri0KFD6mU5OTk4dOgQmuVz7nmzZs00tgekZjzV9l5eXnB1ddXYJiUlBadOncq3THp9x45J9y1bGjYOIiIiObROlsaNG4fBgwfj+vXrsLCwUC/v1KkTjh8/rtPg8nrt77//Hhs2bEBERAQ+/vhjPH36FEOGDAEADBw4UCNh+/TTT7Fv3z4sWrQIV69exZw5c3D27FmMHj0agDSh5meffYbPP/8cf/zxBy5duoSBAwfC3d0d3bp10+uxlFVZWcDRo9Ljtm0NGgoREZEsWnfDnTlzBt9++22u5RUrVixwULQu9OnTBw8ePMCsWbMQFxeHhg0bYt++feoB2jExMTAyep7/NW/eHJs2bcKMGTMwbdo0VKtWDTt37kTdunXV20yaNAlPnz7FiBEjkJSUhJYtW2Lfvn0aiSDpzoULQEoKYGsLNGpk6GiIiIgKp3WyZG5unueAqGvXrsHJyUknQRVk9OjR6pahlx1VNVm84N1338W7776bb3kKhQIhISEICQnRVYhUgCNHpPs2bQBjY4OGQkREJIvW3XDvvPMOQkJC1HMTKBQKxMTEYPLkyejZs6fOA6TS5fBh6b5dO8PGQUREJJfWydKiRYuQmpoKZ2dnPHv2DK1bt4aPjw+sra3xxRdf6CNGKiUyM4G//pIec7wSERGVFFp3w9na2iI0NBQnTpzAv//+i9TUVDRu3FhjFmyivJw+DaSlAU5OQJ06ho6GiIhIHq2SJaVSCUtLS4SHh6NFixZowQt7kRZU45XatgWMStRVCYmIqCzT6ifL1NQUlStXRnZ2tr7ioVJMNV6JXXBERFSSaP3//fTp0zFt2jQkJibqIx4qpZ49A06elB5zcDcREZUkWo9ZWrFiBaKiouDu7o4qVaqgXLlyGuvPnz+vs+Co9Dh5UhrgXbEiUK2aoaMhIiKST+tkiTNb06t4ccqA171wLhERUVHSOlmaPXu2PuKgUu7AAem+fXvDxkFERKQtrZMllXPnziEiIgIAUKdOHTTitSsoHw8eAOfOSY8DAw0bCxERkba0TpYSEhLQt29fHD16FHZ2dgCApKQktG3bFps3by6SS55QyRIaCggBNGgAuLkZOhoiIiLtaH023CeffIInT57g8uXLSExMRGJiIv777z+kpKRgzJgx+oiRSrh9+6T7Dh0MGwcREdGr0Lplad++fTh48CBq1aqlXla7dm2sXLkSgexjoZfk5DwfrxQUZNhYiIiIXoXWLUs5OTkwNTXNtdzU1BQ5OTk6CYpKj3//BeLjgXLlAE74TkREJZHWyVK7du3w6aef4v79++pl9+7dw9ixY9GepzrRS/bvl+7btQPMzAwbCxER0avQOllasWIFUlJS4OnpiapVq6Jq1arw8vJCSkoKli9fro8YqQTjeCUiIirptB6z5OHhgfPnz+PgwYO4evUqAKBWrVrw9/fXeXBUsj15Apw4IT3meCUiIiqpXmmeJYVCgYCAAAQEBOg6HipFDh8GsrIAHx+galVDR0NERPRqZHfDHT58GLVr10ZKSkqudcnJyahTpw7++usvnQZHJZuqC46tSkREVJLJTpaWLl2K4cOHw8bGJtc6W1tbfPjhh1i8eLFOg6OSSwhg1y7pcadOho2FiIjodchOlv799190KGCUbmBgIM6prmlBZV54OHD3LmBlJZ0JR0REVFLJTpbi4+PznF9JxcTEBA8ePNBJUFTyqVqVAgIACwvDxkJERPQ6ZCdLFStWxH///Zfv+osXL8KNF/6i//fnn9J9ly6GjYOIiOh1yU6WOnXqhJkzZyI9PT3XumfPnmH27Nl4++23dRoclUyxscCZM9JjjlciIqKSTvbUATNmzMCOHTtQvXp1jB49GjVq1AAAXL16FStXrkR2djamT5+ut0Cp5Ni9W7p/4w2AjY1ERFTSyU6WXFxccPLkSXz88ceYOnUqhBAApDmXgoKCsHLlSri4uOgtUCo52AVHRESliVaTUlapUgV79uzB48ePERUVBSEEqlWrBnt7e33FRyXMs2fAwYPSYyZLRERUGrzSDN729vZ44403AAC3b99GbGwsatasCSMjrS81R6XMkSNAWhpQqRLQoIGhoyEiInp9srObtWvX5pp0csSIEfD29ka9evVQt25d3LlzR+cBUsny++/S/dtvAwqFYWMhIiLSBdnJ0nfffafR3bZv3z6sW7cOP/30E86cOQM7OzsEBwfrJUgqGZRKYPt26XGPHoaNhYiISFdkd8Ndv34dTZo0UT///fff0bVrVwwYMAAA8OWXX2LIkCG6j5BKjEOHgEePAGdnoG1bQ0dDRESkG7Jblp49e6ZxXbiTJ0+iVatW6ufe3t6Ii4vTbXRUomzeLN2/+y5g8kqj4YiIiIof2clSlSpV1Nd+e/jwIS5fvowWLVqo18fFxcHW1lb3EVKJkJ4O/Pab9LhvX8PGQkREpEuy//8fNGgQRo0ahcuXL+Pw4cOoWbMmfH191etPnjyJunXr6iVIKv727QNSUoCKFYHmzQ0dDRERke7ITpYmTZqEtLQ07NixA66urti6davG+hMnTqBfv346D5BKhi1bpPs+fQDOIEFERKWJ7GTJyMgIISEhCAkJyXP9y8kTlR1PnwJ//CE9ZhccERGVNiWmDSAxMREDBgyAjY0N7OzsMHToUKSmpha4/SeffIIaNWrA0tISlStXxpgxY5CcnKyxnUKhyHXbrBqpTLLs2iVNROntDbxwwiQREVGpUGLOWRowYABiY2MRGhoKpVKJIUOGYMSIEdi0aVOe29+/fx/379/H119/jdq1a+P27dv46KOPcP/+fWzbtk1j23Xr1qFDhw7q53Z2dvo8lFJHlVv27cuJKImIqPQpEclSREQE9u3bhzNnzqjnelq+fDk6deqEr7/+Gu7u7rn2qVu3LrarZkgEULVqVXzxxRd47733kJWVBZMXzm23s7ODq6ur/g+kFEpOBvbskR6zC46IiEqjEpEshYWFwc7OTmNSTH9/fxgZGeHUqVPo3r27rHKSk5NhY2OjkSgBwKhRozBs2DB4e3vjo48+wpAhQ6AooIkkIyMDGRkZ6ucpKSkAAKVSCaVSqc2h5UtVjq7K05dt2xTIzDRBrVoCNWpkwRDhlpS6Kg5YV9phfcnHutIO60s+fdaV3DJLRLIUFxcHZ2dnjWUmJiZwcHCQPRHmw4cPMXfuXIwYMUJjeUhICNq1awcrKyscOHAAI0eORGpqKsaMGZNvWfPmzcvz0i4HDhyAlZWVrHjkCg0N1Wl5urZq1ZsAXNCw4VXs3XvNoLEU97oqTlhX2mF9yce60g7rSz591FVaWpqs7RRCCCG30NjYWBw6dAgODg7w9/eHmZmZet3Tp0+xaNEizJo1S3aQU6ZMwYIFCwrcJiIiAjt27MCGDRsQGRmpsc7Z2RnBwcH4+OOPCywjJSUFAQEBcHBwwB9//AFTU9N8t501axbWrVtX4EWB82pZ8vDwwMOHDzVmOX8dSqUSoaGhCAgIKDBeQ3r0CKhUyQTZ2Qr8958S1asbJo6SUFfFBetKO6wv+VhX2mF9yafPukpJSYGjo6O65yk/sluWzpw5g8DAQOTk5ECpVKJixYrYuXMn6tSpAwBITU1FcHCwVsnS+PHjMXjw4AK38fb2hqurKxISEjSWZ2VlITExsdCxRk+ePEGHDh1gbW2N3377rdCK9vPzw9y5c5GRkQFzc/M8tzE3N89znampqc7fSH2UqSv79wPZ2UD9+kCdOoaPsTjXVXHDutIO60s+1pV2WF/y6es3Vg7ZydK0adPQvXt3/PDDD3j69CkmT56M1q1bIzQ0FI0aNXqlIJ2cnODk5FTods2aNUNSUhLOnTunnjX88OHDyMnJgZ+fX777paSkICgoCObm5vjjjz9gYWFR6GuFh4fD3t4+30SJnvv9d+m+WzeDhkFERKRXspOlc+fOYeXKlTAyMoK1tTVWrVqFypUro3379ti/fz8qV66styBr1aqFDh06YPjw4VizZg2USiVGjx6Nvn37qs+Eu3fvHtq3b4+ffvoJTZs2RUpKCgIDA5GWloZffvkFKSkp6oHYTk5OMDY2xp9//on4+Hi8+eabsLCwQGhoKL788ktMmDBBb8dSWjx7JrUsAUDXroaNhYiISJ+0GuCdnp6u8XzKlCkwMTFBYGAg1q5dq9PAXrZx40aMHj0a7du3h5GREXr27Illy5ap1yuVSkRGRqoHa50/fx6nTp0CAPj4+GiUFR0dDU9PT5iammLlypUYO3YshBDw8fHB4sWLMXz4cL0eS2lw8KA0EaWHB/CKDYtEREQlguxkqW7dujh58iTq16+vsXzChAnIycnR+3XhHBwc8p2AEgA8PT3x4lj1Nm3aoLCx6x06dNCYjJLkU3XBde3KiSiJiKh0k325k4EDB+LEiRN5rps0aRKCg4P12hVHxUd2NvDnn9JjdsEREVFpJztZGjZsGH7++ed810+ePBnR0dE6CYqKt3/+ARISAFtboHVrQ0dDRESkXyXmQrpUfKi64Dp3BnjGKxERlXZaJ0snT57URxxUgrw4XomIiKi00ypZ2rNnj+zrsFHpdPUqcO2a1KLEsfFERFQWyE6WfvnlF/Tt2xcbN27UZzxUzO3cKd23awfo6MouRERExZqsZGnp0qUYNmwYfvnlF/j7++s7JirGOGs3ERGVNbLmWRo3bhyWLVuGd955R9/xUDEWFwf8/zyf4EeBiIjKClktSy1atMCqVavw6NEjfcdDxdiffwJCAG+8Afz/VWaIiIhKPVnJUmhoKLy8vBAQEKC+vhqVParxSjwLjoiIyhJZyZKFhQX++OMP1K5dm5cHKaNSU4FDh6THTJaIiKgskX02nLGxMX755Rc0bdpUn/FQMbV/P5CRAVStCtSpY+hoiIiIio7Wk1IuXbpUD2FQcccL5xIRUVnFy51QoZRKYNcu6TG74IiIqKzRWbK0Y8cO1K9fX1fFUTFy8iTw+DFQoQLQvLmhoyEiIipaWiVL3377LXr16oX+/fvj1P9PuHP48GE0atQI77//Plq0aKGXIMmwdu+W7jt2BExkzcxFRERUeshOlubPn49PPvkEt27dwh9//IF27drhyy+/xIABA9CnTx/cvXsXq1ev1mesZCCqZKlzZ8PGQUREZAiy2wnWrVuH77//HoMGDcJff/2F1q1b4+TJk4iKikK5cuX0GSMZ0K1bwJUrgLExEBRk6GiIiIiKnuyWpZiYGLRr1w4A8NZbb8HU1BTBwcFMlEo5VatS8+aAvb1hYyEiIjIE2clSRkYGLCws1M/NzMzg4OCgl6Co+FCdBccuOCIiKqu0Gq47c+ZMWFlZAQAyMzPx+eefw9bWVmObxYsX6y46MqinT4EjR6THTJaIiKiskp0stWrVCpGRkernzZs3x82bNzW2UXC2wlLl8GFp1u7KlTlrNxERlV2yk6WjR4/qMQwqjl48C455MBERlVWcwZvyJASnDCAiIgKYLFE+/vsPuHsXsLAA2rY1dDRERESGw2SJ8nTggHTfpg3w/2P6iYiIyiQmS5QnVbIUGGjYOIiIiAyNyRLl8uwZcPy49JjJEhERlXWyzoa7ePGi7ALr16//ysFQ8fD330B6OuDuDtSubehoiIiIDEtWstSwYUMoFAoIIQqdSyk7O1sngZHhvNgFxykDiIiorJPVDRcdHY2bN28iOjoa27dvh5eXF1atWoULFy7gwoULWLVqFapWrYrt27frO14qAqpkiRfOJSIiktmyVKVKFfXjd999F8uWLUOnTp3Uy+rXrw8PDw/MnDkT3bp103mQVHRiY4GLF6UWJX9/Q0dDRERkeFoP8L506RK8vLxyLffy8sKVK1d0EhQZzsGD0n3jxoCjo2FjISIiKg60TpZq1aqFefPmITMzU70sMzMT8+bNQ61atXQaHBU9ThlARESkSfa14VTWrFmDLl26oFKlSuoz3y5evAiFQoE///xT5wFS0cnJAUJDpcdMloiIiCRaJ0tNmzbFzZs3sXHjRly9ehUA0KdPH/Tv3x/lypXTeYBUdC5dAuLjgXLlgGbNDB0NERFR8fBKk1KWK1cOI0aMwOLFi7F48WIMHz5c74lSYmIiBgwYABsbG9jZ2WHo0KFITU0tcJ82bdpAoVBo3D766CONbWJiYtC5c2dYWVnB2dkZEydORFZWlj4Ppdh68RIn5uYGDYWIiKjYeKVk6eeff0bLli3h7u6O27dvAwCWLFmC33//XafBvWjAgAG4fPkyQkNDsWvXLhw/fhwjRowodL/hw4cjNjZWfVu4cKF6XXZ2Njp37ozMzEycPHkSGzZswPr16zFr1iy9HUdxxvFKREREuWmdLK1evRrjxo1Dx44d8fjxY/UklPb29li6dKmu4wMAREREYN++ffjhhx/g5+eHli1bYvny5di8eTPu379f4L5WVlZwdXVV32xsbNTrDhw4gCtXruCXX35Bw4YN0bFjR8ydOxcrV67UGMBeFqSlAX/9JT1mskRERPSc1mOWli9fju+//x7dunXD/Pnz1cubNGmCCRMm6DQ4lbCwMNjZ2aFJkybqZf7+/jAyMsKpU6fQvXv3fPfduHEjfvnlF7i6uqJLly6YOXMmrKys1OXWq1cPLi4u6u2DgoLw8ccf4/Lly2jUqFGeZWZkZCAjI0P9PCUlBQCgVCqhVCpf61hVVOXoqrzCHDmiQEaGCTw8BLy9s1BEL6sTRV1XJRnrSjusL/lYV9phfcmnz7qSW6bWyVJ0dHSeSYS5uTmePn2qbXGyxMXFwdnZWWOZiYkJHBwcEBcXl+9+/fv3R5UqVeDu7o6LFy9i8uTJiIyMxI4dO9TlvpgoAVA/L6jcefPmITg4ONfyAwcOqBMxXQlVnZ6mZ2vX1gHggxo1YrB3b3iRvKauFVVdlQasK+2wvuRjXWmH9SWfPuoqLS1N1nZaJ0teXl4IDw/XmNUbAPbt26f1PEtTpkzBggULCtwmIiJC2xDVXhzTVK9ePbi5uaF9+/a4ceMGqlat+srlTp06FePGjVM/T0lJgYeHBwIDAzW6+V6HUqlEaGgoAgICYGpqqpMyCzJ9uvRRGDKkIjp1ctf76+lSUddVSca60g7rSz7WlXZYX/Lps65UPUOF0TpZGjduHEaNGoX09HQIIXD69Gn8+uuvmDdvHn744Qetyho/fjwGDx5c4Dbe3t5wdXVFQkKCxvKsrCwkJibC1dVV9uv5+fkBAKKiolC1alW4urri9OnTGtvEx8cDQIHlmpubwzyP08VMTU11/kbqo8yX3b8PXL4sXeIkKMgEJfV7WxR1VVqwrrTD+pKPdaUd1pd8+vqNlUPrZGnYsGGwtLTEjBkzkJaWhv79+8Pd3R3ffPMN+vbtq1VZTk5OcHJyKnS7Zs2aISkpCefOnYOvry8A4PDhw8jJyVEnQHKEh4cDANzc3NTlfvHFF0hISFB384WGhsLGxga1a9fW6lhKMlXLZpMmQIUKho2FiIiouHmlqQMGDBiA69evIzU1FXFxcbh79y6GDh2q69jUatWqhQ4dOmD48OE4ffo0Tpw4gdGjR6Nv375wd5e6jO7du4eaNWuqW4pu3LiBuXPn4ty5c7h16xb++OMPDBw4EK1atVLPPB4YGIjatWvj/fffx7///ov9+/djxowZGDVqVJ4tR6UVpwwgIiLKn9bJUrt27ZCUlAQA6okcAanfr127djoN7kUbN25EzZo10b59e3Tq1AktW7bEd999p16vVCoRGRmpHqxlZmaGgwcPIjAwEDVr1sT48ePRs2dPjUuyGBsbY9euXTA2NkazZs3w3nvvYeDAgQgJCdHbcRQ3L17iJCjIsLEQEREVR1p3wx09ejTPOYjS09Pxl2qiHj1wcHDApk2b8l3v6ekJIYT6uYeHB44dO1ZouVWqVMGePXt0EmNJ9O+/wIMHQPnywJtvGjoaIiKi4kd2snTx4kX14ytXrmicWp+dnY19+/ahYsWKuo2O9O7gQem+TRuU2IHdRERE+iQ7WWrYsKH6+mp5dbdZWlpi+fLlOg2O9E+VLPn7GzYOIiKi4kp2shQdHQ0hBLy9vXH69GmNs9jMzMzg7OwMY2NjvQRJ+pGR8fwSJ0yWiIiI8iY7WVJNQpmTk6O3YKhohYUBz54Brq5AGZopgYiISCtanw23YcMG7N69W/180qRJsLOzQ/PmzXH79m2dBkf6peqCa99empCSiIiIctM6Wfryyy9haWkJQLoQ7YoVK7Bw4UI4Ojpi7NixOg+Q9OfQIemeXXBERET503rqgDt37sDHxwcAsHPnTvTq1QsjRoxAixYt0KZNG13HR3qSnAyorvTSvr1hYyEiIirOtG5ZKl++PB49egQAOHDgAAICAgAAFhYWePbsmW6jI705elSakLJ6dcDDw9DREBERFV9atywFBARg2LBhaNSoEa5du4ZOnToBAC5fvgxPT09dx0d6ouqCY6sSERFRwbRuWVq5ciWaNWuGBw8eYPv27ajw/1dePXfuHPr166fzAEk/OL8SERGRPFq3LNnZ2WHFihW5lgcHB+skINK/e/eAiAjpDLi2bQ0dDRERUfGmdbJ0/PjxAte3atXqlYOhonH4sHTv6wvY2xs2FiIiouJO62QprzPeFC9M0pOdnf1aAZH+sQuOiIhIPq3HLD1+/FjjlpCQgH379uGNN97AgQMH9BEj6ZAQTJaIiIi0oXXLkq2tba5lAQEBMDMzw7hx43Du3DmdBEb6ERkJ3L8PmJsDzZsbOhoiIqLiT+uWpfy4uLggMjJSV8WRnqhalVq2BP5/InYiIiIqgNYtSxcvXtR4LoRAbGws5s+fj4YNG+oqLtITdsERERFpR+tkqWHDhlAoFBBCaCx/8803sXbtWp0FRrqXlSXN3A1wMkoiIiK5tE6WoqOjNZ4bGRnByckJFhYWOguK9OPcOemacHZ2QOPGho6GiIioZNA6WapSpYo+4qAioOqCa9cOMDY2bCxEREQlhaxkadmyZRgxYgQsLCywbNmyArctX7486tSpAz8/P50ESLrD68ERERFpT1aytGTJEgwYMAAWFhZYsmRJgdtmZGQgISEBY8eOxVdffaWTIOn1paUBJ05Ijzm4m4iISD5ZydKL45ReHrOUl9DQUPTv35/JUjFy4gSQmQlUqgRUq2boaIiIiEoOnc2z9KKWLVtixowZ+iiaXpHqLLh27aQL6BIREZE8sscsyTVmzBhYWlri008/feWgSPdUyVIel/YjIiKiAsges/SiBw8eIC0tDXZ2dgCApKQkWFlZwdnZGWPGjNF5kPR6nj4FTp+WHjNZIiIi0o6sbrjo6Gj17YsvvkDDhg0RERGBxMREJCYmIiIiAo0bN8bcuXP1HS+9gpMnpQkpK1cGPD0NHQ0REVHJovWYpZkzZ2L58uWoUaOGelmNGjWwZMkSjlMqpl7sguN4JSIiIu1onSzFxsYiKysr1/Ls7GzEx8frJCjSLY5XIiIienVaJ0vt27fHhx9+iPPnz6uXnTt3Dh9//DH8OYFPscPxSkRERK9H62Rp7dq1cHV1RZMmTWBubg5zc3M0bdoULi4u+P777/URI70GjlciIiJ6PVpfG87JyQl79uzB9evXERERAQCoWbMmqlevrvPg6PVxvBIREdHr0TpZUqlWrRqq/f9U0CkpKVi9ejV+/PFHnD17VmfB0evjeCUiIqLX88rJEgAcOXIEa9euxY4dO2Bra4vu3bvrKi7SAY5XIiIien1aJ0v37t3D+vXrsW7dOiQlJeHx48fYtGkTevfuDQX7eYqVsDBpvJKHB8crERERvSrZA7y3b9+OTp06oUaNGggPD8eiRYtw//59GBkZoV69enpPlBITEzFgwADY2NjAzs4OQ4cORWpqar7b37p1CwqFIs/b1q1b1dvltX7z5s16PZaicvKkdP/WWxyvRERE9Kpktyz16dMHkydPxpYtW2Btba3PmPI0YMAAxMbGIjQ0FEqlEkOGDMGIESOwadOmPLf38PBAbGysxrLvvvsOX331FTp27KixfN26dejQoYP6ueoyLiWdKllq3tywcRAREZVkspOloUOHYuXKlTh69Cjef/999OnTB/b29vqMTS0iIgL79u3DmTNn0KRJEwDA8uXL0alTJ3z99ddwd3fPtY+xsTFcXV01lv3222/o3bs3ypcvr7Hczs4u17YlXU6O1A0HMFkiIiJ6HbK74b799lvExsZixIgR+PXXX+Hm5oauXbtCCIGcnBx9xoiwsDDY2dmpEyUA8Pf3h5GREU6dOiWrjHPnziE8PBxDhw7NtW7UqFFwdHRE06ZNsXbtWgghdBa7oVy5AqSkAOXKAfXqGToaIiKikkurAd6WlpYYNGgQBg0ahOvXr2PdunU4e/YsWrRogc6dO6NXr17o0aOHzoOMi4uDs7OzxjITExM4ODggLi5OVhk//vgjatWqheYvNbOEhISgXbt2sLKywoEDBzBy5EikpqZizJgx+ZaVkZGBjIwM9fOUlBQAgFKphFKplHtYBVKV86rl/fWXAoAJ3ngjB0JkQ0dhFUuvW1dlCetKO6wv+VhX2mF9yafPupJbpkK8ZjNKTk4Odu/ejR9//BF79+7VSCIKM2XKFCxYsKDAbSIiIrBjxw5s2LABkZGRGuucnZ0RHByMjz/+uMAynj17Bjc3N8ycORPjx48vcNtZs2Zh3bp1uHPnTr7bzJkzB8HBwbmWb9q0CVZWVgWWX1SWLWuEw4cr4913IzFgwFVDh0NERFTspKWloX///khOToaNjU2+2712svSihISEXC1ABXnw4AEePXpU4Dbe3t745ZdfMH78eDx+/Fi9PCsrCxYWFti6dWuh8zv9/PPPGDp0KO7duwcnJ6cCt929ezfefvttpKenw9zcPM9t8mpZ8vDwwMOHDwusbG0olUqEhoYiICAApqamWu9fp44Jrl9X4Pffs9CxY8nvVizI69ZVWcK60g7rSz7WlXZYX/Lps65SUlLg6OhYaLL0WpNSvkybRAmQLp1SWPICAM2aNUNSUhLOnTsHX19fAMDhw4eRk5MDPz+/Qvf/8ccf8c4778h6rfDwcNjb2+ebKAFQXxPvZaampjp/I1+lzIcPgevXpcctW5qgrHwP9VH/pRXrSjusL/lYV9phfcmnr99YOXSaLOlLrVq10KFDBwwfPhxr1qyBUqnE6NGj0bdvX/WZcPfu3UP79u3x008/oWnTpup9o6KicPz4cezZsydXuX/++Sfi4+Px5ptvwsLCAqGhofjyyy8xYcKEIjs2fVCdBVerFuDgYNhYiIiISroSkSwBwMaNGzF69Gi0b98eRkZG6NmzJ5YtW6Zer1QqERkZibS0NI391q5di0qVKiEwMDBXmaampli5ciXGjh0LIQR8fHywePFiDB8+XO/Ho0+cX4mIiEh3Skyy5ODgkO8ElADg6emZ5yn/X375Jb788ss89+nQoYPGZJSlBZMlIiIi3ZE9z5KKt7d3noOyk5KS4O3trZOg6NUplc8vnstkiYiI6PVpnSzdunUL2dnZuZZnZGTg3r17OgmKXl14OJCeLo1Vql7d0NEQERGVfLK74f744w/14/3798PW1lb9PDs7G4cOHYInL21vcP/8I937+QFGWqfCRERE9DLZyVK3bt0AAAqFAoMGDdJYZ2pqCk9PTyxatEinwZH2VFd/kTGjAhEREckgO1lSXf/Ny8sLZ86cgaOjo96ColfHZImIiEi3tD4bLjo6OteypKQk2NnZ6SIeeg2PHgFRUdLjF6aaIiIioteg9aiWBQsWYMuWLern7777LhwcHFCxYkX8+++/Og2OtKM6C65aNU5GSUREpCtaJ0tr1qyBh4cHACA0NBQHDx7Evn370LFjR0ycOFHnAZJ87IIjIiLSPa274eLi4tTJ0q5du9C7d28EBgbC09NT1nXaSH9ULUt8G4iIiHRH65Yle3t73LlzBwCwb98++Pv7AwCEEHnOv0RFQwgmS0RERPqgdctSjx490L9/f1SrVg2PHj1Cx44dAQAXLlyAj4+PzgMkeW7ckAZ4m5sDDRoYOhoiIqLSQ+tkacmSJfD09MSdO3ewcOFClC9fHgAQGxuLkSNH6jxAkkc1XqlRI8DMzLCxEBERlSZaJ0umpqaYMGFCruVjx47VSUD0aji4m4iISD9e6YIYP//8M1q2bAl3d3fcvn0bALB06VL8/vvvOg2O5GOyREREpB9aJ0urV6/GuHHj0LFjRyQlJakHddvZ2WHp0qW6jo9kyMiQLqALMFkiIiLSNa2TpeXLl+P777/H9OnTYWxsrF7epEkTXLp0SafBkTzh4UBmJuDoCHh5GToaIiKi0kXrZCk6OhqNGjXKtdzc3BxPnz7VSVCknRe74BQKw8ZCRERU2midLHl5eSFc1efzgn379qFWrVq6iIm0pEqWeD04IiIi3ZN9NlxISAgmTJiAcePGYdSoUUhPT4cQAqdPn8avv/6KefPm4YcfftBnrJQPTkZJRESkP7KTpeDgYHz00UcYNmwYLC0tMWPGDKSlpaF///5wd3fHN998g759++ozVsrDo0dAVJT0mC1LREREuic7WRJCqB8PGDAAAwYMQFpaGlJTU+Hs7KyX4Khwqlal6tUBe3vDxkJERFQaaTUppeKl0cNWVlawsrLSaUCkHc6vREREpF9aJUvVq1fPlTC9LDEx8bUCIu0wWSIiItIvrZKl4OBg2Nra6isW0pIQwLlz0uM33jBsLERERKWVVslS3759OT6pGLl3D3jwADA2BurXN3Q0REREpZPseZYK636jonfhgnRfuzZgYWHYWIiIiEor2cnSi2fDUfFw/rx0n8eE6kRERKQjsrvhcnJy9BkHvQJVy1LjxoaNg4iIqDTT+nInVHyoWpaYLBEREekPk6US6uFD4M4d6XGDBoaNhYiIqDRjslRCqbrgfHwAGxvDxkJERFSaMVkqof79V7rn4G4iIiL9YrJUQqmSJc6vREREpF9Mlkqoixele45XIiIi0i8mSyVQZiYQESE9ZssSERGRfjFZKoEiIgClErC1BSpXNnQ0REREpVuJSZa++OILNG/eHFZWVrCzs5O1jxACs2bNgpubGywtLeHv74/r169rbJOYmIgBAwbAxsYGdnZ2GDp0KFJTU/VwBLqj6oKrXx/gVWiIiIj0q8QkS5mZmXj33Xfx8ccfy95n4cKFWLZsGdasWYNTp06hXLlyCAoKQnp6unqbAQMG4PLlywgNDcWuXbtw/PhxjBgxQh+HoDOqwd0cr0RERKR/si93YmjBwcEAgPXr18vaXgiBpUuXYsaMGejatSsA4KeffoKLiwt27tyJvn37IiIiAvv27cOZM2fQpEkTAMDy5cvRqVMnfP3113B3d9fLsbwuDu4mIiIqOiUmWdJWdHQ04uLi4O/vr15ma2sLPz8/hIWFoW/fvggLC4OdnZ06UQIAf39/GBkZ4dSpU+jevXueZWdkZCAjI0P9PCUlBQCgVCqhVCp1Er+qnLzK+/dfEwAK1K6dBaWSFzguqK5IE+tKO6wv+VhX2mF9yafPupJbZqlNluLi4gAALi4uGstdXFzU6+Li4uDs7Kyx3sTEBA4ODupt8jJv3jx1S9eLDhw4ACsrq9cNXUNoaKjG88ePzZGQ0AEKhcDdu/vw4EG2Tl+vJHu5rih/rCvtsL7kY11ph/Ulnz7qKi0tTdZ2Bk2WpkyZggULFhS4TUREBGrWrFlEEckzdepUjBs3Tv08JSUFHh4eCAwMhI2Orj2iVCoRGhqKgIAAmJqaqpeHhkojun18gO7dg3TyWiVdfnVFubGutMP6ko91pR3Wl3z6rCtVz1BhDJosjR8/HoMHDy5wG29v71cq29XVFQAQHx8PNzc39fL4+Hg0bNhQvU1CQoLGfllZWUhMTFTvnxdzc3OYm5vnWm5qaqrzN/LlMi9flu4bNlTwC/YSfdR/acW60g7rSz7WlXZYX/Lp6zdWDoMmS05OTnByctJL2V5eXnB1dcWhQ4fUyVFKSgpOnTqlPqOuWbNmSEpKwrlz5+Dr6wsAOHz4MHJycuDn56eXuF4XL3NCRERUtErM1AExMTEIDw9HTEwMsrOzER4ejvDwcI05kWrWrInffvsNAKBQKPDZZ5/h888/xx9//IFLly5h4MCBcHd3R7du3QAAtWrVQocOHTB8+HCcPn0aJ06cwOjRo9G3b1+eCUdEREQAStAA71mzZmHDhg3q540aNQIAHDlyBG3atAEAREZGIjk5Wb3NpEmT8PTpU4wYMQJJSUlo2bIl9u3bBwsLC/U2GzduxOjRo9G+fXsYGRmhZ8+eWLZsWdEclJYyMniZEyIioqJWYpKl9evXFzrHkhCap9ErFAqEhIQgJCQk330cHBywadMmXYSod1evAllZgJ0dL3NCRERUVEpMNxw974KrV4+XOSEiIioqTJZKkP/+k+7r1TNsHERERGUJk6USRJUs1a1r2DiIiIjKEiZLJYhqjqU6dQwbBxERUVnCZKmESEkBbt+WHjNZIiIiKjpMlkqIK1ekezc3oEIFw8ZCRERUljBZKiE4XomIiMgwmCyVEEyWiIiIDIPJUgnBwd1ERESGwWSphGDLEhERkWEwWSoBHj4E4uKkx7VrGzYWIiKisobJUgmg6oLz9ASsrQ0aChERUZnDZKkEYBccERGR4TBZKgFULUtMloiIiIoek6USQNWyxDPhiIiIih6TpWJOCHbDERERGRKTpWIuNhZ4/BgwMgJq1jR0NERERGUPk6Vi7vJlBQCgWjXAwsLAwRAREZVBTJaKOVWyxC44IiIiw2CyVMxducJkiYiIyJCYLBVzvCYcERGRYTFZKsZyctiyREREZGhMloqxBw+s8PSpAmZmgI+PoaMhIiIqm5gsFWO3b0sXgqtZEzA1NXAwREREZRSTpWLszh0bAOyCIyIiMiQmS8VYTIzUssRkiYiIyHCYLBVjt29LLUs8E46IiMhwTAwdAOVNCMDZOQ1ZWTaoW1dh6HCIiIjKLLYsFVMKBTBt2mncvJkFb29DR0NERFR2MVkiIiIiKgCTJSIiIqICMFkiIiIiKgCTJSIiIqICMFkiIiIiKgCTJSIiIqICMFkiIiIiKkCJSZa++OILNG/eHFZWVrCzsyt0e6VSicmTJ6NevXooV64c3N3dMXDgQNy/f19jO09PTygUCo3b/Pnz9XQUREREVNKUmGQpMzMT7777Lj7++GNZ26elpeH8+fOYOXMmzp8/jx07diAyMhLvvPNOrm1DQkIQGxurvn3yySe6Dp+IiIhKqBJzuZPg4GAAwPr162Vtb2tri9DQUI1lK1asQNOmTRETE4PKlSurl1tbW8PV1VVnsRIREVHpUWKSJV1ITk6GQqHI1Y03f/58zJ07F5UrV0b//v0xduxYmJjkXzUZGRnIyMhQP09JSQEgdf0plUqdxKoqR1fllWasK/lYV9phfcnHutIO60s+fdaV3DIVQgih81fXo/Xr1+Ozzz5DUlKSVvulp6ejRYsWqFmzJjZu3KhevnjxYjRu3BgODg44efIkpk6diiFDhmDx4sX5ljVnzhx1S9eLNm3aBCsrK63iIiIiIsNIS0tD//79kZycDBsbm3y3M2iyNGXKFCxYsKDAbSIiIlCzZk3181dJlpRKJXr27Im7d+/i6NGjBVbI2rVr8eGHHyI1NRXm5uZ5bpNXy5KHhwcePnxYYNnaUCqVCA0NRUBAAExNTXVSZmnFupKPdaUd1pd8rCvtsL7k02ddpaSkwNHRsdBkyaDdcOPHj8fgwYML3Mbb2/u1XkOpVKJ37964ffs2Dh8+XGgy4+fnh6ysLNy6dQs1atTIcxtzc/M8EylTU1Odv5H6KLO0Yl3Jx7rSDutLPtaVdlhf8unrN1YOgyZLTk5OcHJy0lv5qkTp+vXrOHLkCCpUqFDoPuHh4TAyMoKzs7Ps11E1zqnGLumCUqlEWloaUlJS+EUqBOtKPtaVdlhf8rGutMP6kk+fdaX63S6sk63EDPCOiYlBYmIiYmJikJ2djfDwcACAj48PypcvDwCoWbMm5s2bh+7du0OpVKJXr144f/48du3ahezsbMTFxQEAHBwcYGZmhrCwMJw6dQpt27aFtbU1wsLCMHbsWLz33nuwt7eXHduTJ08AAB4eHro9aCIiItK7J0+ewNbWNt/1JWaA9+DBg7Fhw4Zcy48cOYI2bdoAABQKBdatW4fBgwfj1q1b8PLyyrMs1T7nz5/HyJEjcfXqVWRkZMDLywvvv/8+xo0bl+94pbzk5OTg/v37sLa2hkKheKXje5lqHNSdO3d0Ng6qtGJdyce60g7rSz7WlXZYX/Lps66EEHjy5Anc3d1hZJT/1JMlJlkqa1JSUmBra1vooDNiXWmDdaUd1pd8rCvtsL7kKw51VWJm8CYiIiIyBCZLRERERAVgslRMmZubY/bs2VqNnSqrWFfysa60w/qSj3WlHdaXfMWhrjhmiYiIiKgAbFkiIiIiKgCTJSIiIqICMFkiIiIiKgCTJSIiIqICMFkqhlauXAlPT09YWFjAz88Pp0+fNnRIBjdnzhwoFAqNW82aNdXr09PTMWrUKFSoUAHly5dHz549ER8fb8CIi9bx48fRpUsXuLu7Q6FQYOfOnRrrhRCYNWsW3NzcYGlpCX9/f1y/fl1jm8TERAwYMAA2Njaws7PD0KFDkZqaWoRHUTQKq6vBgwfn+qx16NBBY5uyUlfz5s3DG2+8AWtrazg7O6Nbt26IjIzU2EbOdy8mJgadO3eGlZUVnJ2dMXHiRGRlZRXloRQJOfXVpk2bXJ+vjz76SGObslBfq1evRv369WFjYwMbGxs0a9YMe/fuVa8vbp8rJkvFzJYtWzBu3DjMnj0b58+fR4MGDRAUFISEhARDh2ZwderUQWxsrPr2999/q9eNHTsWf/75J7Zu3Ypjx47h/v376NGjhwGjLVpPnz5FgwYNsHLlyjzXL1y4EMuWLcOaNWtw6tQplCtXDkFBQUhPT1dvM2DAAFy+fBmhoaHYtWsXjh8/jhEjRhTVIRSZwuoKADp06KDxWfv111811peVujp27BhGjRqFf/75B6GhoVAqlQgMDMTTp0/V2xT23cvOzkbnzp2RmZmJkydPYsOGDVi/fj1mzZpliEPSKzn1BQDDhw/X+HwtXLhQva6s1FelSpUwf/58nDt3DmfPnkW7du3QtWtXXL58GUAx/FwJKlaaNm0qRo0apX6enZ0t3N3dxbx58wwYleHNnj1bNGjQIM91SUlJwtTUVGzdulW9LCIiQgAQYWFhRRRh8QFA/Pbbb+rnOTk5wtXVVXz11VfqZUlJScLc3Fz8+uuvQgghrly5IgCIM2fOqLfZu3evUCgU4t69e0UWe1F7ua6EEGLQoEGia9eu+e5TVutKCCESEhIEAHHs2DEhhLzv3p49e4SRkZGIi4tTb7N69WphY2MjMjIyivYAitjL9SWEEK1btxaffvppvvuU5fqyt7cXP/zwQ7H8XLFlqRjJzMzEuXPn4O/vr15mZGQEf39/hIWFGTCy4uH69etwd3eHt7c3BgwYgJiYGADAuXPnoFQqNeqtZs2aqFy5MusNQHR0NOLi4jTqx9bWFn5+fur6CQsLg52dHZo0aaLext/fH0ZGRjh16lSRx2xoR48ehbOzM2rUqIGPP/4Yjx49Uq8ry3WVnJwMAHBwcAAg77sXFhaGevXqwcXFRb1NUFAQUlJS1K0IpdXL9aWyceNGODo6om7dupg6dSrS0tLU68pifWVnZ2Pz5s14+vQpmjVrViw/VyY6L5Fe2cOHD5Gdna3x5gOAi4sLrl69aqCoigc/Pz+sX78eNWrUQGxsLIKDg/HWW2/hv//+Q1xcHMzMzGBnZ6exj4uLC+Li4gwTcDGiqoO8PleqdXFxcXB2dtZYb2JiAgcHhzJXhx06dECPHj3g5eWFGzduYNq0aejYsSPCwsJgbGxcZusqJycHn332GVq0aIG6desCgKzvXlxcXJ6fPdW60iqv+gKA/v37o0qVKnB3d8fFixcxefJkREZGYseOHQDKVn1dunQJzZo1Q3p6OsqXL4/ffvsNtWvXRnh4eLH7XDFZohKhY8eO6sf169eHn58fqlSpgv/973+wtLQ0YGRU2vTt21f9uF69eqhfvz6qVq2Ko0ePon379gaMzLBGjRqF//77T2OsIOUvv/p6cWxbvXr14Obmhvbt2+PGjRuoWrVqUYdpUDVq1EB4eDiSk5Oxbds2DBo0CMeOHTN0WHliN1wx4ujoCGNj41wj/uPj4+Hq6mqgqIonOzs7VK9eHVFRUXB1dUVmZiaSkpI0tmG9SVR1UNDnytXVNddJBFlZWUhMTCzzdejt7Q1HR0dERUUBKJt1NXr0aOzatQtHjhxBpUqV1MvlfPdcXV3z/Oyp1pVG+dVXXvz8/ABA4/NVVurLzMwMPj4+8PX1xbx589CgQQN88803xfJzxWSpGDEzM4Ovry8OHTqkXpaTk4NDhw6hWbNmBoys+ElNTcWNGzfg5uYGX19fmJqaatRbZGQkYmJiWG8AvLy84OrqqlE/KSkpOHXqlLp+mjVrhqSkJJw7d069zeHDh5GTk6P+Y15W3b17F48ePYKbmxuAslVXQgiMHj0av/32Gw4fPgwvLy+N9XK+e82aNcOlS5c0EszQ0FDY2Nigdu3aRXMgRaSw+spLeHg4AGh8vspKfb0sJycHGRkZxfNzpfMh4/RaNm/eLMzNzcX69evFlStXxIgRI4SdnZ3GiP+yaPz48eLo0aMiOjpanDhxQvj7+wtHR0eRkJAghBDio48+EpUrVxaHDx8WZ8+eFc2aNRPNmjUzcNRF58mTJ+LChQviwoULAoBYvHixuHDhgrh9+7YQQoj58+cLOzs78fvvv4uLFy+Krl27Ci8vL/Hs2TN1GR06dBCNGjUSp06dEn///beoVq2a6Nevn6EOSW8KqqsnT56ICRMmiLCwMBEdHS0OHjwoGjduLKpVqybS09PVZZSVuvr444+Fra2tOHr0qIiNjVXf0tLS1NsU9t3LysoSdevWFYGBgSI8PFzs27dPODk5ialTpxrikPSqsPqKiooSISEh4uzZsyI6Olr8/vvvwtvbW7Rq1UpdRlmprylTpohjx46J6OhocfHiRTFlyhShUCjEgQMHhBDF73PFZKkYWr58uahcubIwMzMTTZs2Ff/884+hQzK4Pn36CDc3N2FmZiYqVqwo+vTpI6KiotTrnz17JkaOHCns7e2FlZWV6N69u4iNjTVgxEXryJEjAkCu26BBg4QQ0vQBM2fOFC4uLsLc3Fy0b99eREZGapTx6NEj0a9fP1G+fHlhY2MjhgwZIp48eWKAo9GvguoqLS1NBAYGCicnJ2FqaiqqVKkihg8fnuuflbJSV3nVEwCxbt069TZyvnu3bt0SHTt2FJaWlsLR0VGMHz9eKJXKIj4a/SusvmJiYkSrVq2Eg4ODMDc3Fz4+PmLixIkiOTlZo5yyUF8ffPCBqFKlijAzMxNOTk6iffv26kRJiOL3uVIIIYTu26uIiIiISgeOWSIiIiIqAJMlIiIiogIwWSIiIiIqAJMlIiIiogIwWSIiIiIqAJMlIiIiogIwWSIiIiIqAJMlIqJi5OjRo1AoFLmui0VEhsNkiYiIiKgATJaIiIiICsBkiYgMok2bNhgzZgwmTZoEBwcHuLq6Ys6cOQCAW7duQaFQqK/IDgBJSUlQKBQ4evQogOfdVfv370ejRo1gaWmJdu3aISEhAXv37kWtWrVgY2OD/v37Iy0tTVZMOTk5mDdvHry8vGBpaYkGDRpg27Zt6vWq19y9ezfq168PCwsLvPnmm/jvv/80ytm+fTvq1KkDc3NzeHp6YtGiRRrrMzIyMHnyZHh4eMDc3Bw+Pj748ccfNbY5d+4cmjRpAisrKzRv3hyRkZHqdf/++y/atm0La2tr2NjYwNfXF2fPnpV1jESkPSZLRGQwGzZsQLly5XDq1CksXLgQISEhCA0N1aqMOXPmYMWKFTh58iTu3LmD3r17Y+nSpdi0aRN2796NAwcOYPny5bLKmjdvHn766SesWbMGly9fxtixY/Hee+/h2LFjGttNnDgRixYtwpkzZ+Dk5IQuXbpAqVQCkJKc3r17o2/fvrh06RLmzJmDmTNnYv369er9Bw4ciF9//RXLli1DREQEvv32W5QvX17jNaZPn45Fixbh7NmzMDExwQcffKBeN2DAAFSqVAlnzpzBuXPnMGXKFJiammpVb0SkBb1cnpeIqBCtW7cWLVu21Fj2xhtviMmTJ4vo6GgBQFy4cEG97vHjxwKAOHLkiBBCiCNHjggA4uDBg+pt5s2bJwCIGzduqJd9+OGHIigoqNB40tPThZWVlTh58qTG8qFDh4p+/fppvObmzZvV6x89eiQsLS3Fli1bhBBC9O/fXwQEBGiUMXHiRFG7dm0hhBCRkZECgAgNDc0zjryOa/fu3QKAePbsmRBCCGtra7F+/fpCj4mIdIMtS0RkMPXr19d47ubmhoSEhFcuw8XFBVZWVvD29tZYJqfMqKgopKWlISAgAOXLl1fffvrpJ9y4cUNj22bNmqkfOzg4oEaNGoiIiAAAREREoEWLFhrbt2jRAtevX0d2djbCw8NhbGyM1q1byz4uNzc3AFAfx7hx4zBs2DD4+/tj/vz5ueIjIt0yMXQARFR2vdx1pFAokJOTAyMj6f84IYR6naqbq6AyFApFvmUWJjU1FQCwe/duVKxYUWOdubl5ofvLZWlpKWu7l48LgPo45syZg/79+2P37t3Yu3cvZs+ejc2bN6N79+46i5OInmPLEhEVO05OTgCA2NhY9bIXB3vrQ+3atWFubo6YmBj4+Pho3Dw8PDS2/eeff9SPHz9+jGvXrqFWrVoAgFq1auHEiRMa2584cQLVq1eHsbEx6tWrh5ycnFzjoLRVvXp1jB07FgcOHECPHj2wbt261yqPiPLHliUiKnYsLS3x5ptvYv78+fDy8kJCQgJmzJih19e0trbGhAkTMHbsWOTk5KBly5ZITk7GiRMnYGNjg0GDBqm3DQkJQYUKFeDi4oLp06fD0dER3bp1AwCMHz8eb7zxBubOnYs+ffogLCwMK1aswKpVqwAAnp6eGDRoED744AMsW7YMDRo0wO3bt5GQkIDevXsXGuezZ88wceJE9OrVC15eXrh79y7OnDmDnj176qVeiIjJEhEVU2vXrsXQoUPh6+uLGjVqYOHChQgMDNTra86dOxdOTk6YN28ebt68CTs7OzRu3BjTpk3T2G7+/Pn49NNPcf36dTRs2BB//vknzMzMAACNGzfG//73P8yaNQtz586Fm5sbQkJCMHjwYPX+q1evxrRp0zBy5Eg8evQIlStXzvUa+TE2NsajR48wcOBAxMfHw9HRET169EBwcLDO6oGINCnEi4MCiIgoX0ePHkXbtm3x+PFj2NnZGTocIioiHLNEREREVAAmS0RUJsTExGhMCfDyLSYmxtAhElExxW44IioTsrKycOvWrXzXe3p6wsSEwziJKDcmS0REREQFYDccERERUQGYLBEREREVgMkSERERUQGYLBEREREVgMkSERERUQGYLBEREREVgMkSERERUQGYLBEREREV4P8AjSNrONVOO8EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.6177777531086619\n",
      "Corresponding RMSE: 0.2377393722518685\n",
      "Corresponding num_epochs: 112\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
