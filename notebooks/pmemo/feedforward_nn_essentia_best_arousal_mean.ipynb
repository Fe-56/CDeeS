{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Arousal Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.average_loudness</th>\n",
       "      <th>lowlevel.barkbands_spread.mean</th>\n",
       "      <th>lowlevel.melbands_crest.mean</th>\n",
       "      <th>lowlevel.melbands_flatness_db.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.melbands_spread.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.spectral_entropy.mean</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.984985</td>\n",
       "      <td>0.579073</td>\n",
       "      <td>0.466983</td>\n",
       "      <td>0.314570</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.548723</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.763577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.952336</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.811687</td>\n",
       "      <td>0.690897</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.944813</td>\n",
       "      <td>0.188384</td>\n",
       "      <td>0.658175</td>\n",
       "      <td>0.666715</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.615791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.355408</td>\n",
       "      <td>0.721706</td>\n",
       "      <td>0.523877</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.710481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.985253</td>\n",
       "      <td>0.434349</td>\n",
       "      <td>0.197786</td>\n",
       "      <td>0.252470</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.451415</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.764426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.914366</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.288119</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.460787</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.768472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.937592</td>\n",
       "      <td>0.571015</td>\n",
       "      <td>0.235807</td>\n",
       "      <td>0.188595</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.547003</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.943027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.927177</td>\n",
       "      <td>0.793285</td>\n",
       "      <td>0.353024</td>\n",
       "      <td>0.220160</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.623593</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.969786</td>\n",
       "      <td>0.915269</td>\n",
       "      <td>0.395830</td>\n",
       "      <td>0.204894</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.586678</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.975343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241478</td>\n",
       "      <td>0.136039</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.788438</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.975293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.333946</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.average_loudness  lowlevel.barkbands_spread.mean  \\\n",
       "0          1                   0.984985                        0.579073   \n",
       "1          4                   0.952336                        0.071979   \n",
       "2          5                   0.944813                        0.188384   \n",
       "3          6                   0.968798                        0.355408   \n",
       "4          7                   0.985253                        0.434349   \n",
       "..       ...                        ...                             ...   \n",
       "762      993                   0.914366                        0.429802   \n",
       "763      996                   0.937592                        0.571015   \n",
       "764      997                   0.927177                        0.793285   \n",
       "765      999                   0.969786                        0.915269   \n",
       "766     1000                   0.975343                        1.000000   \n",
       "\n",
       "     lowlevel.melbands_crest.mean  lowlevel.melbands_flatness_db.mean  \\\n",
       "0                        0.466983                            0.314570   \n",
       "1                        0.811687                            0.690897   \n",
       "2                        0.658175                            0.666715   \n",
       "3                        0.721706                            0.523877   \n",
       "4                        0.197786                            0.252470   \n",
       "..                            ...                                 ...   \n",
       "762                      0.301846                            0.288119   \n",
       "763                      0.235807                            0.188595   \n",
       "764                      0.353024                            0.220160   \n",
       "765                      0.395830                            0.204894   \n",
       "766                      0.241478                            0.136039   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           0.064362                         0.214289   \n",
       "1                           0.345452                         0.539877   \n",
       "2                           0.566046                         0.636537   \n",
       "3                           0.168166                         0.426521   \n",
       "4                           0.050890                         0.193447   \n",
       "..                               ...                              ...   \n",
       "762                         0.058467                         0.186511   \n",
       "763                         0.121857                         0.234562   \n",
       "764                         0.046206                         0.200557   \n",
       "765                         0.063658                         0.238118   \n",
       "766                         0.028755                         0.176146   \n",
       "\n",
       "     lowlevel.melbands_spread.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                         0.548723                       0.125964   \n",
       "1                         0.075821                       0.949078   \n",
       "2                         0.194606                       0.525953   \n",
       "3                         0.216854                       0.278434   \n",
       "4                         0.451415                       0.201743   \n",
       "..                             ...                            ...   \n",
       "762                       0.460787                       0.184759   \n",
       "763                       0.547003                       0.179638   \n",
       "764                       0.623593                       0.195031   \n",
       "765                       0.586678                       0.238011   \n",
       "766                       0.788438                       0.162377   \n",
       "\n",
       "     lowlevel.spectral_entropy.mean  ...  tonal.chords_histogram_14  \\\n",
       "0                          0.763577  ...                   0.000000   \n",
       "1                          0.593323  ...                   0.016972   \n",
       "2                          0.615791  ...                   0.000000   \n",
       "3                          0.710481  ...                   0.000000   \n",
       "4                          0.764426  ...                   0.000000   \n",
       "..                              ...  ...                        ...   \n",
       "762                        0.768472  ...                   0.000000   \n",
       "763                        0.943027  ...                   0.000000   \n",
       "764                        0.925110  ...                   0.008208   \n",
       "765                        1.000000  ...                   0.000000   \n",
       "766                        0.975293  ...                   0.155668   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.077021                   0.402838   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.063161                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.025688                   0.000000   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.333946                   0.483745   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.000000                   0.030457   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.031499                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.053602                   0.000000   \n",
       "765                   0.000000                   0.005471   \n",
       "766                   0.073610                   0.174831   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.089160                   0.000000   \n",
       "1                     0.117746                   0.000000   \n",
       "2                     0.008505                   0.007329   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.027702                   0.000000   \n",
       "765                   0.000000                   0.051231   \n",
       "766                   0.019717                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     0.223899                   0.098657   \n",
       "1                     0.107674                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.051213                   0.000000   \n",
       "4                     0.142857                   0.165104   \n",
       "..                         ...                        ...   \n",
       "762                   0.013240                   0.156009   \n",
       "763                   0.000000                   0.044460   \n",
       "764                   0.149709                   0.035320   \n",
       "765                   0.000000                   0.112450   \n",
       "766                   0.000000                   0.000761   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.288181  \n",
       "1                     0.155779  \n",
       "2                     0.000000  \n",
       "3                     0.000000  \n",
       "4                     0.096976  \n",
       "..                         ...  \n",
       "762                   0.863862  \n",
       "763                   0.016906  \n",
       "764                   0.149458  \n",
       "765                   0.019859  \n",
       "766                   0.000000  \n",
       "\n",
       "[767 rows x 305 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_arousal_features_mean = pd.read_csv(get_pmemo_path('processed/features/normalised_essentia_best_arousal_features_mean.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_arousal_features_mean = df_essentia_best_arousal_features_mean[df_essentia_best_arousal_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_arousal_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 305 columns):\n",
      " #    Column                                   Dtype  \n",
      "---   ------                                   -----  \n",
      " 0    song_id                                  int64  \n",
      " 1    lowlevel.average_loudness                float64\n",
      " 2    lowlevel.barkbands_spread.mean           float64\n",
      " 3    lowlevel.melbands_crest.mean             float64\n",
      " 4    lowlevel.melbands_flatness_db.mean       float64\n",
      " 5    lowlevel.melbands_kurtosis.mean          float64\n",
      " 6    lowlevel.melbands_skewness.mean          float64\n",
      " 7    lowlevel.melbands_spread.mean            float64\n",
      " 8    lowlevel.spectral_energy.mean            float64\n",
      " 9    lowlevel.spectral_entropy.mean           float64\n",
      " 10   lowlevel.spectral_flux.mean              float64\n",
      " 11   lowlevel.spectral_kurtosis.mean          float64\n",
      " 12   lowlevel.spectral_rolloff.mean           float64\n",
      " 13   lowlevel.spectral_skewness.mean          float64\n",
      " 14   rhythm.bpm_histogram_first_peak_bpm      float64\n",
      " 15   rhythm.bpm_histogram_first_peak_weight   float64\n",
      " 16   rhythm.bpm_histogram_second_peak_bpm     float64\n",
      " 17   rhythm.bpm_histogram_second_peak_spread  float64\n",
      " 18   rhythm.bpm_histogram_second_peak_weight  float64\n",
      " 19   rhythm.danceability                      float64\n",
      " 20   rhythm.onset_rate                        float64\n",
      " 21   tonal.chords_strength.mean               float64\n",
      " 22   tonal.hpcp_entropy.mean                  float64\n",
      " 23   tonal.key_edma.strength                  float64\n",
      " 24   tonal.key_temperley.strength             float64\n",
      " 25   rhythm.beats_loudness_band_ratio.mean_0  float64\n",
      " 26   rhythm.beats_loudness_band_ratio.mean_1  float64\n",
      " 27   rhythm.beats_loudness_band_ratio.mean_2  float64\n",
      " 28   rhythm.beats_loudness_band_ratio.mean_3  float64\n",
      " 29   rhythm.beats_loudness_band_ratio.mean_4  float64\n",
      " 30   rhythm.beats_loudness_band_ratio.mean_5  float64\n",
      " 31   rhythm.bpm_histogram_0                   float64\n",
      " 32   rhythm.bpm_histogram_1                   float64\n",
      " 33   rhythm.bpm_histogram_2                   float64\n",
      " 34   rhythm.bpm_histogram_3                   float64\n",
      " 35   rhythm.bpm_histogram_4                   float64\n",
      " 36   rhythm.bpm_histogram_5                   float64\n",
      " 37   rhythm.bpm_histogram_6                   float64\n",
      " 38   rhythm.bpm_histogram_7                   float64\n",
      " 39   rhythm.bpm_histogram_8                   float64\n",
      " 40   rhythm.bpm_histogram_9                   float64\n",
      " 41   rhythm.bpm_histogram_10                  float64\n",
      " 42   rhythm.bpm_histogram_11                  float64\n",
      " 43   rhythm.bpm_histogram_12                  float64\n",
      " 44   rhythm.bpm_histogram_13                  float64\n",
      " 45   rhythm.bpm_histogram_14                  float64\n",
      " 46   rhythm.bpm_histogram_15                  float64\n",
      " 47   rhythm.bpm_histogram_16                  float64\n",
      " 48   rhythm.bpm_histogram_17                  float64\n",
      " 49   rhythm.bpm_histogram_18                  float64\n",
      " 50   rhythm.bpm_histogram_19                  float64\n",
      " 51   rhythm.bpm_histogram_20                  float64\n",
      " 52   rhythm.bpm_histogram_21                  float64\n",
      " 53   rhythm.bpm_histogram_22                  float64\n",
      " 54   rhythm.bpm_histogram_23                  float64\n",
      " 55   rhythm.bpm_histogram_24                  float64\n",
      " 56   rhythm.bpm_histogram_25                  float64\n",
      " 57   rhythm.bpm_histogram_26                  float64\n",
      " 58   rhythm.bpm_histogram_27                  float64\n",
      " 59   rhythm.bpm_histogram_28                  float64\n",
      " 60   rhythm.bpm_histogram_29                  float64\n",
      " 61   rhythm.bpm_histogram_30                  float64\n",
      " 62   rhythm.bpm_histogram_31                  float64\n",
      " 63   rhythm.bpm_histogram_32                  float64\n",
      " 64   rhythm.bpm_histogram_33                  float64\n",
      " 65   rhythm.bpm_histogram_34                  float64\n",
      " 66   rhythm.bpm_histogram_35                  float64\n",
      " 67   rhythm.bpm_histogram_36                  float64\n",
      " 68   rhythm.bpm_histogram_37                  float64\n",
      " 69   rhythm.bpm_histogram_38                  float64\n",
      " 70   rhythm.bpm_histogram_39                  float64\n",
      " 71   rhythm.bpm_histogram_40                  float64\n",
      " 72   rhythm.bpm_histogram_41                  float64\n",
      " 73   rhythm.bpm_histogram_42                  float64\n",
      " 74   rhythm.bpm_histogram_43                  float64\n",
      " 75   rhythm.bpm_histogram_44                  float64\n",
      " 76   rhythm.bpm_histogram_45                  float64\n",
      " 77   rhythm.bpm_histogram_46                  float64\n",
      " 78   rhythm.bpm_histogram_47                  float64\n",
      " 79   rhythm.bpm_histogram_48                  float64\n",
      " 80   rhythm.bpm_histogram_49                  float64\n",
      " 81   rhythm.bpm_histogram_50                  float64\n",
      " 82   rhythm.bpm_histogram_51                  float64\n",
      " 83   rhythm.bpm_histogram_52                  float64\n",
      " 84   rhythm.bpm_histogram_53                  float64\n",
      " 85   rhythm.bpm_histogram_54                  float64\n",
      " 86   rhythm.bpm_histogram_55                  float64\n",
      " 87   rhythm.bpm_histogram_56                  float64\n",
      " 88   rhythm.bpm_histogram_57                  float64\n",
      " 89   rhythm.bpm_histogram_58                  float64\n",
      " 90   rhythm.bpm_histogram_59                  float64\n",
      " 91   rhythm.bpm_histogram_60                  float64\n",
      " 92   rhythm.bpm_histogram_61                  float64\n",
      " 93   rhythm.bpm_histogram_62                  float64\n",
      " 94   rhythm.bpm_histogram_63                  float64\n",
      " 95   rhythm.bpm_histogram_64                  float64\n",
      " 96   rhythm.bpm_histogram_65                  float64\n",
      " 97   rhythm.bpm_histogram_66                  float64\n",
      " 98   rhythm.bpm_histogram_67                  float64\n",
      " 99   rhythm.bpm_histogram_68                  float64\n",
      " 100  rhythm.bpm_histogram_69                  float64\n",
      " 101  rhythm.bpm_histogram_70                  float64\n",
      " 102  rhythm.bpm_histogram_71                  float64\n",
      " 103  rhythm.bpm_histogram_72                  float64\n",
      " 104  rhythm.bpm_histogram_73                  float64\n",
      " 105  rhythm.bpm_histogram_74                  float64\n",
      " 106  rhythm.bpm_histogram_75                  float64\n",
      " 107  rhythm.bpm_histogram_76                  float64\n",
      " 108  rhythm.bpm_histogram_77                  float64\n",
      " 109  rhythm.bpm_histogram_78                  float64\n",
      " 110  rhythm.bpm_histogram_79                  float64\n",
      " 111  rhythm.bpm_histogram_80                  float64\n",
      " 112  rhythm.bpm_histogram_81                  float64\n",
      " 113  rhythm.bpm_histogram_82                  float64\n",
      " 114  rhythm.bpm_histogram_83                  float64\n",
      " 115  rhythm.bpm_histogram_84                  float64\n",
      " 116  rhythm.bpm_histogram_85                  float64\n",
      " 117  rhythm.bpm_histogram_86                  float64\n",
      " 118  rhythm.bpm_histogram_87                  float64\n",
      " 119  rhythm.bpm_histogram_88                  float64\n",
      " 120  rhythm.bpm_histogram_89                  float64\n",
      " 121  rhythm.bpm_histogram_90                  float64\n",
      " 122  rhythm.bpm_histogram_91                  float64\n",
      " 123  rhythm.bpm_histogram_92                  float64\n",
      " 124  rhythm.bpm_histogram_93                  float64\n",
      " 125  rhythm.bpm_histogram_94                  float64\n",
      " 126  rhythm.bpm_histogram_95                  float64\n",
      " 127  rhythm.bpm_histogram_96                  float64\n",
      " 128  rhythm.bpm_histogram_97                  float64\n",
      " 129  rhythm.bpm_histogram_98                  float64\n",
      " 130  rhythm.bpm_histogram_99                  float64\n",
      " 131  rhythm.bpm_histogram_100                 float64\n",
      " 132  rhythm.bpm_histogram_101                 float64\n",
      " 133  rhythm.bpm_histogram_102                 float64\n",
      " 134  rhythm.bpm_histogram_103                 float64\n",
      " 135  rhythm.bpm_histogram_104                 float64\n",
      " 136  rhythm.bpm_histogram_105                 float64\n",
      " 137  rhythm.bpm_histogram_106                 float64\n",
      " 138  rhythm.bpm_histogram_107                 float64\n",
      " 139  rhythm.bpm_histogram_108                 float64\n",
      " 140  rhythm.bpm_histogram_109                 float64\n",
      " 141  rhythm.bpm_histogram_110                 float64\n",
      " 142  rhythm.bpm_histogram_111                 float64\n",
      " 143  rhythm.bpm_histogram_112                 float64\n",
      " 144  rhythm.bpm_histogram_113                 float64\n",
      " 145  rhythm.bpm_histogram_114                 float64\n",
      " 146  rhythm.bpm_histogram_115                 float64\n",
      " 147  rhythm.bpm_histogram_116                 float64\n",
      " 148  rhythm.bpm_histogram_117                 float64\n",
      " 149  rhythm.bpm_histogram_118                 float64\n",
      " 150  rhythm.bpm_histogram_119                 float64\n",
      " 151  rhythm.bpm_histogram_120                 float64\n",
      " 152  rhythm.bpm_histogram_121                 float64\n",
      " 153  rhythm.bpm_histogram_122                 float64\n",
      " 154  rhythm.bpm_histogram_123                 float64\n",
      " 155  rhythm.bpm_histogram_124                 float64\n",
      " 156  rhythm.bpm_histogram_125                 float64\n",
      " 157  rhythm.bpm_histogram_126                 float64\n",
      " 158  rhythm.bpm_histogram_127                 float64\n",
      " 159  rhythm.bpm_histogram_128                 float64\n",
      " 160  rhythm.bpm_histogram_129                 float64\n",
      " 161  rhythm.bpm_histogram_130                 float64\n",
      " 162  rhythm.bpm_histogram_131                 float64\n",
      " 163  rhythm.bpm_histogram_132                 float64\n",
      " 164  rhythm.bpm_histogram_133                 float64\n",
      " 165  rhythm.bpm_histogram_134                 float64\n",
      " 166  rhythm.bpm_histogram_135                 float64\n",
      " 167  rhythm.bpm_histogram_136                 float64\n",
      " 168  rhythm.bpm_histogram_137                 float64\n",
      " 169  rhythm.bpm_histogram_138                 float64\n",
      " 170  rhythm.bpm_histogram_139                 float64\n",
      " 171  rhythm.bpm_histogram_140                 float64\n",
      " 172  rhythm.bpm_histogram_141                 float64\n",
      " 173  rhythm.bpm_histogram_142                 float64\n",
      " 174  rhythm.bpm_histogram_143                 float64\n",
      " 175  rhythm.bpm_histogram_144                 float64\n",
      " 176  rhythm.bpm_histogram_145                 float64\n",
      " 177  rhythm.bpm_histogram_146                 float64\n",
      " 178  rhythm.bpm_histogram_147                 float64\n",
      " 179  rhythm.bpm_histogram_148                 float64\n",
      " 180  rhythm.bpm_histogram_149                 float64\n",
      " 181  rhythm.bpm_histogram_150                 float64\n",
      " 182  rhythm.bpm_histogram_151                 float64\n",
      " 183  rhythm.bpm_histogram_152                 float64\n",
      " 184  rhythm.bpm_histogram_153                 float64\n",
      " 185  rhythm.bpm_histogram_154                 float64\n",
      " 186  rhythm.bpm_histogram_155                 float64\n",
      " 187  rhythm.bpm_histogram_156                 float64\n",
      " 188  rhythm.bpm_histogram_157                 float64\n",
      " 189  rhythm.bpm_histogram_158                 float64\n",
      " 190  rhythm.bpm_histogram_159                 float64\n",
      " 191  rhythm.bpm_histogram_160                 float64\n",
      " 192  rhythm.bpm_histogram_161                 float64\n",
      " 193  rhythm.bpm_histogram_162                 float64\n",
      " 194  rhythm.bpm_histogram_163                 float64\n",
      " 195  rhythm.bpm_histogram_164                 float64\n",
      " 196  rhythm.bpm_histogram_165                 float64\n",
      " 197  rhythm.bpm_histogram_166                 float64\n",
      " 198  rhythm.bpm_histogram_167                 float64\n",
      " 199  rhythm.bpm_histogram_168                 float64\n",
      " 200  rhythm.bpm_histogram_169                 float64\n",
      " 201  rhythm.bpm_histogram_170                 float64\n",
      " 202  rhythm.bpm_histogram_171                 float64\n",
      " 203  rhythm.bpm_histogram_172                 float64\n",
      " 204  rhythm.bpm_histogram_173                 float64\n",
      " 205  rhythm.bpm_histogram_174                 float64\n",
      " 206  rhythm.bpm_histogram_175                 float64\n",
      " 207  rhythm.bpm_histogram_176                 float64\n",
      " 208  rhythm.bpm_histogram_177                 float64\n",
      " 209  rhythm.bpm_histogram_178                 float64\n",
      " 210  rhythm.bpm_histogram_179                 float64\n",
      " 211  rhythm.bpm_histogram_180                 float64\n",
      " 212  rhythm.bpm_histogram_181                 float64\n",
      " 213  rhythm.bpm_histogram_182                 float64\n",
      " 214  rhythm.bpm_histogram_183                 float64\n",
      " 215  rhythm.bpm_histogram_184                 float64\n",
      " 216  rhythm.bpm_histogram_185                 float64\n",
      " 217  rhythm.bpm_histogram_186                 float64\n",
      " 218  rhythm.bpm_histogram_187                 float64\n",
      " 219  rhythm.bpm_histogram_188                 float64\n",
      " 220  rhythm.bpm_histogram_189                 float64\n",
      " 221  rhythm.bpm_histogram_190                 float64\n",
      " 222  rhythm.bpm_histogram_191                 float64\n",
      " 223  rhythm.bpm_histogram_192                 float64\n",
      " 224  rhythm.bpm_histogram_193                 float64\n",
      " 225  rhythm.bpm_histogram_194                 float64\n",
      " 226  rhythm.bpm_histogram_195                 float64\n",
      " 227  rhythm.bpm_histogram_196                 float64\n",
      " 228  rhythm.bpm_histogram_197                 float64\n",
      " 229  rhythm.bpm_histogram_198                 float64\n",
      " 230  rhythm.bpm_histogram_199                 float64\n",
      " 231  rhythm.bpm_histogram_200                 float64\n",
      " 232  rhythm.bpm_histogram_201                 float64\n",
      " 233  rhythm.bpm_histogram_202                 float64\n",
      " 234  rhythm.bpm_histogram_203                 float64\n",
      " 235  rhythm.bpm_histogram_204                 float64\n",
      " 236  rhythm.bpm_histogram_205                 float64\n",
      " 237  rhythm.bpm_histogram_206                 float64\n",
      " 238  rhythm.bpm_histogram_207                 float64\n",
      " 239  rhythm.bpm_histogram_208                 float64\n",
      " 240  rhythm.bpm_histogram_209                 float64\n",
      " 241  rhythm.bpm_histogram_210                 float64\n",
      " 242  rhythm.bpm_histogram_211                 float64\n",
      " 243  rhythm.bpm_histogram_212                 float64\n",
      " 244  rhythm.bpm_histogram_213                 float64\n",
      " 245  rhythm.bpm_histogram_214                 float64\n",
      " 246  rhythm.bpm_histogram_215                 float64\n",
      " 247  rhythm.bpm_histogram_216                 float64\n",
      " 248  rhythm.bpm_histogram_217                 float64\n",
      " 249  rhythm.bpm_histogram_218                 float64\n",
      " 250  rhythm.bpm_histogram_219                 float64\n",
      " 251  rhythm.bpm_histogram_220                 float64\n",
      " 252  rhythm.bpm_histogram_221                 float64\n",
      " 253  rhythm.bpm_histogram_222                 float64\n",
      " 254  rhythm.bpm_histogram_223                 float64\n",
      " 255  rhythm.bpm_histogram_224                 float64\n",
      " 256  rhythm.bpm_histogram_225                 float64\n",
      " 257  rhythm.bpm_histogram_226                 float64\n",
      " 258  rhythm.bpm_histogram_227                 float64\n",
      " 259  rhythm.bpm_histogram_228                 float64\n",
      " 260  rhythm.bpm_histogram_229                 float64\n",
      " 261  rhythm.bpm_histogram_230                 float64\n",
      " 262  rhythm.bpm_histogram_231                 float64\n",
      " 263  rhythm.bpm_histogram_232                 float64\n",
      " 264  rhythm.bpm_histogram_233                 float64\n",
      " 265  rhythm.bpm_histogram_234                 float64\n",
      " 266  rhythm.bpm_histogram_235                 float64\n",
      " 267  rhythm.bpm_histogram_236                 float64\n",
      " 268  rhythm.bpm_histogram_237                 float64\n",
      " 269  rhythm.bpm_histogram_238                 float64\n",
      " 270  rhythm.bpm_histogram_239                 float64\n",
      " 271  rhythm.bpm_histogram_240                 float64\n",
      " 272  rhythm.bpm_histogram_241                 float64\n",
      " 273  rhythm.bpm_histogram_242                 float64\n",
      " 274  rhythm.bpm_histogram_243                 float64\n",
      " 275  rhythm.bpm_histogram_244                 float64\n",
      " 276  rhythm.bpm_histogram_245                 float64\n",
      " 277  rhythm.bpm_histogram_246                 float64\n",
      " 278  rhythm.bpm_histogram_247                 float64\n",
      " 279  rhythm.bpm_histogram_248                 float64\n",
      " 280  rhythm.bpm_histogram_249                 float64\n",
      " 281  tonal.chords_histogram_0                 float64\n",
      " 282  tonal.chords_histogram_1                 float64\n",
      " 283  tonal.chords_histogram_2                 float64\n",
      " 284  tonal.chords_histogram_3                 float64\n",
      " 285  tonal.chords_histogram_4                 float64\n",
      " 286  tonal.chords_histogram_5                 float64\n",
      " 287  tonal.chords_histogram_6                 float64\n",
      " 288  tonal.chords_histogram_7                 float64\n",
      " 289  tonal.chords_histogram_8                 float64\n",
      " 290  tonal.chords_histogram_9                 float64\n",
      " 291  tonal.chords_histogram_10                float64\n",
      " 292  tonal.chords_histogram_11                float64\n",
      " 293  tonal.chords_histogram_12                float64\n",
      " 294  tonal.chords_histogram_13                float64\n",
      " 295  tonal.chords_histogram_14                float64\n",
      " 296  tonal.chords_histogram_15                float64\n",
      " 297  tonal.chords_histogram_16                float64\n",
      " 298  tonal.chords_histogram_17                float64\n",
      " 299  tonal.chords_histogram_18                float64\n",
      " 300  tonal.chords_histogram_19                float64\n",
      " 301  tonal.chords_histogram_20                float64\n",
      " 302  tonal.chords_histogram_21                float64\n",
      " 303  tonal.chords_histogram_22                float64\n",
      " 304  tonal.chords_histogram_23                float64\n",
      "dtypes: float64(304), int64(1)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_arousal_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.average_loudness</th>\n",
       "      <th>lowlevel.barkbands_spread.mean</th>\n",
       "      <th>lowlevel.melbands_crest.mean</th>\n",
       "      <th>lowlevel.melbands_flatness_db.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.melbands_spread.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.spectral_entropy.mean</th>\n",
       "      <th>lowlevel.spectral_flux.mean</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984985</td>\n",
       "      <td>0.579073</td>\n",
       "      <td>0.466983</td>\n",
       "      <td>0.314570</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.548723</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.763577</td>\n",
       "      <td>0.485793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.952336</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.811687</td>\n",
       "      <td>0.690897</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>0.838254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944813</td>\n",
       "      <td>0.188384</td>\n",
       "      <td>0.658175</td>\n",
       "      <td>0.666715</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.615791</td>\n",
       "      <td>0.560126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.355408</td>\n",
       "      <td>0.721706</td>\n",
       "      <td>0.523877</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.710481</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.985253</td>\n",
       "      <td>0.434349</td>\n",
       "      <td>0.197786</td>\n",
       "      <td>0.252470</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.451415</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.764426</td>\n",
       "      <td>0.762925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.914366</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.288119</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.460787</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.768472</td>\n",
       "      <td>0.590104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.937592</td>\n",
       "      <td>0.571015</td>\n",
       "      <td>0.235807</td>\n",
       "      <td>0.188595</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.547003</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.943027</td>\n",
       "      <td>0.706776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.927177</td>\n",
       "      <td>0.793285</td>\n",
       "      <td>0.353024</td>\n",
       "      <td>0.220160</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.623593</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>0.551323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.969786</td>\n",
       "      <td>0.915269</td>\n",
       "      <td>0.395830</td>\n",
       "      <td>0.204894</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.586678</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.975343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241478</td>\n",
       "      <td>0.136039</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.788438</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.975293</td>\n",
       "      <td>0.605076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.average_loudness  lowlevel.barkbands_spread.mean  \\\n",
       "0                     0.984985                        0.579073   \n",
       "1                     0.952336                        0.071979   \n",
       "2                     0.944813                        0.188384   \n",
       "3                     0.968798                        0.355408   \n",
       "4                     0.985253                        0.434349   \n",
       "..                         ...                             ...   \n",
       "762                   0.914366                        0.429802   \n",
       "763                   0.937592                        0.571015   \n",
       "764                   0.927177                        0.793285   \n",
       "765                   0.969786                        0.915269   \n",
       "766                   0.975343                        1.000000   \n",
       "\n",
       "     lowlevel.melbands_crest.mean  lowlevel.melbands_flatness_db.mean  \\\n",
       "0                        0.466983                            0.314570   \n",
       "1                        0.811687                            0.690897   \n",
       "2                        0.658175                            0.666715   \n",
       "3                        0.721706                            0.523877   \n",
       "4                        0.197786                            0.252470   \n",
       "..                            ...                                 ...   \n",
       "762                      0.301846                            0.288119   \n",
       "763                      0.235807                            0.188595   \n",
       "764                      0.353024                            0.220160   \n",
       "765                      0.395830                            0.204894   \n",
       "766                      0.241478                            0.136039   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           0.064362                         0.214289   \n",
       "1                           0.345452                         0.539877   \n",
       "2                           0.566046                         0.636537   \n",
       "3                           0.168166                         0.426521   \n",
       "4                           0.050890                         0.193447   \n",
       "..                               ...                              ...   \n",
       "762                         0.058467                         0.186511   \n",
       "763                         0.121857                         0.234562   \n",
       "764                         0.046206                         0.200557   \n",
       "765                         0.063658                         0.238118   \n",
       "766                         0.028755                         0.176146   \n",
       "\n",
       "     lowlevel.melbands_spread.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                         0.548723                       0.125964   \n",
       "1                         0.075821                       0.949078   \n",
       "2                         0.194606                       0.525953   \n",
       "3                         0.216854                       0.278434   \n",
       "4                         0.451415                       0.201743   \n",
       "..                             ...                            ...   \n",
       "762                       0.460787                       0.184759   \n",
       "763                       0.547003                       0.179638   \n",
       "764                       0.623593                       0.195031   \n",
       "765                       0.586678                       0.238011   \n",
       "766                       0.788438                       0.162377   \n",
       "\n",
       "     lowlevel.spectral_entropy.mean  lowlevel.spectral_flux.mean  ...  \\\n",
       "0                          0.763577                     0.485793  ...   \n",
       "1                          0.593323                     0.838254  ...   \n",
       "2                          0.615791                     0.560126  ...   \n",
       "3                          0.710481                     0.337842  ...   \n",
       "4                          0.764426                     0.762925  ...   \n",
       "..                              ...                          ...  ...   \n",
       "762                        0.768472                     0.590104  ...   \n",
       "763                        0.943027                     0.706776  ...   \n",
       "764                        0.925110                     0.551323  ...   \n",
       "765                        1.000000                     0.661528  ...   \n",
       "766                        0.975293                     0.605076  ...   \n",
       "\n",
       "     tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.402838                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.031499   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.053602   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.483745                   0.073610   \n",
       "\n",
       "     tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                     0.000000                   0.089160   \n",
       "1                     0.030457                   0.117746   \n",
       "2                     0.000000                   0.008505   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.027702   \n",
       "765                   0.005471                   0.000000   \n",
       "766                   0.174831                   0.019717   \n",
       "\n",
       "     tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                     0.000000                   0.223899   \n",
       "1                     0.000000                   0.107674   \n",
       "2                     0.007329                   0.000000   \n",
       "3                     0.000000                   0.051213   \n",
       "4                     0.000000                   0.142857   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.013240   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.149709   \n",
       "765                   0.051231                   0.000000   \n",
       "766                   0.000000                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_22  tonal.chords_histogram_23  \\\n",
       "0                     0.098657                   0.288181   \n",
       "1                     0.000000                   0.155779   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.165104                   0.096976   \n",
       "..                         ...                        ...   \n",
       "762                   0.156009                   0.863862   \n",
       "763                   0.044460                   0.016906   \n",
       "764                   0.035320                   0.149458   \n",
       "765                   0.112450                   0.019859   \n",
       "766                   0.000761                   0.000000   \n",
       "\n",
       "     valence_mean_mapped  arousal_mean_mapped  \n",
       "0                  0.150               -0.200  \n",
       "1                 -0.425               -0.475  \n",
       "2                 -0.600               -0.700  \n",
       "3                 -0.300                0.025  \n",
       "4                  0.450                0.400  \n",
       "..                   ...                  ...  \n",
       "762                0.525                0.725  \n",
       "763                0.125                0.750  \n",
       "764                0.325                0.425  \n",
       "765                0.550                0.750  \n",
       "766                0.150                0.325  \n",
       "\n",
       "[767 rows x 306 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_arousal_features_mean_whole = pd.merge(df_essentia_best_arousal_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_arousal_features_mean_whole = df_essentia_best_arousal_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_arousal_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.average_loudness</th>\n",
       "      <th>lowlevel.barkbands_spread.mean</th>\n",
       "      <th>lowlevel.melbands_crest.mean</th>\n",
       "      <th>lowlevel.melbands_flatness_db.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.melbands_spread.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.spectral_entropy.mean</th>\n",
       "      <th>lowlevel.spectral_flux.mean</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984985</td>\n",
       "      <td>0.579073</td>\n",
       "      <td>0.466983</td>\n",
       "      <td>0.314570</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.548723</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.763577</td>\n",
       "      <td>0.485793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.952336</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.811687</td>\n",
       "      <td>0.690897</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>0.838254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944813</td>\n",
       "      <td>0.188384</td>\n",
       "      <td>0.658175</td>\n",
       "      <td>0.666715</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.194606</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.615791</td>\n",
       "      <td>0.560126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968798</td>\n",
       "      <td>0.355408</td>\n",
       "      <td>0.721706</td>\n",
       "      <td>0.523877</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.710481</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.985253</td>\n",
       "      <td>0.434349</td>\n",
       "      <td>0.197786</td>\n",
       "      <td>0.252470</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.451415</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.764426</td>\n",
       "      <td>0.762925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.914366</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.301846</td>\n",
       "      <td>0.288119</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.460787</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.768472</td>\n",
       "      <td>0.590104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.937592</td>\n",
       "      <td>0.571015</td>\n",
       "      <td>0.235807</td>\n",
       "      <td>0.188595</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.547003</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.943027</td>\n",
       "      <td>0.706776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.927177</td>\n",
       "      <td>0.793285</td>\n",
       "      <td>0.353024</td>\n",
       "      <td>0.220160</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.623593</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>0.551323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.969786</td>\n",
       "      <td>0.915269</td>\n",
       "      <td>0.395830</td>\n",
       "      <td>0.204894</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.586678</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.975343</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241478</td>\n",
       "      <td>0.136039</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.788438</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.975293</td>\n",
       "      <td>0.605076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.333946</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.average_loudness  lowlevel.barkbands_spread.mean  \\\n",
       "0                     0.984985                        0.579073   \n",
       "1                     0.952336                        0.071979   \n",
       "2                     0.944813                        0.188384   \n",
       "3                     0.968798                        0.355408   \n",
       "4                     0.985253                        0.434349   \n",
       "..                         ...                             ...   \n",
       "762                   0.914366                        0.429802   \n",
       "763                   0.937592                        0.571015   \n",
       "764                   0.927177                        0.793285   \n",
       "765                   0.969786                        0.915269   \n",
       "766                   0.975343                        1.000000   \n",
       "\n",
       "     lowlevel.melbands_crest.mean  lowlevel.melbands_flatness_db.mean  \\\n",
       "0                        0.466983                            0.314570   \n",
       "1                        0.811687                            0.690897   \n",
       "2                        0.658175                            0.666715   \n",
       "3                        0.721706                            0.523877   \n",
       "4                        0.197786                            0.252470   \n",
       "..                            ...                                 ...   \n",
       "762                      0.301846                            0.288119   \n",
       "763                      0.235807                            0.188595   \n",
       "764                      0.353024                            0.220160   \n",
       "765                      0.395830                            0.204894   \n",
       "766                      0.241478                            0.136039   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           0.064362                         0.214289   \n",
       "1                           0.345452                         0.539877   \n",
       "2                           0.566046                         0.636537   \n",
       "3                           0.168166                         0.426521   \n",
       "4                           0.050890                         0.193447   \n",
       "..                               ...                              ...   \n",
       "762                         0.058467                         0.186511   \n",
       "763                         0.121857                         0.234562   \n",
       "764                         0.046206                         0.200557   \n",
       "765                         0.063658                         0.238118   \n",
       "766                         0.028755                         0.176146   \n",
       "\n",
       "     lowlevel.melbands_spread.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                         0.548723                       0.125964   \n",
       "1                         0.075821                       0.949078   \n",
       "2                         0.194606                       0.525953   \n",
       "3                         0.216854                       0.278434   \n",
       "4                         0.451415                       0.201743   \n",
       "..                             ...                            ...   \n",
       "762                       0.460787                       0.184759   \n",
       "763                       0.547003                       0.179638   \n",
       "764                       0.623593                       0.195031   \n",
       "765                       0.586678                       0.238011   \n",
       "766                       0.788438                       0.162377   \n",
       "\n",
       "     lowlevel.spectral_entropy.mean  lowlevel.spectral_flux.mean  ...  \\\n",
       "0                          0.763577                     0.485793  ...   \n",
       "1                          0.593323                     0.838254  ...   \n",
       "2                          0.615791                     0.560126  ...   \n",
       "3                          0.710481                     0.337842  ...   \n",
       "4                          0.764426                     0.762925  ...   \n",
       "..                              ...                          ...  ...   \n",
       "762                        0.768472                     0.590104  ...   \n",
       "763                        0.943027                     0.706776  ...   \n",
       "764                        0.925110                     0.551323  ...   \n",
       "765                        1.000000                     0.661528  ...   \n",
       "766                        0.975293                     0.605076  ...   \n",
       "\n",
       "     tonal.chords_histogram_14  tonal.chords_histogram_15  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.016972                   0.077021   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.063161   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.008208                   0.025688   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.155668                   0.333946   \n",
       "\n",
       "     tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.402838                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.031499   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.053602   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.483745                   0.073610   \n",
       "\n",
       "     tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                     0.000000                   0.089160   \n",
       "1                     0.030457                   0.117746   \n",
       "2                     0.000000                   0.008505   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.027702   \n",
       "765                   0.005471                   0.000000   \n",
       "766                   0.174831                   0.019717   \n",
       "\n",
       "     tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                     0.000000                   0.223899   \n",
       "1                     0.000000                   0.107674   \n",
       "2                     0.007329                   0.000000   \n",
       "3                     0.000000                   0.051213   \n",
       "4                     0.000000                   0.142857   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.013240   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.149709   \n",
       "765                   0.051231                   0.000000   \n",
       "766                   0.000000                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_22  tonal.chords_histogram_23  \n",
       "0                     0.098657                   0.288181  \n",
       "1                     0.000000                   0.155779  \n",
       "2                     0.000000                   0.000000  \n",
       "3                     0.000000                   0.000000  \n",
       "4                     0.165104                   0.096976  \n",
       "..                         ...                        ...  \n",
       "762                   0.156009                   0.863862  \n",
       "763                   0.044460                   0.016906  \n",
       "764                   0.035320                   0.149458  \n",
       "765                   0.112450                   0.019859  \n",
       "766                   0.000761                   0.000000  \n",
       "\n",
       "[767 rows x 304 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_arousal_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117e33e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 304])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 304])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  # metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  # metric.update(test_pred, target_test_labels)\n",
    "  # adjusted_r2_score = metric.compute()\n",
    "  # print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.252418742693043\n",
      "Valence RMSE: 0.24661390906679054\n",
      "Arousal RMSE: 0.25809305139865746\n",
      "Test R^2 score: tensor([0.3501, 0.5087], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4294175562780569\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_arousal_mean_normalised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0398e-01,  3.6570e-01],\n",
       "        [-6.1817e-02, -7.7546e-02],\n",
       "        [ 4.7596e-01,  6.0040e-01],\n",
       "        [ 2.4847e-01,  2.9194e-01],\n",
       "        [ 4.3734e-01,  5.4794e-01],\n",
       "        [ 3.3659e-02,  1.8680e-02],\n",
       "        [-7.3906e-02, -7.2921e-02],\n",
       "        [ 4.8764e-01,  6.1592e-01],\n",
       "        [-7.4004e-02, -7.2969e-02],\n",
       "        [ 2.7884e-01,  3.3215e-01],\n",
       "        [ 3.7195e-01,  4.5641e-01],\n",
       "        [-7.4080e-02, -7.2996e-02],\n",
       "        [-7.3931e-02, -7.2935e-02],\n",
       "        [-7.3959e-02, -7.2952e-02],\n",
       "        [-1.7012e-02, -3.7133e-02],\n",
       "        [-6.0453e-02, -7.7333e-02],\n",
       "        [ 2.1707e-01,  2.5025e-01],\n",
       "        [ 1.6587e-01,  1.8232e-01],\n",
       "        [-7.3938e-02, -7.2941e-02],\n",
       "        [ 2.1323e-01,  2.4519e-01],\n",
       "        [ 1.9856e-01,  2.2571e-01],\n",
       "        [-7.2880e-02, -7.3565e-02],\n",
       "        [ 4.1069e-01,  5.0942e-01],\n",
       "        [-7.4076e-02, -7.2994e-02],\n",
       "        [-6.1620e-02, -7.7505e-02],\n",
       "        [ 3.1057e-01,  3.7446e-01],\n",
       "        [ 3.7201e-01,  4.5648e-01],\n",
       "        [-7.3960e-02, -7.2951e-02],\n",
       "        [ 8.6615e-03, -8.6690e-03],\n",
       "        [ 4.1035e-01,  5.0893e-01],\n",
       "        [-7.3984e-02, -7.2961e-02],\n",
       "        [ 4.1531e-01,  5.1610e-01],\n",
       "        [-2.7360e-04, -1.8416e-02],\n",
       "        [ 2.9243e-01,  3.5027e-01],\n",
       "        [ 3.1755e-01,  3.8377e-01],\n",
       "        [ 4.7329e-01,  5.9685e-01],\n",
       "        [ 4.1243e-01,  5.1194e-01],\n",
       "        [ 5.3081e-01,  6.7329e-01],\n",
       "        [ 2.5266e-01,  2.9745e-01],\n",
       "        [ 4.3488e-01,  5.4437e-01],\n",
       "        [ 5.9998e-01,  7.6522e-01],\n",
       "        [ 4.4814e-01,  5.6342e-01],\n",
       "        [ 4.3503e-01,  5.4458e-01],\n",
       "        [-7.3922e-02, -7.2924e-02],\n",
       "        [ 3.3311e-01,  4.0454e-01],\n",
       "        [ 4.4833e-01,  5.6369e-01],\n",
       "        [ 4.3553e-01,  5.4531e-01],\n",
       "        [ 1.2813e-01,  1.3204e-01],\n",
       "        [-7.3984e-02, -7.2961e-02],\n",
       "        [ 2.5589e-01,  3.0171e-01],\n",
       "        [ 5.1187e-02,  3.7879e-02],\n",
       "        [-7.3942e-02, -7.2945e-02],\n",
       "        [ 1.3889e-01,  1.4651e-01],\n",
       "        [ 2.7372e-01,  3.2537e-01],\n",
       "        [ 2.2158e-02,  6.0935e-03],\n",
       "        [ 5.0301e-01,  6.3634e-01],\n",
       "        [ 1.4928e-01,  1.6035e-01],\n",
       "        [ 4.1508e-01,  5.1577e-01],\n",
       "        [ 4.2288e-01,  5.2704e-01],\n",
       "        [ 4.6124e-01,  5.8084e-01],\n",
       "        [ 3.7147e-01,  4.5577e-01],\n",
       "        [ 3.6615e-01,  4.4865e-01],\n",
       "        [ 2.8165e-01,  3.3587e-01],\n",
       "        [ 4.4914e-01,  5.6475e-01],\n",
       "        [ 5.9182e-02,  4.6629e-02],\n",
       "        [-5.9750e-03, -2.4677e-02],\n",
       "        [-3.4498e-02, -5.5617e-02],\n",
       "        [ 2.8335e-01,  3.3812e-01],\n",
       "        [ 3.4404e-01,  4.1912e-01],\n",
       "        [ 3.7590e-01,  4.6168e-01],\n",
       "        [ 4.5437e-01,  5.7170e-01],\n",
       "        [-7.3949e-02, -7.2947e-02],\n",
       "        [ 4.2996e-01,  5.3727e-01],\n",
       "        [ 4.9713e-01,  6.2854e-01],\n",
       "        [ 4.6532e-02,  3.2783e-02],\n",
       "        [ 4.7675e-01,  6.0144e-01],\n",
       "        [-7.3933e-02, -7.2941e-02],\n",
       "        [ 3.8578e-01,  4.7488e-01],\n",
       "        [-6.5430e-02, -7.7056e-02],\n",
       "        [ 1.5710e-02, -9.8516e-04],\n",
       "        [ 3.1148e-01,  3.7570e-01],\n",
       "        [-7.3932e-02, -7.2941e-02],\n",
       "        [ 4.7599e-01,  6.0044e-01],\n",
       "        [ 5.4167e-01,  6.8772e-01],\n",
       "        [ 3.1168e-02,  1.5939e-02],\n",
       "        [ 2.7531e-01,  3.2748e-01],\n",
       "        [ 5.9273e-02,  4.6739e-02],\n",
       "        [-7.4011e-02, -7.2972e-02],\n",
       "        [ 5.4872e-01,  6.9709e-01],\n",
       "        [-7.4026e-02, -7.2977e-02],\n",
       "        [ 3.2989e-01,  4.0024e-01],\n",
       "        [ 3.7768e-01,  4.6406e-01],\n",
       "        [ 3.9985e-01,  4.9376e-01],\n",
       "        [-4.9011e-02, -7.1491e-02],\n",
       "        [ 5.4225e-02,  4.1182e-02],\n",
       "        [ 2.4132e-02,  8.2411e-03],\n",
       "        [ 3.9467e-01,  4.8676e-01],\n",
       "        [ 4.9101e-01,  6.2040e-01],\n",
       "        [ 4.2033e-01,  5.2336e-01],\n",
       "        [ 3.3109e-01,  4.0182e-01],\n",
       "        [ 1.6794e-02,  2.1090e-04],\n",
       "        [ 1.8113e-01,  2.0267e-01],\n",
       "        [ 1.2082e-01,  1.2234e-01],\n",
       "        [ 3.2151e-01,  3.8904e-01],\n",
       "        [-2.8605e-02, -4.9426e-02],\n",
       "        [ 2.9413e-01,  3.5250e-01],\n",
       "        [ 1.2312e-01,  1.2548e-01],\n",
       "        [-2.3465e-02, -4.4038e-02],\n",
       "        [ 4.3469e-01,  5.4410e-01],\n",
       "        [ 4.8864e-01,  6.1725e-01],\n",
       "        [ 3.3041e-01,  4.0095e-01],\n",
       "        [ 6.4935e-01,  8.3083e-01],\n",
       "        [-7.3982e-02, -7.2960e-02],\n",
       "        [ 5.7735e-01,  7.3513e-01],\n",
       "        [ 3.2011e-01,  3.8719e-01],\n",
       "        [ 4.9098e-01,  6.2036e-01],\n",
       "        [ 3.1229e-01,  3.7675e-01],\n",
       "        [ 3.5434e-01,  4.3289e-01],\n",
       "        [ 1.2333e-01,  1.2572e-01],\n",
       "        [ 1.5013e-01,  1.6142e-01],\n",
       "        [-7.3933e-02, -7.2941e-02],\n",
       "        [-7.4036e-02, -7.2981e-02],\n",
       "        [ 2.5093e-01,  2.9515e-01],\n",
       "        [ 1.6159e-01,  1.7667e-01],\n",
       "        [ 2.6999e-01,  3.2042e-01],\n",
       "        [ 3.3755e-01,  4.1046e-01],\n",
       "        [-7.3996e-02, -7.2966e-02],\n",
       "        [ 1.4090e-01,  1.4907e-01],\n",
       "        [-7.1710e-02, -7.4425e-02],\n",
       "        [ 2.5984e-01,  3.0693e-01],\n",
       "        [-7.3942e-02, -7.2943e-02],\n",
       "        [ 5.4910e-02,  4.1940e-02],\n",
       "        [ 4.6439e-01,  5.8502e-01],\n",
       "        [-2.6641e-02, -4.7313e-02],\n",
       "        [ 3.5403e-01,  4.3247e-01],\n",
       "        [-6.4980e-02, -7.7247e-02],\n",
       "        [ 2.7199e-02,  1.1608e-02],\n",
       "        [ 4.1534e-01,  5.1614e-01],\n",
       "        [-7.4002e-02, -7.2967e-02],\n",
       "        [ 4.3841e-01,  5.4949e-01],\n",
       "        [ 3.4137e-01,  4.1557e-01],\n",
       "        [-7.3976e-02, -7.2958e-02],\n",
       "        [ 3.4329e-01,  4.1811e-01],\n",
       "        [ 4.1921e-01,  5.2175e-01],\n",
       "        [ 2.9364e-01,  3.5186e-01],\n",
       "        [ 2.6332e-01,  3.1155e-01],\n",
       "        [ 4.3838e-01,  5.4943e-01],\n",
       "        [ 3.0760e-02,  1.5495e-02],\n",
       "        [-7.3829e-02, -7.2954e-02],\n",
       "        [-7.3861e-02, -7.2923e-02],\n",
       "        [ 4.0387e-01,  4.9957e-01],\n",
       "        [ 3.0667e-01,  3.6925e-01],\n",
       "        [ 1.8577e-01,  2.0876e-01],\n",
       "        [ 5.1106e-02,  3.7788e-02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3460, 0.5094], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4194313540891983\n",
      "Valence RMSE: 0.44852346763588913\n",
      "Arousal RMSE: 0.38816493989807266\n",
      "Test R^2 score: tensor([-1.1497, -0.1112], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.6304661444126418\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.41530068440101303\n",
      "Valence RMSE: 0.4426491777143541\n",
      "Arousal RMSE: 0.3860194585726292\n",
      "Test R^2 score: tensor([-1.0938, -0.0990], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5963706804764694\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4111275846004299\n",
      "Valence RMSE: 0.43657923563130435\n",
      "Arousal RMSE: 0.3839926466149493\n",
      "Test R^2 score: tensor([-1.0368, -0.0874], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5621009440115142\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4069812139378636\n",
      "Valence RMSE: 0.43040436602751764\n",
      "Arousal RMSE: 0.38212497785557636\n",
      "Test R^2 score: tensor([-0.9796, -0.0769], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5282209658337681\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4028674210141028\n",
      "Valence RMSE: 0.4242040903409521\n",
      "Arousal RMSE: 0.3803356511913049\n",
      "Test R^2 score: tensor([-0.9229, -0.0668], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4948787122141036\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3987866874317205\n",
      "Valence RMSE: 0.4179465205744236\n",
      "Arousal RMSE: 0.37865861945195767\n",
      "Test R^2 score: tensor([-0.8666, -0.0574], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.46202852126254446\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.39473530087472886\n",
      "Valence RMSE: 0.4116148625936826\n",
      "Arousal RMSE: 0.377100941931082\n",
      "Test R^2 score: tensor([-0.8105, -0.0488], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.42962348168771614\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3908094726748031\n",
      "Valence RMSE: 0.40544140960601893\n",
      "Arousal RMSE: 0.37560797547629315\n",
      "Test R^2 score: tensor([-0.7566, -0.0405], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3985292712048778\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3868758972401584\n",
      "Valence RMSE: 0.3990745093983506\n",
      "Arousal RMSE: 0.37427991621141116\n",
      "Test R^2 score: tensor([-0.7019, -0.0331], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3674886952105083\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.38297003740512414\n",
      "Valence RMSE: 0.3925913721926053\n",
      "Arousal RMSE: 0.3731006748587996\n",
      "Test R^2 score: tensor([-0.6470, -0.0266], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.33681600245261456\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3791023002496701\n",
      "Valence RMSE: 0.3860153367585379\n",
      "Arousal RMSE: 0.3720608389717664\n",
      "Test R^2 score: tensor([-0.5923, -0.0209], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.30660190928193254\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3753019624371632\n",
      "Valence RMSE: 0.37940470737868454\n",
      "Arousal RMSE: 0.3711538684121\n",
      "Test R^2 score: tensor([-0.5382, -0.0159], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2770812560038356\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37160682280345914\n",
      "Valence RMSE: 0.37283837052440916\n",
      "Arousal RMSE: 0.37037117999766983\n",
      "Test R^2 score: tensor([-0.4854, -0.0117], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.24854952425717658\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.368057580416992\n",
      "Valence RMSE: 0.36639798450956157\n",
      "Arousal RMSE: 0.36970972661285517\n",
      "Test R^2 score: tensor([-0.4346, -0.0080], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.22130661256683892\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3646154348954462\n",
      "Valence RMSE: 0.3599953397158022\n",
      "Arousal RMSE: 0.3691777161624127\n",
      "Test R^2 score: tensor([-0.3849, -0.0051], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.19500771421889807\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3612734207969214\n",
      "Valence RMSE: 0.3536248887996612\n",
      "Arousal RMSE: 0.3687633484635453\n",
      "Test R^2 score: tensor([-0.3363, -0.0029], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.16959050392137198\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35803582546718254\n",
      "Valence RMSE: 0.3472848635135799\n",
      "Arousal RMSE: 0.36847323947647515\n",
      "Test R^2 score: tensor([-0.2888, -0.0013], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.14505874482864323\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35500486532883074\n",
      "Valence RMSE: 0.3411942333614916\n",
      "Arousal RMSE: 0.3682979825292918\n",
      "Test R^2 score: tensor([-0.2440, -0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12217800976766746\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35208545399261\n",
      "Valence RMSE: 0.3351690203310918\n",
      "Arousal RMSE: 0.36822555809810475\n",
      "Test R^2 score: tensor([-2.0044e-01,  2.8875e-05], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10020739916935129\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34935973541448423\n",
      "Valence RMSE: 0.32939252521149603\n",
      "Arousal RMSE: 0.36824586052330693\n",
      "Test R^2 score: tensor([-1.5942e-01, -8.1397e-05], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07975168441040215\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34683767251950987\n",
      "Valence RMSE: 0.3239048700215162\n",
      "Arousal RMSE: 0.36834545922794576\n",
      "Test R^2 score: tensor([-0.1211, -0.0006], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06086722697215785\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3446262559378113\n",
      "Valence RMSE: 0.31892108083356047\n",
      "Arousal RMSE: 0.3685428832081571\n",
      "Test R^2 score: tensor([-0.0869, -0.0017], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.044286306313696344\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34279165578608267\n",
      "Valence RMSE: 0.3145960447880719\n",
      "Arousal RMSE: 0.3688381313758515\n",
      "Test R^2 score: tensor([-0.0576, -0.0033], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.030449410087209583\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34132093502924177\n",
      "Valence RMSE: 0.3109859035903136\n",
      "Arousal RMSE: 0.36917167977322957\n",
      "Test R^2 score: tensor([-0.0335, -0.0051], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.019290318099101866\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34025061053852595\n",
      "Valence RMSE: 0.3082394956131889\n",
      "Arousal RMSE: 0.36949880823597553\n",
      "Test R^2 score: tensor([-0.0153, -0.0069], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.011094835325814234\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3395997570852684\n",
      "Valence RMSE: 0.30647222827128384\n",
      "Arousal RMSE: 0.36977123106480403\n",
      "Test R^2 score: tensor([-0.0037, -0.0084], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.006033065457177522\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33935075058393044\n",
      "Valence RMSE: 0.3057259610641914\n",
      "Arousal RMSE: 0.36993175123950967\n",
      "Test R^2 score: tensor([ 0.0012, -0.0093], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004029891618993309\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33944403156000474\n",
      "Valence RMSE: 0.30594171938159953\n",
      "Arousal RMSE: 0.3699245402311796\n",
      "Test R^2 score: tensor([-0.0002, -0.0092], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004715345165541285\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3397515974337891\n",
      "Valence RMSE: 0.3068614828998416\n",
      "Arousal RMSE: 0.36972736743461815\n",
      "Test R^2 score: tensor([-0.0062, -0.0081], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00718905482008747\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3401178219421307\n",
      "Valence RMSE: 0.30811905476570084\n",
      "Arousal RMSE: 0.3693547261043358\n",
      "Test R^2 score: tensor([-0.0145, -0.0061], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.010305647296511955\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3404403588127963\n",
      "Valence RMSE: 0.30937106383721136\n",
      "Arousal RMSE: 0.36890218307486755\n",
      "Test R^2 score: tensor([-0.0228, -0.0036], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.013204366045443616\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3406334921780917\n",
      "Valence RMSE: 0.31030892026848444\n",
      "Arousal RMSE: 0.36847079394260196\n",
      "Test R^2 score: tensor([-0.0290, -0.0013], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.015136586294661858\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3406671842568106\n",
      "Valence RMSE: 0.31075960291584875\n",
      "Arousal RMSE: 0.36815313397350224\n",
      "Test R^2 score: tensor([-0.0320,  0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01576925861524664\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3406025979373144\n",
      "Valence RMSE: 0.3107734935454005\n",
      "Arousal RMSE: 0.36802186776451556\n",
      "Test R^2 score: tensor([-0.0321,  0.0011], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01545904800016984\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.340480184086361\n",
      "Valence RMSE: 0.31043022773373624\n",
      "Arousal RMSE: 0.36808502444427854\n",
      "Test R^2 score: tensor([-0.0298,  0.0008], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.014491151480212128\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34035167851640213\n",
      "Valence RMSE: 0.3098636598935252\n",
      "Arousal RMSE: 0.3683246426934457\n",
      "Test R^2 score: tensor([-0.0260, -0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.013264102340930894\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34022791792943974\n",
      "Valence RMSE: 0.30916917739303623\n",
      "Arousal RMSE: 0.3686793892087221\n",
      "Test R^2 score: tensor([-0.0214, -0.0024], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.011931202717494371\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34011291319328724\n",
      "Valence RMSE: 0.30843131712428545\n",
      "Arousal RMSE: 0.36908496319767287\n",
      "Test R^2 score: tensor([-0.0166, -0.0046], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.010599748928362018\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3400062570567056\n",
      "Valence RMSE: 0.307723256041463\n",
      "Arousal RMSE: 0.3694792380725355\n",
      "Test R^2 score: tensor([-0.0119, -0.0068], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.009342521314702146\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3398989293319618\n",
      "Valence RMSE: 0.307088228436885\n",
      "Arousal RMSE: 0.3698099299336396\n",
      "Test R^2 score: tensor([-0.0077, -0.0086], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.008158004128254737\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3397760443312569\n",
      "Valence RMSE: 0.30653947580037133\n",
      "Arousal RMSE: 0.37003928221045135\n",
      "Test R^2 score: tensor([-0.0041, -0.0098], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0069845753555163315\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3396298603032813\n",
      "Valence RMSE: 0.30608245445706983\n",
      "Arousal RMSE: 0.3701491795111649\n",
      "Test R^2 score: tensor([-0.0011, -0.0104], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.005788596109308264\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.339455163671121\n",
      "Valence RMSE: 0.30570931707179105\n",
      "Arousal RMSE: 0.37013704183920065\n",
      "Test R^2 score: tensor([ 0.0013, -0.0104], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0045357531975470655\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3392500313285835\n",
      "Valence RMSE: 0.3054039951541453\n",
      "Arousal RMSE: 0.3700129284995006\n",
      "Test R^2 score: tensor([ 0.0033, -0.0097], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0032000842136632457\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33901679493095305\n",
      "Valence RMSE: 0.30514961788775236\n",
      "Arousal RMSE: 0.36979519357810803\n",
      "Test R^2 score: tensor([ 0.0050, -0.0085], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001776273110383586\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3387578715591126\n",
      "Valence RMSE: 0.30492549372250516\n",
      "Arousal RMSE: 0.36950539152305606\n",
      "Test R^2 score: tensor([ 0.0064, -0.0069], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00025566732684834115\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33847596714629063\n",
      "Valence RMSE: 0.30471229940605027\n",
      "Arousal RMSE: 0.36916442848940795\n",
      "Test R^2 score: tensor([ 0.0078, -0.0051], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0013674928603161218\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3381728599839416\n",
      "Valence RMSE: 0.30449467615486747\n",
      "Arousal RMSE: 0.36878823008981654\n",
      "Test R^2 score: tensor([ 0.0092, -0.0030], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.003099558694370619\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3378515509760794\n",
      "Valence RMSE: 0.3042483340145741\n",
      "Arousal RMSE: 0.36840235102838437\n",
      "Test R^2 score: tensor([ 0.0108, -0.0009], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.004949751891653087\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33751082418422573\n",
      "Valence RMSE: 0.30396055129964666\n",
      "Arousal RMSE: 0.36801507596380556\n",
      "Test R^2 score: tensor([0.0127, 0.0012], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.006936600247969327\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33715459814351395\n",
      "Valence RMSE: 0.3036297729517059\n",
      "Arousal RMSE: 0.36763488283339524\n",
      "Test R^2 score: tensor([0.0148, 0.0032], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.009041769106425845\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3367889741236626\n",
      "Valence RMSE: 0.303260859603926\n",
      "Arousal RMSE: 0.3672689439835964\n",
      "Test R^2 score: tensor([0.0172, 0.0052], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.011229683907361232\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33641258597624957\n",
      "Valence RMSE: 0.30285201595472916\n",
      "Arousal RMSE: 0.36691621991758755\n",
      "Test R^2 score: tensor([0.0199, 0.0071], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01350863202015895\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3360047834865153\n",
      "Valence RMSE: 0.3023730015130654\n",
      "Arousal RMSE: 0.3665637693602844\n",
      "Test R^2 score: tensor([0.0230, 0.0090], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.016010895620388488\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33554063452784594\n",
      "Valence RMSE: 0.3017938378729163\n",
      "Arousal RMSE: 0.3661905436526657\n",
      "Test R^2 score: tensor([0.0267, 0.0111], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01888892975245482\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3349859908004895\n",
      "Valence RMSE: 0.30107376736557306\n",
      "Arousal RMSE: 0.3657674324888317\n",
      "Test R^2 score: tensor([0.0314, 0.0133], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.022350369302082784\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33430273617494966\n",
      "Valence RMSE: 0.30018149996460314\n",
      "Arousal RMSE: 0.36525019631359124\n",
      "Test R^2 score: tensor([0.0371, 0.0161], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.026611045349883233\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3334669313972866\n",
      "Valence RMSE: 0.2991273900422493\n",
      "Arousal RMSE: 0.36458633161098797\n",
      "Test R^2 score: tensor([0.0438, 0.0197], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03177304045373974\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33244255867859357\n",
      "Valence RMSE: 0.29791629246368145\n",
      "Arousal RMSE: 0.36370591461541346\n",
      "Test R^2 score: tensor([0.0516, 0.0244], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.038000852085728676\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3312131940774236\n",
      "Valence RMSE: 0.29653347402465796\n",
      "Arousal RMSE: 0.36259103497577777\n",
      "Test R^2 score: tensor([0.0604, 0.0304], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04537874903423189\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32970916184860716\n",
      "Valence RMSE: 0.2948947683132051\n",
      "Arousal RMSE: 0.3611832477224312\n",
      "Test R^2 score: tensor([0.0707, 0.0379], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0543142948045634\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.327920938171284\n",
      "Valence RMSE: 0.2928914278073476\n",
      "Arousal RMSE: 0.3595537441040537\n",
      "Test R^2 score: tensor([0.0833, 0.0466], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.06493658920456702\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3258265510571416\n",
      "Valence RMSE: 0.29072995307120525\n",
      "Arousal RMSE: 0.3574940239148047\n",
      "Test R^2 score: tensor([0.0968, 0.0575], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.07712277862497391\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3234496678083899\n",
      "Valence RMSE: 0.28857881041875794\n",
      "Arousal RMSE: 0.3549107569347682\n",
      "Test R^2 score: tensor([0.1101, 0.0710], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09056726411021698\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32069331663654604\n",
      "Valence RMSE: 0.2861864442278207\n",
      "Arousal RMSE: 0.35183195677898915\n",
      "Test R^2 score: tensor([0.1248, 0.0871], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.10593779039751466\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3175688097611964\n",
      "Valence RMSE: 0.28349335106068785\n",
      "Arousal RMSE: 0.3483265964158744\n",
      "Test R^2 score: tensor([0.1412, 0.1052], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.12318521429054907\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31405245111947483\n",
      "Valence RMSE: 0.28048496850768273\n",
      "Arousal RMSE: 0.34436327700488695\n",
      "Test R^2 score: tensor([0.1593, 0.1254], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14237388493244307\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3101393309524841\n",
      "Valence RMSE: 0.2770971160872225\n",
      "Arousal RMSE: 0.3399852900692224\n",
      "Test R^2 score: tensor([0.1795, 0.1475], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16351476002072468\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30606173645124196\n",
      "Valence RMSE: 0.2735378053297042\n",
      "Arousal RMSE: 0.335446928879231\n",
      "Test R^2 score: tensor([0.2004, 0.1701], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18528981091396945\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3019626556481544\n",
      "Valence RMSE: 0.27009658526666785\n",
      "Arousal RMSE: 0.3307729212608816\n",
      "Test R^2 score: tensor([0.2204, 0.1931], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20676778912156418\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29816149959255334\n",
      "Valence RMSE: 0.26729657785074484\n",
      "Arousal RMSE: 0.32611822878802876\n",
      "Test R^2 score: tensor([0.2365, 0.2157], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2260823403486461\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2948180661911771\n",
      "Valence RMSE: 0.26535001443891726\n",
      "Arousal RMSE: 0.3215971923737472\n",
      "Test R^2 score: tensor([0.2476, 0.2372], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2424203226397988\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29192777062766057\n",
      "Valence RMSE: 0.26388966781637635\n",
      "Arousal RMSE: 0.3174994326719884\n",
      "Test R^2 score: tensor([0.2559, 0.2566], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2562067830233115\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28909692922466\n",
      "Valence RMSE: 0.2619009435925553\n",
      "Arousal RMSE: 0.31394579901566655\n",
      "Test R^2 score: tensor([0.2670, 0.2731], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2700681173945716\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2862152604286552\n",
      "Valence RMSE: 0.25948479516584705\n",
      "Arousal RMSE: 0.3106541351442536\n",
      "Test R^2 score: tensor([0.2805, 0.2883], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28438027757924994\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2833709343534919\n",
      "Valence RMSE: 0.25725199843497054\n",
      "Arousal RMSE: 0.3072776955360672\n",
      "Test R^2 score: tensor([0.2928, 0.3037], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2982384373949573\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28043601315257766\n",
      "Valence RMSE: 0.25501379582965905\n",
      "Arousal RMSE: 0.3037378456537386\n",
      "Test R^2 score: tensor([0.3051, 0.3196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31234012436098935\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2774840599989523\n",
      "Valence RMSE: 0.252698093665923\n",
      "Arousal RMSE: 0.3002307122274682\n",
      "Test R^2 score: tensor([0.3176, 0.3352], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32643273680757884\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2748729075501112\n",
      "Valence RMSE: 0.2506174640553898\n",
      "Arousal RMSE: 0.2971550391976372\n",
      "Test R^2 score: tensor([0.3288, 0.3488], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33880322191179624\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27272005637997104\n",
      "Valence RMSE: 0.2491480309912787\n",
      "Arousal RMSE: 0.2944107962642641\n",
      "Test R^2 score: tensor([0.3367, 0.3608], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34871321649773357\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2709287123575823\n",
      "Valence RMSE: 0.24825865085751395\n",
      "Arousal RMSE: 0.29184306850408864\n",
      "Test R^2 score: tensor([0.3414, 0.3719], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35662777754475344\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26931214881335047\n",
      "Valence RMSE: 0.24773387202860728\n",
      "Arousal RMSE: 0.28928531875406693\n",
      "Test R^2 score: tensor([0.3442, 0.3828], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36349948336118443\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2678498143595868\n",
      "Valence RMSE: 0.24742681688224838\n",
      "Arousal RMSE: 0.28682227318053344\n",
      "Test R^2 score: tensor([0.3458, 0.3933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36954429653645265\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26663227213939833\n",
      "Valence RMSE: 0.24753844348138737\n",
      "Arousal RMSE: 0.28444728174346884\n",
      "Test R^2 score: tensor([0.3452, 0.4033], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3742521153596048\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2656555927176234\n",
      "Valence RMSE: 0.24799378420644472\n",
      "Arousal RMSE: 0.28221422869740453\n",
      "Test R^2 score: tensor([0.3428, 0.4126], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37771262724174415\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2648119925158396\n",
      "Valence RMSE: 0.24855161325194483\n",
      "Arousal RMSE: 0.28013010961029133\n",
      "Test R^2 score: tensor([0.3398, 0.4213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3805543856330389\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2638880978383978\n",
      "Valence RMSE: 0.24862985392934076\n",
      "Arousal RMSE: 0.27831107074029643\n",
      "Test R^2 score: tensor([0.3394, 0.4288], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3840923827223916\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26285259780173\n",
      "Valence RMSE: 0.24830357901198025\n",
      "Arousal RMSE: 0.2766375046736745\n",
      "Test R^2 score: tensor([0.3412, 0.4356], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3883833991076835\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26173979045363926\n",
      "Valence RMSE: 0.247733265984373\n",
      "Arousal RMSE: 0.2750339337938973\n",
      "Test R^2 score: tensor([0.3442, 0.4421], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39315701748176773\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26056331548523004\n",
      "Valence RMSE: 0.24701682441886935\n",
      "Arousal RMSE: 0.27343952020022366\n",
      "Test R^2 score: tensor([0.3480, 0.4486], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3982755667531484\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2594578880980534\n",
      "Valence RMSE: 0.24655172170978676\n",
      "Arousal RMSE: 0.27175179836487534\n",
      "Test R^2 score: tensor([0.3504, 0.4554], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40289506823582877\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2586457212576556\n",
      "Valence RMSE: 0.24671518685169155\n",
      "Arousal RMSE: 0.270049689550869\n",
      "Test R^2 score: tensor([0.3496, 0.4622], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4058648651541954\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2580627095864057\n",
      "Valence RMSE: 0.24709856605756753\n",
      "Arousal RMSE: 0.26857963960518133\n",
      "Test R^2 score: tensor([0.3475, 0.4680], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4077731326167335\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2576574720871804\n",
      "Valence RMSE: 0.24750039044898636\n",
      "Arousal RMSE: 0.26742906082232637\n",
      "Test R^2 score: tensor([0.3454, 0.4726], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4089854004474395\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2573031953586747\n",
      "Valence RMSE: 0.24805735915026836\n",
      "Arousal RMSE: 0.26622812634085713\n",
      "Test R^2 score: tensor([0.3425, 0.4773], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40987394079181716\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2567922225330593\n",
      "Valence RMSE: 0.24788519617534258\n",
      "Arousal RMSE: 0.2654004910018977\n",
      "Test R^2 score: tensor([0.3434, 0.4805], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4119526118401044\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25627283862814343\n",
      "Valence RMSE: 0.24754156396035493\n",
      "Arousal RMSE: 0.26471628160941363\n",
      "Test R^2 score: tensor([0.3452, 0.4832], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4141997152480031\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2558926338370054\n",
      "Valence RMSE: 0.2478185990199418\n",
      "Arousal RMSE: 0.2637195898750655\n",
      "Test R^2 score: tensor([0.3437, 0.4871], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4154086357898871\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2555687489156119\n",
      "Valence RMSE: 0.24841908662641451\n",
      "Arousal RMSE: 0.26252376700839886\n",
      "Test R^2 score: tensor([0.3405, 0.4917], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4161370163802503\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2548918020487659\n",
      "Valence RMSE: 0.24788683210544835\n",
      "Arousal RMSE: 0.26170934253874045\n",
      "Test R^2 score: tensor([0.3434, 0.4949], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4191227882927364\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25422351414253586\n",
      "Valence RMSE: 0.24766124961736075\n",
      "Arousal RMSE: 0.26062059727490383\n",
      "Test R^2 score: tensor([0.3446, 0.4991], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4218170759897294\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25370103149683043\n",
      "Valence RMSE: 0.24682709986007054\n",
      "Arousal RMSE: 0.26039356662517277\n",
      "Test R^2 score: tensor([0.3490, 0.4999], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4244571145726693\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25378206035532297\n",
      "Valence RMSE: 0.24779218962536018\n",
      "Arousal RMSE: 0.2596337787674307\n",
      "Test R^2 score: tensor([0.3439, 0.5029], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42336359105730126\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25370910545804354\n",
      "Valence RMSE: 0.24712006781966356\n",
      "Arousal RMSE: 0.2601312985118212\n",
      "Test R^2 score: tensor([0.3474, 0.5009], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4241873312369756\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2538265228942945\n",
      "Valence RMSE: 0.24866150375578386\n",
      "Arousal RMSE: 0.25888851654549666\n",
      "Test R^2 score: tensor([0.3393, 0.5057], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4224826674093946\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25301285079677965\n",
      "Valence RMSE: 0.2467398881872928\n",
      "Arousal RMSE: 0.2591340057073737\n",
      "Test R^2 score: tensor([0.3494, 0.5048], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4271001027027469\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2526703074962369\n",
      "Valence RMSE: 0.24672472968694897\n",
      "Arousal RMSE: 0.2584791603618903\n",
      "Test R^2 score: tensor([0.3495, 0.5073], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4283899646076351\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2526817335409376\n",
      "Valence RMSE: 0.24701927179655198\n",
      "Arousal RMSE: 0.2582200540074453\n",
      "Test R^2 score: tensor([0.3480, 0.5083], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4281066226052432\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25242890470958096\n",
      "Valence RMSE: 0.24617876856182289\n",
      "Arousal RMSE: 0.25852798257662907\n",
      "Test R^2 score: tensor([0.3524, 0.5071], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4297347177742974\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2527105666519095\n",
      "Valence RMSE: 0.24739514261223083\n",
      "Arousal RMSE: 0.25791646788627576\n",
      "Test R^2 score: tensor([0.3460, 0.5094], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4276915017962267\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.252418742693043\n",
      "Valence RMSE: 0.24661390906679054\n",
      "Arousal RMSE: 0.25809305139865746\n",
      "Test R^2 score: tensor([0.3501, 0.5087], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4294175562780569\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.252444473326757\n",
      "Valence RMSE: 0.2463216914728074\n",
      "Arousal RMSE: 0.2584222291839898\n",
      "Test R^2 score: tensor([0.3516, 0.5075], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4295602188098815\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2529672493506327\n",
      "Valence RMSE: 0.2474041840579108\n",
      "Arousal RMSE: 0.2584105806631952\n",
      "Test R^2 score: tensor([0.3459, 0.5075], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4267268351393501\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2529806071582337\n",
      "Valence RMSE: 0.24664702704700092\n",
      "Arousal RMSE: 0.2591594475321575\n",
      "Test R^2 score: tensor([0.3499, 0.5047], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42729627515233365\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2533612473476316\n",
      "Valence RMSE: 0.24766091793428874\n",
      "Arousal RMSE: 0.25893611768743463\n",
      "Test R^2 score: tensor([0.3446, 0.5055], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4250451719178377\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25353863585879116\n",
      "Valence RMSE: 0.24742837011533403\n",
      "Arousal RMSE: 0.25950507010141954\n",
      "Test R^2 score: tensor([0.3458, 0.5033], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42457263047346944\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2537095779427975\n",
      "Valence RMSE: 0.24695400557188524\n",
      "Arousal RMSE: 0.2602898749698616\n",
      "Test R^2 score: tensor([0.3483, 0.5003], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42432139255643087\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2544858130190608\n",
      "Valence RMSE: 0.24814671086808845\n",
      "Arousal RMSE: 0.26067080377613083\n",
      "Test R^2 score: tensor([0.3420, 0.4989], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42043453346366816\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.254403983890532\n",
      "Valence RMSE: 0.2477985861882995\n",
      "Arousal RMSE: 0.2608421643864888\n",
      "Test R^2 score: tensor([0.3438, 0.4982], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4210274661266782\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2549569723564493\n",
      "Valence RMSE: 0.24874071486719246\n",
      "Arousal RMSE: 0.26102523302104835\n",
      "Test R^2 score: tensor([0.3388, 0.4975], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4181757035097249\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.254889064820962\n",
      "Valence RMSE: 0.2483982136977507\n",
      "Arousal RMSE: 0.26121867881599836\n",
      "Test R^2 score: tensor([0.3407, 0.4968], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.418712926431855\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25476971841801976\n",
      "Valence RMSE: 0.24795782344500966\n",
      "Arousal RMSE: 0.2614041633907302\n",
      "Test R^2 score: tensor([0.3430, 0.4961], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4195233930750956\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2555375645422609\n",
      "Valence RMSE: 0.2490497773349885\n",
      "Arousal RMSE: 0.26186466388914137\n",
      "Test R^2 score: tensor([0.3372, 0.4943], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41573514880877566\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25530048354190443\n",
      "Valence RMSE: 0.24847890564270056\n",
      "Arousal RMSE: 0.2619444735894734\n",
      "Test R^2 score: tensor([0.3402, 0.4940], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.417098535418035\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25611377281813974\n",
      "Valence RMSE: 0.24972584629578157\n",
      "Arousal RMSE: 0.2623462043677002\n",
      "Test R^2 score: tensor([0.3336, 0.4924], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41300263214400185\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25602929374177275\n",
      "Valence RMSE: 0.24938254011307848\n",
      "Arousal RMSE: 0.26250780406412827\n",
      "Test R^2 score: tensor([0.3354, 0.4918], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4136053785999609\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.256241275904427\n",
      "Valence RMSE: 0.24955342973923883\n",
      "Arousal RMSE: 0.2627589554318562\n",
      "Test R^2 score: tensor([0.3345, 0.4908], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41266336198004383\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2568586337415296\n",
      "Valence RMSE: 0.25039922617412746\n",
      "Arousal RMSE: 0.2631595390376884\n",
      "Test R^2 score: tensor([0.3300, 0.4893], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4096271793513959\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25667241051459605\n",
      "Valence RMSE: 0.24993397764498707\n",
      "Arousal RMSE: 0.26323840802075543\n",
      "Test R^2 score: tensor([0.3325, 0.4890], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.410717824735612\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2574879187195008\n",
      "Valence RMSE: 0.25109059023106084\n",
      "Arousal RMSE: 0.2637301121798912\n",
      "Test R^2 score: tensor([0.3263, 0.4870], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4066661381813787\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2572595370183992\n",
      "Valence RMSE: 0.2505552862408983\n",
      "Arousal RMSE: 0.2637934557766431\n",
      "Test R^2 score: tensor([0.3292, 0.4868], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4079776884907553\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2579653733821985\n",
      "Valence RMSE: 0.25156194757151024\n",
      "Arousal RMSE: 0.2642136526799153\n",
      "Test R^2 score: tensor([0.3238, 0.4852], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40445888217181786\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2579538438815458\n",
      "Valence RMSE: 0.25142685003889315\n",
      "Arousal RMSE: 0.26431971214052463\n",
      "Test R^2 score: tensor([0.3245, 0.4847], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4046152464713935\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2582496375381428\n",
      "Valence RMSE: 0.25172395886320215\n",
      "Arousal RMSE: 0.2646144348129958\n",
      "Test R^2 score: tensor([0.3229, 0.4836], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4032416834954522\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25856559598849876\n",
      "Valence RMSE: 0.252010695857148\n",
      "Arousal RMSE: 0.26495838169676983\n",
      "Test R^2 score: tensor([0.3213, 0.4823], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4017982903418419\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2587338348354582\n",
      "Valence RMSE: 0.25197353515055015\n",
      "Arousal RMSE: 0.2653219405949286\n",
      "Test R^2 score: tensor([0.3215, 0.4808], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4011874537168859\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2597851550952452\n",
      "Valence RMSE: 0.25337467446521944\n",
      "Arousal RMSE: 0.2660412147682483\n",
      "Test R^2 score: tensor([0.3140, 0.4780], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39599494245600414\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25982814073346394\n",
      "Valence RMSE: 0.2529516082933297\n",
      "Arousal RMSE: 0.266527314352273\n",
      "Test R^2 score: tensor([0.3163, 0.4761], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39618484135765164\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26088870000348025\n",
      "Valence RMSE: 0.2545301610791634\n",
      "Arousal RMSE: 0.26709590914131764\n",
      "Test R^2 score: tensor([0.3077, 0.4739], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39078580787327444\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.260330184167073\n",
      "Valence RMSE: 0.25390182638148123\n",
      "Arousal RMSE: 0.26660358612943\n",
      "Test R^2 score: tensor([0.3111, 0.4758], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39346160200483277\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2605174644352527\n",
      "Valence RMSE: 0.2542078144952878\n",
      "Arousal RMSE: 0.26667786860006903\n",
      "Test R^2 score: tensor([0.3095, 0.4755], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3924848246656433\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26135432192289626\n",
      "Valence RMSE: 0.255173778033066\n",
      "Arousal RMSE: 0.2673920458425892\n",
      "Test R^2 score: tensor([0.3042, 0.4727], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38844935959185656\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2608439603882355\n",
      "Valence RMSE: 0.25456433214636076\n",
      "Arousal RMSE: 0.2669759242720512\n",
      "Test R^2 score: tensor([0.3075, 0.4743], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39092915534076444\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.261610001416214\n",
      "Valence RMSE: 0.25563710688919655\n",
      "Arousal RMSE: 0.2674495377885416\n",
      "Test R^2 score: tensor([0.3017, 0.4725], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38707143014933376\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26171963732705944\n",
      "Valence RMSE: 0.25597780533107073\n",
      "Arousal RMSE: 0.2673381759178878\n",
      "Test R^2 score: tensor([0.2998, 0.4729], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38635972113381933\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26132422450964315\n",
      "Valence RMSE: 0.2555376765517074\n",
      "Arousal RMSE: 0.2669853862923731\n",
      "Test R^2 score: tensor([0.3022, 0.4743], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38825770587711395\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.261818683541515\n",
      "Valence RMSE: 0.25609550101235373\n",
      "Arousal RMSE: 0.26741940928821056\n",
      "Test R^2 score: tensor([0.2992, 0.4726], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38587752097915323\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2611507090925153\n",
      "Valence RMSE: 0.25535330409436957\n",
      "Arousal RMSE: 0.266822180125895\n",
      "Test R^2 score: tensor([0.3032, 0.4749], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3890822400622407\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613115990079019\n",
      "Valence RMSE: 0.2557194623603958\n",
      "Arousal RMSE: 0.2667865441176582\n",
      "Test R^2 score: tensor([0.3012, 0.4751], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38815250820727537\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26222244164755215\n",
      "Valence RMSE: 0.2569239328275963\n",
      "Arousal RMSE: 0.2674159878309608\n",
      "Test R^2 score: tensor([0.2946, 0.4726], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3836134909686711\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26142612937363685\n",
      "Valence RMSE: 0.2560094907892627\n",
      "Arousal RMSE: 0.26673279300525726\n",
      "Test R^2 score: tensor([0.2996, 0.4753], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38746527066527364\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26145182657120264\n",
      "Valence RMSE: 0.2560560786129428\n",
      "Arousal RMSE: 0.26673844837255634\n",
      "Test R^2 score: tensor([0.2994, 0.4753], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3873266832412309\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26144417951746246\n",
      "Valence RMSE: 0.25614251961918505\n",
      "Arousal RMSE: 0.2666404463884909\n",
      "Test R^2 score: tensor([0.2989, 0.4757], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3872828753545341\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613848892895658\n",
      "Valence RMSE: 0.2562082068813315\n",
      "Arousal RMSE: 0.2664610204598227\n",
      "Test R^2 score: tensor([0.2985, 0.4764], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38745577321510316\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26206912118104847\n",
      "Valence RMSE: 0.2573424383217358\n",
      "Arousal RMSE: 0.2667120507060709\n",
      "Test R^2 score: tensor([0.2923, 0.4754], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3838500206534823\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2615726777988796\n",
      "Valence RMSE: 0.25680175910929964\n",
      "Arousal RMSE: 0.2662581229936887\n",
      "Test R^2 score: tensor([0.2953, 0.4772], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3862274134263754\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26151529591517125\n",
      "Valence RMSE: 0.2568779743270764\n",
      "Arousal RMSE: 0.26607180666281993\n",
      "Test R^2 score: tensor([0.2949, 0.4779], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3863839650771993\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26183453401282686\n",
      "Valence RMSE: 0.25759491780707877\n",
      "Arousal RMSE: 0.2660065877443502\n",
      "Test R^2 score: tensor([0.2909, 0.4782], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3845411767373676\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.261744660374606\n",
      "Valence RMSE: 0.2576852949132373\n",
      "Arousal RMSE: 0.2657420238778835\n",
      "Test R^2 score: tensor([0.2904, 0.4792], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3848111163791072\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2621099916567999\n",
      "Valence RMSE: 0.25840826633968844\n",
      "Arousal RMSE: 0.2657601613108558\n",
      "Test R^2 score: tensor([0.2864, 0.4791], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38278198895794613\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2617284776284519\n",
      "Valence RMSE: 0.2581795020887454\n",
      "Arousal RMSE: 0.26522996946917904\n",
      "Test R^2 score: tensor([0.2877, 0.4812], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3844515297177272\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2618029926695859\n",
      "Valence RMSE: 0.25872151710887886\n",
      "Arousal RMSE: 0.2648486181319106\n",
      "Test R^2 score: tensor([0.2847, 0.4827], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3837000052151652\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26166068815545146\n",
      "Valence RMSE: 0.258952376819037\n",
      "Arousal RMSE: 0.2643412529132377\n",
      "Test R^2 score: tensor([0.2834, 0.4847], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3840515253921939\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26140242889652565\n",
      "Valence RMSE: 0.2589176935045327\n",
      "Arousal RMSE: 0.26386376722903654\n",
      "Test R^2 score: tensor([0.2836, 0.4865], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38507751359698705\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613064172542986\n",
      "Valence RMSE: 0.2590585461344233\n",
      "Arousal RMSE: 0.26353511544246633\n",
      "Test R^2 score: tensor([0.2829, 0.4878], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3853268512906487\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2611696170659052\n",
      "Valence RMSE: 0.25926949429219176\n",
      "Arousal RMSE: 0.26305601511118165\n",
      "Test R^2 score: tensor([0.2817, 0.4897], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38567296146549124\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613552870099399\n",
      "Valence RMSE: 0.2599663897599057\n",
      "Arousal RMSE: 0.26273684228005956\n",
      "Test R^2 score: tensor([0.2778, 0.4909], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3843584135204751\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26149042967858444\n",
      "Valence RMSE: 0.2604648621902898\n",
      "Arousal RMSE: 0.2625119905664782\n",
      "Test R^2 score: tensor([0.2750, 0.4918], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3834078354240183\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613506643281391\n",
      "Valence RMSE: 0.2604591741354701\n",
      "Arousal RMSE: 0.2622391238892246\n",
      "Test R^2 score: tensor([0.2751, 0.4928], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38395166638906014\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2612834478607024\n",
      "Valence RMSE: 0.2603973776248175\n",
      "Arousal RMSE: 0.26216652337411617\n",
      "Test R^2 score: tensor([0.2754, 0.4931], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38426403187260877\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2610457512232293\n",
      "Valence RMSE: 0.2601509812447371\n",
      "Arousal RMSE: 0.2619374647140052\n",
      "Test R^2 score: tensor([0.2768, 0.4940], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3853920138196443\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2619280084262395\n",
      "Valence RMSE: 0.2615826444477652\n",
      "Arousal RMSE: 0.2622729176258702\n",
      "Test R^2 score: tensor([0.2688, 0.4927], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.380752651694316\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26140702023076473\n",
      "Valence RMSE: 0.2607155045901943\n",
      "Arousal RMSE: 0.26209671138367074\n",
      "Test R^2 score: tensor([0.2736, 0.4934], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3835132325332224\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26293201806235894\n",
      "Valence RMSE: 0.262717760359505\n",
      "Arousal RMSE: 0.2631461013132724\n",
      "Test R^2 score: tensor([0.2624, 0.4893], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3758810488508249\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26107863453546576\n",
      "Valence RMSE: 0.2604048256166515\n",
      "Arousal RMSE: 0.26175070891473795\n",
      "Test R^2 score: tensor([0.2754, 0.4947], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38504663289803637\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2614913307231809\n",
      "Valence RMSE: 0.26109208527964023\n",
      "Arousal RMSE: 0.26188996752661037\n",
      "Test R^2 score: tensor([0.2715, 0.4942], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3828627899579645\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2626156264238168\n",
      "Valence RMSE: 0.26236844795954306\n",
      "Arousal RMSE: 0.26286257245802025\n",
      "Test R^2 score: tensor([0.2644, 0.4904], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37741100035070974\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26123676253350586\n",
      "Valence RMSE: 0.2607998732158319\n",
      "Arousal RMSE: 0.26167292242144713\n",
      "Test R^2 score: tensor([0.2732, 0.4950], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38409664527889165\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26260247204428994\n",
      "Valence RMSE: 0.2625315343776174\n",
      "Arousal RMSE: 0.26267339055351246\n",
      "Test R^2 score: tensor([0.2635, 0.4911], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37732023524130615\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26226769598073246\n",
      "Valence RMSE: 0.2620253781509493\n",
      "Arousal RMSE: 0.2625097901315985\n",
      "Test R^2 score: tensor([0.2663, 0.4918], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3790556684015662\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26114980950241773\n",
      "Valence RMSE: 0.26073952811890505\n",
      "Arousal RMSE: 0.2615594473205448\n",
      "Test R^2 score: tensor([0.2735, 0.4955], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38448374142272435\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2628437901780367\n",
      "Valence RMSE: 0.2628236397403091\n",
      "Arousal RMSE: 0.2628639390710864\n",
      "Test R^2 score: tensor([0.2619, 0.4904], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37613104006997056\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2627308021924889\n",
      "Valence RMSE: 0.26291033236171674\n",
      "Arousal RMSE: 0.26255114926213724\n",
      "Test R^2 score: tensor([0.2614, 0.4916], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37649353832704874\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26127159600732724\n",
      "Valence RMSE: 0.2612030903268897\n",
      "Arousal RMSE: 0.2613400837302131\n",
      "Test R^2 score: tensor([0.2709, 0.4963], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3836139633306404\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2622962560576031\n",
      "Valence RMSE: 0.2622727006547038\n",
      "Arousal RMSE: 0.26231980934530946\n",
      "Test R^2 score: tensor([0.2649, 0.4925], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37873050907227734\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2621234813385568\n",
      "Valence RMSE: 0.26211703358718685\n",
      "Arousal RMSE: 0.26212992893132797\n",
      "Test R^2 score: tensor([0.2658, 0.4933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3795338674489276\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26134698623111313\n",
      "Valence RMSE: 0.2613030371373957\n",
      "Arousal RMSE: 0.261390927935428\n",
      "Test R^2 score: tensor([0.2704, 0.4961], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3832369322712526\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.263221877212072\n",
      "Valence RMSE: 0.2637508108809819\n",
      "Arousal RMSE: 0.2626918785301875\n",
      "Test R^2 score: tensor([0.2566, 0.4911], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3738559118382443\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26219033212353654\n",
      "Valence RMSE: 0.2625554160290994\n",
      "Arousal RMSE: 0.2618247391517603\n",
      "Test R^2 score: tensor([0.2634, 0.4944], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37889459085710214\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26138128262296223\n",
      "Valence RMSE: 0.2613627910832953\n",
      "Arousal RMSE: 0.26139977285452914\n",
      "Test R^2 score: tensor([0.2700, 0.4961], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38305301256524626\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2625144035681058\n",
      "Valence RMSE: 0.2624364874833857\n",
      "Arousal RMSE: 0.26259229653365573\n",
      "Test R^2 score: tensor([0.2640, 0.4915], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3777439035064939\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26181057432876265\n",
      "Valence RMSE: 0.2619946624880257\n",
      "Arousal RMSE: 0.26162635663957595\n",
      "Test R^2 score: tensor([0.2665, 0.4952], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.380849115450602\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2625465969523075\n",
      "Valence RMSE: 0.263003287910931\n",
      "Arousal RMSE: 0.26208911020970455\n",
      "Test R^2 score: tensor([0.2608, 0.4934], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3771261987809489\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26285506853703344\n",
      "Valence RMSE: 0.2631286499567119\n",
      "Arousal RMSE: 0.2625812020750444\n",
      "Test R^2 score: tensor([0.2601, 0.4915], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3758217360329801\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2617872499963296\n",
      "Valence RMSE: 0.2617221477853045\n",
      "Arousal RMSE: 0.26185233602152325\n",
      "Test R^2 score: tensor([0.2680, 0.4943], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38117545745366804\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2633723380989618\n",
      "Valence RMSE: 0.263220236426321\n",
      "Arousal RMSE: 0.26352435198117735\n",
      "Test R^2 score: tensor([0.2596, 0.4878], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37373445567944885\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26176412397734833\n",
      "Valence RMSE: 0.26172307119257404\n",
      "Arousal RMSE: 0.26180517032477413\n",
      "Test R^2 score: tensor([0.2680, 0.4945], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3812639507462969\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26391462357616313\n",
      "Valence RMSE: 0.26429709962522385\n",
      "Arousal RMSE: 0.263531592421839\n",
      "Test R^2 score: tensor([0.2536, 0.4878], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37068522568794815\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26257700528230427\n",
      "Valence RMSE: 0.26290880045299947\n",
      "Arousal RMSE: 0.2622447903208211\n",
      "Test R^2 score: tensor([0.2614, 0.4928], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3770907000676086\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2624223387547997\n",
      "Valence RMSE: 0.2627180236992717\n",
      "Arousal RMSE: 0.26212632027064087\n",
      "Test R^2 score: tensor([0.2624, 0.4933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37785555531855\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2634438795233447\n",
      "Valence RMSE: 0.26378391190923617\n",
      "Arousal RMSE: 0.2631034076831417\n",
      "Test R^2 score: tensor([0.2564, 0.4895], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3729647157992665\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2618537875293391\n",
      "Valence RMSE: 0.26223803830757525\n",
      "Arousal RMSE: 0.26146897206268127\n",
      "Test R^2 score: tensor([0.2651, 0.4958], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38047100717251786\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2637302402798619\n",
      "Valence RMSE: 0.26410080392871993\n",
      "Arousal RMSE: 0.26335915522409925\n",
      "Test R^2 score: tensor([0.2547, 0.4885], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.371574440177315\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26171891137290004\n",
      "Valence RMSE: 0.26198712347147773\n",
      "Arousal RMSE: 0.2614504241258226\n",
      "Test R^2 score: tensor([0.2665, 0.4959], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3812095666549771\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2622071611044083\n",
      "Valence RMSE: 0.26273349459993445\n",
      "Arousal RMSE: 0.2616797689623186\n",
      "Test R^2 score: tensor([0.2624, 0.4950], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37867464571940934\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26358229060063837\n",
      "Valence RMSE: 0.26432821372999976\n",
      "Arousal RMSE: 0.26283425055130527\n",
      "Test R^2 score: tensor([0.2534, 0.4905], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.371950860755668\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26162201935760815\n",
      "Valence RMSE: 0.26180648679309126\n",
      "Arousal RMSE: 0.26143742176391715\n",
      "Test R^2 score: tensor([0.2676, 0.4959], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3817401707919126\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26493990605231227\n",
      "Valence RMSE: 0.2649946487442256\n",
      "Arousal RMSE: 0.264885152046959\n",
      "Test R^2 score: tensor([0.2496, 0.4825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36607511187489805\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613755616542866\n",
      "Valence RMSE: 0.26124289823848057\n",
      "Arousal RMSE: 0.2615081577697843\n",
      "Test R^2 score: tensor([0.2707, 0.4957], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38317879723631415\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2641824282707588\n",
      "Valence RMSE: 0.26452372227715926\n",
      "Arousal RMSE: 0.26384069278014244\n",
      "Test R^2 score: tensor([0.2523, 0.4866], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36944380781585145\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2618395090517634\n",
      "Valence RMSE: 0.26212617268815075\n",
      "Arousal RMSE: 0.2615525312299592\n",
      "Test R^2 score: tensor([0.2658, 0.4955], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3806232630018372\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2616846877242942\n",
      "Valence RMSE: 0.2618150707595395\n",
      "Arousal RMSE: 0.2615542396939879\n",
      "Test R^2 score: tensor([0.2675, 0.4955], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38149086957509437\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2646596564558378\n",
      "Valence RMSE: 0.26472292010646475\n",
      "Arousal RMSE: 0.26459637767919353\n",
      "Test R^2 score: tensor([0.2511, 0.4837], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3674079964424631\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2616341574300386\n",
      "Valence RMSE: 0.26176177950906615\n",
      "Arousal RMSE: 0.2615064730680667\n",
      "Test R^2 score: tensor([0.2678, 0.4957], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3817320810267177\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26537100584999823\n",
      "Valence RMSE: 0.26585289339957285\n",
      "Arousal RMSE: 0.2648882416468405\n",
      "Test R^2 score: tensor([0.2447, 0.4825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36363482742364817\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26243261603963797\n",
      "Valence RMSE: 0.2626839217574118\n",
      "Arousal RMSE: 0.26218106944049563\n",
      "Test R^2 score: tensor([0.2626, 0.4931], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37784543626177197\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2633865962439003\n",
      "Valence RMSE: 0.26349032680885387\n",
      "Arousal RMSE: 0.2632828248102374\n",
      "Test R^2 score: tensor([0.2581, 0.4888], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37344355378100996\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26598996909851624\n",
      "Valence RMSE: 0.2659596204113262\n",
      "Arousal RMSE: 0.2660203143234037\n",
      "Test R^2 score: tensor([0.2441, 0.4781], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36111529119628283\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26268160382441047\n",
      "Valence RMSE: 0.262587559973943\n",
      "Arousal RMSE: 0.2627756140178536\n",
      "Test R^2 score: tensor([0.2632, 0.4908], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3769649776130982\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2679015292483124\n",
      "Valence RMSE: 0.26756395454734894\n",
      "Arousal RMSE: 0.2682386791164782\n",
      "Test R^2 score: tensor([0.2350, 0.4694], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3521716299104581\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2626454208758345\n",
      "Valence RMSE: 0.26218787739986127\n",
      "Arousal RMSE: 0.26310216866959457\n",
      "Test R^2 score: tensor([0.2654, 0.4895], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37745239189545066\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2689294475559181\n",
      "Valence RMSE: 0.26815717432462544\n",
      "Arousal RMSE: 0.2696995094242758\n",
      "Test R^2 score: tensor([0.2316, 0.4636], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34757587133813417\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26382677292483087\n",
      "Valence RMSE: 0.2636925671545785\n",
      "Arousal RMSE: 0.26396091046078896\n",
      "Test R^2 score: tensor([0.2570, 0.4861], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3715555627352273\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2643507466393332\n",
      "Valence RMSE: 0.26438552486886\n",
      "Arousal RMSE: 0.2643159638337482\n",
      "Test R^2 score: tensor([0.2531, 0.4848], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3689087253182913\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2693242567755121\n",
      "Valence RMSE: 0.26877545160815425\n",
      "Arousal RMSE: 0.26987194590804864\n",
      "Test R^2 score: tensor([0.2280, 0.4629], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34545905313359077\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2633447201013903\n",
      "Valence RMSE: 0.26328205851869557\n",
      "Arousal RMSE: 0.26340736677761567\n",
      "Test R^2 score: tensor([0.2593, 0.4883], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3737878523767927\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2657538261666096\n",
      "Valence RMSE: 0.2658437287758238\n",
      "Arousal RMSE: 0.26566389313369504\n",
      "Test R^2 score: tensor([0.2448, 0.4795], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3621433780923298\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2684802026844201\n",
      "Valence RMSE: 0.2683298361357589\n",
      "Arousal RMSE: 0.26863048506506365\n",
      "Test R^2 score: tensor([0.2306, 0.4678], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34920304585970596\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26459186312499494\n",
      "Valence RMSE: 0.2649783433278082\n",
      "Arousal RMSE: 0.2642048175775043\n",
      "Test R^2 score: tensor([0.2497, 0.4852], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3674486211668684\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.267861102801872\n",
      "Valence RMSE: 0.26793655109434317\n",
      "Arousal RMSE: 0.2677856332519325\n",
      "Test R^2 score: tensor([0.2329, 0.4711], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3520010479467677\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2684582479280662\n",
      "Valence RMSE: 0.2684126344857798\n",
      "Arousal RMSE: 0.2685038536215399\n",
      "Test R^2 score: tensor([0.2301, 0.4683], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.349216411453715\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2653926887538256\n",
      "Valence RMSE: 0.26577354175990275\n",
      "Arousal RMSE: 0.2650112884168656\n",
      "Test R^2 score: tensor([0.2452, 0.4820], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36361979098691005\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2709642725888739\n",
      "Valence RMSE: 0.27087294357930336\n",
      "Arousal RMSE: 0.27105557082620274\n",
      "Test R^2 score: tensor([0.2159, 0.4582], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3370503551826743\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26647208766674285\n",
      "Valence RMSE: 0.26701243991602064\n",
      "Arousal RMSE: 0.26593063746217377\n",
      "Test R^2 score: tensor([0.2381, 0.4784], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3582931231060504\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26927467047638765\n",
      "Valence RMSE: 0.2692211625786791\n",
      "Arousal RMSE: 0.2693281677435888\n",
      "Test R^2 score: tensor([0.2255, 0.4650], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3452590414895018\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2685142215234487\n",
      "Valence RMSE: 0.26875366344219065\n",
      "Arousal RMSE: 0.2682745658967489\n",
      "Test R^2 score: tensor([0.2282, 0.4692], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34869147656544663\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2670506818920677\n",
      "Valence RMSE: 0.2679132929414582\n",
      "Arousal RMSE: 0.2661852754441933\n",
      "Test R^2 score: tensor([0.2330, 0.4775], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3552187522385883\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.270981540868461\n",
      "Valence RMSE: 0.27135762666176594\n",
      "Arousal RMSE: 0.27060493239293404\n",
      "Test R^2 score: tensor([0.2131, 0.4600], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33654625313202596\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2658234218828698\n",
      "Valence RMSE: 0.2667770120130215\n",
      "Arousal RMSE: 0.26486639859453776\n",
      "Test R^2 score: tensor([0.2395, 0.4826], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36104760855756285\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2734116671743241\n",
      "Valence RMSE: 0.27205314893843713\n",
      "Arousal RMSE: 0.2747634685447962\n",
      "Test R^2 score: tensor([0.2091, 0.4432], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3261638877865618\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26458080114077653\n",
      "Valence RMSE: 0.2650335802668859\n",
      "Arousal RMSE: 0.2641272458409136\n",
      "Test R^2 score: tensor([0.2494, 0.4855], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36744332495376525\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2746917324215973\n",
      "Valence RMSE: 0.27298631910443777\n",
      "Arousal RMSE: 0.276386622873407\n",
      "Test R^2 score: tensor([0.2037, 0.4366], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32014754195725675\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2659608680712294\n",
      "Valence RMSE: 0.2671310090051652\n",
      "Arousal RMSE: 0.26478555609830035\n",
      "Test R^2 score: tensor([0.2375, 0.4829], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3601956662896216\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26366765522897767\n",
      "Valence RMSE: 0.2643856634322821\n",
      "Arousal RMSE: 0.2629476864309061\n",
      "Test R^2 score: tensor([0.2531, 0.4901], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3715686398368243\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27537246260033754\n",
      "Valence RMSE: 0.2729181940206436\n",
      "Arousal RMSE: 0.277805049791502\n",
      "Test R^2 score: tensor([0.2041, 0.4308], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3174475912443089\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26795524373561447\n",
      "Valence RMSE: 0.2690350721428681\n",
      "Arousal RMSE: 0.26687104610262025\n",
      "Test R^2 score: tensor([0.2266, 0.4748], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35065249698019313\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26400439170002393\n",
      "Valence RMSE: 0.2645879747135686\n",
      "Arousal RMSE: 0.2634195158122711\n",
      "Test R^2 score: tensor([0.2519, 0.4883], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37008104185834445\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2704086160314493\n",
      "Valence RMSE: 0.27019992489625944\n",
      "Arousal RMSE: 0.27061714623087596\n",
      "Test R^2 score: tensor([0.2198, 0.4599], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3398717274422683\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.274763785523136\n",
      "Valence RMSE: 0.27268404761565557\n",
      "Arousal RMSE: 0.2768278993272727\n",
      "Test R^2 score: tensor([0.2054, 0.4348], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3201286277954171\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2660440480477303\n",
      "Valence RMSE: 0.2671617531811428\n",
      "Arousal RMSE: 0.2649216273550051\n",
      "Test R^2 score: tensor([0.2373, 0.4824], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3598421144560875\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2665280614202997\n",
      "Valence RMSE: 0.2677304597061205\n",
      "Arousal RMSE: 0.2653202140706123\n",
      "Test R^2 score: tensor([0.2340, 0.4808], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35743745671483007\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27435772780619816\n",
      "Valence RMSE: 0.27370987456086343\n",
      "Arousal RMSE: 0.2750040548461718\n",
      "Test R^2 score: tensor([0.1994, 0.4423], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32084513862730213\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2719220125395963\n",
      "Valence RMSE: 0.27188235018590123\n",
      "Arousal RMSE: 0.27196166910901204\n",
      "Test R^2 score: tensor([0.2101, 0.4545], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3323088032090483\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26758280856670835\n",
      "Valence RMSE: 0.2681371044328522\n",
      "Arousal RMSE: 0.2670273620947132\n",
      "Test R^2 score: tensor([0.2317, 0.4741], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35292201181353894\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2696388725533989\n",
      "Valence RMSE: 0.27040880896452507\n",
      "Arousal RMSE: 0.26886673133380257\n",
      "Test R^2 score: tensor([0.2186, 0.4669], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34275053596480837\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2746533236017072\n",
      "Valence RMSE: 0.27426796104323387\n",
      "Arousal RMSE: 0.2750381462199658\n",
      "Test R^2 score: tensor([0.1962, 0.4421], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3191420053263823\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2700130069009639\n",
      "Valence RMSE: 0.2711964418962295\n",
      "Arousal RMSE: 0.26882436216649885\n",
      "Test R^2 score: tensor([0.2141, 0.4670], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34055529855677613\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26814866995828546\n",
      "Valence RMSE: 0.2692355620955748\n",
      "Arousal RMSE: 0.26705735433399863\n",
      "Test R^2 score: tensor([0.2254, 0.4740], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3497090804171906\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27293433558045355\n",
      "Valence RMSE: 0.27303173934753633\n",
      "Arousal RMSE: 0.27283689703987296\n",
      "Test R^2 score: tensor([0.2034, 0.4510], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32720411821038253\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2726002715649585\n",
      "Valence RMSE: 0.2728057740216151\n",
      "Arousal RMSE: 0.2723946140712875\n",
      "Test R^2 score: tensor([0.2047, 0.4528], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3287523471729959\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2678201090366072\n",
      "Valence RMSE: 0.26916071141493064\n",
      "Arousal RMSE: 0.26647276228419436\n",
      "Test R^2 score: tensor([0.2258, 0.4763], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3510745146506634\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2693145390993557\n",
      "Valence RMSE: 0.2707467569999318\n",
      "Arousal RMSE: 0.2678746638161525\n",
      "Test R^2 score: tensor([0.2167, 0.4708], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3437369202060576\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27295344751541145\n",
      "Valence RMSE: 0.2733763913024362\n",
      "Arousal RMSE: 0.2725298473553938\n",
      "Test R^2 score: tensor([0.2014, 0.4522], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.326815409395532\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26892530123465624\n",
      "Valence RMSE: 0.2704647627584261\n",
      "Arousal RMSE: 0.26737697618592643\n",
      "Test R^2 score: tensor([0.2183, 0.4728], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3455346596271886\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2679443395972718\n",
      "Valence RMSE: 0.269495707852165\n",
      "Arousal RMSE: 0.26638393662850024\n",
      "Test R^2 score: tensor([0.2239, 0.4767], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35028491694210406\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27213677056255886\n",
      "Valence RMSE: 0.2726175673332202\n",
      "Arousal RMSE: 0.27165512284099264\n",
      "Test R^2 score: tensor([0.2058, 0.4558], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33078436386199184\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2708669298830327\n",
      "Valence RMSE: 0.2718220525790964\n",
      "Arousal RMSE: 0.26990842732342846\n",
      "Test R^2 score: tensor([0.2104, 0.4627], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33658662199486183\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26850204044066744\n",
      "Valence RMSE: 0.2699898507729129\n",
      "Arousal RMSE: 0.2670059398613145\n",
      "Test R^2 score: tensor([0.2211, 0.4742], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3476371719296284\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27154098373712676\n",
      "Valence RMSE: 0.2723980358952959\n",
      "Arousal RMSE: 0.27068121792661964\n",
      "Test R^2 score: tensor([0.2071, 0.4596], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33337130952310823\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2704932076686441\n",
      "Valence RMSE: 0.27167381172423705\n",
      "Arousal RMSE: 0.2693074280686818\n",
      "Test R^2 score: tensor([0.2113, 0.4651], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3382120925527238\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2679749021256447\n",
      "Valence RMSE: 0.26952061810803213\n",
      "Arousal RMSE: 0.26642021836407614\n",
      "Test R^2 score: tensor([0.2238, 0.4765], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.350141893597674\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2712939484569904\n",
      "Valence RMSE: 0.2719529068258229\n",
      "Arousal RMSE: 0.2706333856118894\n",
      "Test R^2 score: tensor([0.2097, 0.4598], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33476142686017324\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27046800457005576\n",
      "Valence RMSE: 0.27163694460584614\n",
      "Arousal RMSE: 0.2692939904962898\n",
      "Test R^2 score: tensor([0.2115, 0.4652], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33834580229545413\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26885794623980463\n",
      "Valence RMSE: 0.27049293712625355\n",
      "Arousal RMSE: 0.2672129515524724\n",
      "Test R^2 score: tensor([0.2181, 0.4734], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3457765664348055\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2717447597037385\n",
      "Valence RMSE: 0.2728246278724585\n",
      "Arousal RMSE: 0.2706605831648556\n",
      "Test R^2 score: tensor([0.2046, 0.4597], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33216978795153873\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2694559376760347\n",
      "Valence RMSE: 0.2710490710552333\n",
      "Arousal RMSE: 0.2678533288533807\n",
      "Test R^2 score: tensor([0.2149, 0.4709], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3429039260926235\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26895929676190533\n",
      "Valence RMSE: 0.2703390809561301\n",
      "Arousal RMSE: 0.2675723975620659\n",
      "Test R^2 score: tensor([0.2190, 0.4720], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34551232703279144\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2720558881503072\n",
      "Valence RMSE: 0.2724163408187578\n",
      "Arousal RMSE: 0.27169495727639725\n",
      "Test R^2 score: tensor([0.2070, 0.4556], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33129054536823976\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27009067759244054\n",
      "Valence RMSE: 0.27128350707939425\n",
      "Arousal RMSE: 0.2688925566678848\n",
      "Test R^2 score: tensor([0.2136, 0.4668], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3401677259293902\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27052175413981167\n",
      "Valence RMSE: 0.27191771322741487\n",
      "Arousal RMSE: 0.26911855409639596\n",
      "Test R^2 score: tensor([0.2099, 0.4659], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3378787014811915\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2724490526133722\n",
      "Valence RMSE: 0.2732173772112046\n",
      "Arousal RMSE: 0.2716785551523683\n",
      "Test R^2 score: tensor([0.2023, 0.4557], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32898813781005903\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2701347587029835\n",
      "Valence RMSE: 0.2713119121936866\n",
      "Arousal RMSE: 0.2689524530857518\n",
      "Test R^2 score: tensor([0.2134, 0.4665], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33996658512467565\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27224610413336353\n",
      "Valence RMSE: 0.2723539907600125\n",
      "Arousal RMSE: 0.27213817473607677\n",
      "Test R^2 score: tensor([0.2073, 0.4538], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3305832111647479\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2713988299552395\n",
      "Valence RMSE: 0.27191931332829267\n",
      "Arousal RMSE: 0.27087734648954276\n",
      "Test R^2 score: tensor([0.2099, 0.4589], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33437190282096885\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2714818836763915\n",
      "Valence RMSE: 0.27225419756164987\n",
      "Arousal RMSE: 0.27070736642918597\n",
      "Test R^2 score: tensor([0.2079, 0.4595], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3337376872453034\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27320681413402387\n",
      "Valence RMSE: 0.27342989158471165\n",
      "Arousal RMSE: 0.2729835543883531\n",
      "Test R^2 score: tensor([0.2011, 0.4504], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3257464409210326\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27135940748532933\n",
      "Valence RMSE: 0.2722645328990522\n",
      "Arousal RMSE: 0.2704512528845798\n",
      "Test R^2 score: tensor([0.2079, 0.4606], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33421869553465\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2740175030182349\n",
      "Valence RMSE: 0.2732589947237747\n",
      "Arousal RMSE: 0.2747739174726175\n",
      "Test R^2 score: tensor([0.2021, 0.4432], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3226293674435745\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27183522254938447\n",
      "Valence RMSE: 0.2722803905646548\n",
      "Arousal RMSE: 0.27138932431270363\n",
      "Test R^2 score: tensor([0.2078, 0.4568], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3322982666353286\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27471209931416113\n",
      "Valence RMSE: 0.2735506442445291\n",
      "Arousal RMSE: 0.27586866449923253\n",
      "Test R^2 score: tensor([0.2004, 0.4387], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3195544198321604\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27240717576808965\n",
      "Valence RMSE: 0.2728835073738945\n",
      "Arousal RMSE: 0.27193000978784126\n",
      "Test R^2 score: tensor([0.2043, 0.4547], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3294582541550593\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27451210663421377\n",
      "Valence RMSE: 0.2735106559481422\n",
      "Arousal RMSE: 0.2755099171723676\n",
      "Test R^2 score: tensor([0.2006, 0.4402], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32040070731367354\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27291760102137286\n",
      "Valence RMSE: 0.27289939060418733\n",
      "Arousal RMSE: 0.27293581022355007\n",
      "Test R^2 score: tensor([0.2042, 0.4506], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3273911012717767\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27368430795461934\n",
      "Valence RMSE: 0.27328480621214873\n",
      "Arousal RMSE: 0.27408322738683977\n",
      "Test R^2 score: tensor([0.2019, 0.4460], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3239518806254723\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2744636794832353\n",
      "Valence RMSE: 0.27384288355174413\n",
      "Arousal RMSE: 0.27508307443210345\n",
      "Test R^2 score: tensor([0.1987, 0.4419], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.320295725844839\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Epoch 296, Loss: 0.4428728433761608\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2739735796051347\n",
      "Valence RMSE: 0.2744940939748102\n",
      "Arousal RMSE: 0.2734520744412999\n",
      "Test R^2 score: tensor([0.1948, 0.4485], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32168688099293047\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Epoch 296, Loss: 0.4428728433761608\n",
      "Epoch 297, Loss: 0.443123321879751\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2752811830397198\n",
      "Valence RMSE: 0.2741563597821272\n",
      "Arousal RMSE: 0.27640142883586594\n",
      "Test R^2 score: tensor([0.1968, 0.4366], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3166968929899154\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Epoch 296, Loss: 0.4428728433761608\n",
      "Epoch 297, Loss: 0.443123321879751\n",
      "Epoch 298, Loss: 0.44346545122591835\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2743632240393003\n",
      "Valence RMSE: 0.2748524119996985\n",
      "Arousal RMSE: 0.27387316230040976\n",
      "Test R^2 score: tensor([0.1927, 0.4468], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31978530275486833\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Epoch 296, Loss: 0.4428728433761608\n",
      "Epoch 297, Loss: 0.443123321879751\n",
      "Epoch 298, Loss: 0.44346545122591835\n",
      "Epoch 299, Loss: 0.4437371043897013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27645909754486175\n",
      "Valence RMSE: 0.27406807925415977\n",
      "Arousal RMSE: 0.27882961314138494\n",
      "Test R^2 score: tensor([0.1973, 0.4266], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3119840084559576\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.6597031161123814\n",
      "Epoch 2, Loss: 0.656605873799551\n",
      "Epoch 3, Loss: 0.6535210407988056\n",
      "Epoch 4, Loss: 0.6503875319948625\n",
      "Epoch 5, Loss: 0.647256257859055\n",
      "Epoch 6, Loss: 0.6441351395854026\n",
      "Epoch 7, Loss: 0.6410230702415904\n",
      "Epoch 8, Loss: 0.6379166316882257\n",
      "Epoch 9, Loss: 0.6348954522807412\n",
      "Epoch 10, Loss: 0.6318483660977546\n",
      "Epoch 11, Loss: 0.6288034343196786\n",
      "Epoch 12, Loss: 0.6257689752220208\n",
      "Epoch 13, Loss: 0.622768064675636\n",
      "Epoch 14, Loss: 0.6198322512591645\n",
      "Epoch 15, Loss: 0.6169952382842182\n",
      "Epoch 16, Loss: 0.6142244314757267\n",
      "Epoch 17, Loss: 0.6115146876354594\n",
      "Epoch 18, Loss: 0.6088665773047627\n",
      "Epoch 19, Loss: 0.6063658436477279\n",
      "Epoch 20, Loss: 0.6039324311852292\n",
      "Epoch 21, Loss: 0.6016328448496068\n",
      "Epoch 22, Loss: 0.5994741430218481\n",
      "Epoch 23, Loss: 0.5975562367250178\n",
      "Epoch 24, Loss: 0.5959313627666157\n",
      "Epoch 25, Loss: 0.5945818244186035\n",
      "Epoch 26, Loss: 0.5935377444290209\n",
      "Epoch 27, Loss: 0.5928172012086385\n",
      "Epoch 28, Loss: 0.5924098551055365\n",
      "Epoch 29, Loss: 0.5922751681373271\n",
      "Epoch 30, Loss: 0.5923145409549184\n",
      "Epoch 31, Loss: 0.5924115906921534\n",
      "Epoch 32, Loss: 0.5924893850268987\n",
      "Epoch 33, Loss: 0.5924908185802786\n",
      "Epoch 34, Loss: 0.5923923983428768\n",
      "Epoch 35, Loss: 0.5922411016195904\n",
      "Epoch 36, Loss: 0.5920704376146074\n",
      "Epoch 37, Loss: 0.5919183537155156\n",
      "Epoch 38, Loss: 0.591788893038361\n",
      "Epoch 39, Loss: 0.5916832792833296\n",
      "Epoch 40, Loss: 0.5916003413926094\n",
      "Epoch 41, Loss: 0.5915310454633497\n",
      "Epoch 42, Loss: 0.5914609352136103\n",
      "Epoch 43, Loss: 0.5913800575170709\n",
      "Epoch 44, Loss: 0.5912804947342168\n",
      "Epoch 45, Loss: 0.5911589101501744\n",
      "Epoch 46, Loss: 0.59101430687313\n",
      "Epoch 47, Loss: 0.5908459618415497\n",
      "Epoch 48, Loss: 0.5906542584069969\n",
      "Epoch 49, Loss: 0.590439375716818\n",
      "Epoch 50, Loss: 0.5902009097394456\n",
      "Epoch 51, Loss: 0.589934992522504\n",
      "Epoch 52, Loss: 0.5896435075612552\n",
      "Epoch 53, Loss: 0.5893292488390647\n",
      "Epoch 54, Loss: 0.5889885620745567\n",
      "Epoch 55, Loss: 0.5886062879166241\n",
      "Epoch 56, Loss: 0.5881572891679371\n",
      "Epoch 57, Loss: 0.5876137978338659\n",
      "Epoch 58, Loss: 0.5869440715309391\n",
      "Epoch 59, Loss: 0.5861162294548011\n",
      "Epoch 60, Loss: 0.5851048211389537\n",
      "Epoch 61, Loss: 0.5838907734547811\n",
      "Epoch 62, Loss: 0.5824154603375897\n",
      "Epoch 63, Loss: 0.580623768249782\n",
      "Epoch 64, Loss: 0.5785190682627475\n",
      "Epoch 65, Loss: 0.5761011629840784\n",
      "Epoch 66, Loss: 0.5733680911765417\n",
      "Epoch 67, Loss: 0.5703255900095718\n",
      "Epoch 68, Loss: 0.5670054718994832\n",
      "Epoch 69, Loss: 0.5634286420467546\n",
      "Epoch 70, Loss: 0.5596936474435344\n",
      "Epoch 71, Loss: 0.5559769538916075\n",
      "Epoch 72, Loss: 0.5523958011689292\n",
      "Epoch 73, Loss: 0.5490681405122388\n",
      "Epoch 74, Loss: 0.5460365134438362\n",
      "Epoch 75, Loss: 0.5432108332562865\n",
      "Epoch 76, Loss: 0.5403612496759619\n",
      "Epoch 77, Loss: 0.5373774466413413\n",
      "Epoch 78, Loss: 0.534284898623987\n",
      "Epoch 79, Loss: 0.5313141727957414\n",
      "Epoch 80, Loss: 0.5287824995906365\n",
      "Epoch 81, Loss: 0.5266062630598586\n",
      "Epoch 82, Loss: 0.5245867965054697\n",
      "Epoch 83, Loss: 0.5225766918261382\n",
      "Epoch 84, Loss: 0.5205286533556375\n",
      "Epoch 85, Loss: 0.5185017531796633\n",
      "Epoch 86, Loss: 0.516613590554756\n",
      "Epoch 87, Loss: 0.5148909285235399\n",
      "Epoch 88, Loss: 0.5133319498741201\n",
      "Epoch 89, Loss: 0.5118400779334811\n",
      "Epoch 90, Loss: 0.510391932167144\n",
      "Epoch 91, Loss: 0.5090074125567072\n",
      "Epoch 92, Loss: 0.5077074088880127\n",
      "Epoch 93, Loss: 0.5064904536715531\n",
      "Epoch 94, Loss: 0.5053770498026138\n",
      "Epoch 95, Loss: 0.5042762285256632\n",
      "Epoch 96, Loss: 0.5031666910182664\n",
      "Epoch 97, Loss: 0.5021487200041443\n",
      "Epoch 98, Loss: 0.501281258947333\n",
      "Epoch 99, Loss: 0.5003343001029535\n",
      "Epoch 100, Loss: 0.4994733540260066\n",
      "Epoch 101, Loss: 0.4986956690114285\n",
      "Epoch 102, Loss: 0.4979247747609699\n",
      "Epoch 103, Loss: 0.49724536803549313\n",
      "Epoch 104, Loss: 0.4965889629229062\n",
      "Epoch 105, Loss: 0.49604849721613453\n",
      "Epoch 106, Loss: 0.4954364507198952\n",
      "Epoch 107, Loss: 0.49475130786597593\n",
      "Epoch 108, Loss: 0.4941229232813927\n",
      "Epoch 109, Loss: 0.49365549812856097\n",
      "Epoch 110, Loss: 0.49326630089685214\n",
      "Epoch 111, Loss: 0.49271961996311736\n",
      "Epoch 112, Loss: 0.49217731223083266\n",
      "Epoch 113, Loss: 0.49178686942299177\n",
      "Epoch 114, Loss: 0.4914347932952779\n",
      "Epoch 115, Loss: 0.4910258253399118\n",
      "Epoch 116, Loss: 0.49055045223617666\n",
      "Epoch 117, Loss: 0.4901595191493897\n",
      "Epoch 118, Loss: 0.48987459757896146\n",
      "Epoch 119, Loss: 0.48956415632579653\n",
      "Epoch 120, Loss: 0.4892483251569303\n",
      "Epoch 121, Loss: 0.4888519156703771\n",
      "Epoch 122, Loss: 0.488499813754301\n",
      "Epoch 123, Loss: 0.4882082464957051\n",
      "Epoch 124, Loss: 0.48793724148883816\n",
      "Epoch 125, Loss: 0.4876219609118672\n",
      "Epoch 126, Loss: 0.48722997792200706\n",
      "Epoch 127, Loss: 0.48684456608938514\n",
      "Epoch 128, Loss: 0.48652941969260727\n",
      "Epoch 129, Loss: 0.48627716777973945\n",
      "Epoch 130, Loss: 0.4860368185568679\n",
      "Epoch 131, Loss: 0.48578550715404695\n",
      "Epoch 132, Loss: 0.48549863248803377\n",
      "Epoch 133, Loss: 0.48520016371664476\n",
      "Epoch 134, Loss: 0.4848977006602432\n",
      "Epoch 135, Loss: 0.4845858545666896\n",
      "Epoch 136, Loss: 0.48433549781432933\n",
      "Epoch 137, Loss: 0.48417204019494015\n",
      "Epoch 138, Loss: 0.48407077464548015\n",
      "Epoch 139, Loss: 0.4841456793336131\n",
      "Epoch 140, Loss: 0.4836477080087057\n",
      "Epoch 141, Loss: 0.4830031503096806\n",
      "Epoch 142, Loss: 0.48265680006199024\n",
      "Epoch 143, Loss: 0.4826986623093519\n",
      "Epoch 144, Loss: 0.4825621644181833\n",
      "Epoch 145, Loss: 0.4819852420607281\n",
      "Epoch 146, Loss: 0.48168386928842716\n",
      "Epoch 147, Loss: 0.4816240980742321\n",
      "Epoch 148, Loss: 0.4813352695077875\n",
      "Epoch 149, Loss: 0.4808787790531241\n",
      "Epoch 150, Loss: 0.48063117282509804\n",
      "Epoch 151, Loss: 0.4805077792807182\n",
      "Epoch 152, Loss: 0.48014670333036447\n",
      "Epoch 153, Loss: 0.47966224156583664\n",
      "Epoch 154, Loss: 0.4793527715122394\n",
      "Epoch 155, Loss: 0.47912112482615166\n",
      "Epoch 156, Loss: 0.47874955667818825\n",
      "Epoch 157, Loss: 0.47825084289281083\n",
      "Epoch 158, Loss: 0.47780383362280016\n",
      "Epoch 159, Loss: 0.4774099694254637\n",
      "Epoch 160, Loss: 0.4770181228875681\n",
      "Epoch 161, Loss: 0.4765691600396721\n",
      "Epoch 162, Loss: 0.4760329634568554\n",
      "Epoch 163, Loss: 0.4754564081471238\n",
      "Epoch 164, Loss: 0.4749217006496656\n",
      "Epoch 165, Loss: 0.4744132621824346\n",
      "Epoch 166, Loss: 0.47387783359011454\n",
      "Epoch 167, Loss: 0.4732695139868392\n",
      "Epoch 168, Loss: 0.47271247403262057\n",
      "Epoch 169, Loss: 0.47219034356927914\n",
      "Epoch 170, Loss: 0.4716857372709449\n",
      "Epoch 171, Loss: 0.4712282156525772\n",
      "Epoch 172, Loss: 0.4709007671334966\n",
      "Epoch 173, Loss: 0.4707155819586018\n",
      "Epoch 174, Loss: 0.47089592558397686\n",
      "Epoch 175, Loss: 0.4708192294323626\n",
      "Epoch 176, Loss: 0.47003309752214883\n",
      "Epoch 177, Loss: 0.4689854030798034\n",
      "Epoch 178, Loss: 0.46913328395312176\n",
      "Epoch 179, Loss: 0.469397701369705\n",
      "Epoch 180, Loss: 0.4681986427188055\n",
      "Epoch 181, Loss: 0.4677189773362255\n",
      "Epoch 182, Loss: 0.46801816714413413\n",
      "Epoch 183, Loss: 0.4671409009575455\n",
      "Epoch 184, Loss: 0.46674547750430473\n",
      "Epoch 185, Loss: 0.4668575231505885\n",
      "Epoch 186, Loss: 0.4661903431134841\n",
      "Epoch 187, Loss: 0.46578885018163474\n",
      "Epoch 188, Loss: 0.4658274300149455\n",
      "Epoch 189, Loss: 0.46539589602751463\n",
      "Epoch 190, Loss: 0.464854023924057\n",
      "Epoch 191, Loss: 0.46471063326709966\n",
      "Epoch 192, Loss: 0.4644993379668249\n",
      "Epoch 193, Loss: 0.4640611102664573\n",
      "Epoch 194, Loss: 0.46367324790333553\n",
      "Epoch 195, Loss: 0.4634430241014394\n",
      "Epoch 196, Loss: 0.46337171896695706\n",
      "Epoch 197, Loss: 0.46346279787762446\n",
      "Epoch 198, Loss: 0.4634363683479676\n",
      "Epoch 199, Loss: 0.4626372887495186\n",
      "Epoch 200, Loss: 0.4619068846840482\n",
      "Epoch 201, Loss: 0.4616068974025482\n",
      "Epoch 202, Loss: 0.461683590836408\n",
      "Epoch 203, Loss: 0.4618194838106924\n",
      "Epoch 204, Loss: 0.4611732235771984\n",
      "Epoch 205, Loss: 0.46048944323861896\n",
      "Epoch 206, Loss: 0.45997862569243647\n",
      "Epoch 207, Loss: 0.4599314718669155\n",
      "Epoch 208, Loss: 0.4602514916447095\n",
      "Epoch 209, Loss: 0.46018770625265115\n",
      "Epoch 210, Loss: 0.46028184326390564\n",
      "Epoch 211, Loss: 0.45892090054155593\n",
      "Epoch 212, Loss: 0.45788541675333344\n",
      "Epoch 213, Loss: 0.45762004582761595\n",
      "Epoch 214, Loss: 0.45789246235133724\n",
      "Epoch 215, Loss: 0.4580031600097686\n",
      "Epoch 216, Loss: 0.4571902169794167\n",
      "Epoch 217, Loss: 0.45619877370901996\n",
      "Epoch 218, Loss: 0.4555021447163662\n",
      "Epoch 219, Loss: 0.45567042562568627\n",
      "Epoch 220, Loss: 0.4560765785991673\n",
      "Epoch 221, Loss: 0.45611219023188254\n",
      "Epoch 222, Loss: 0.456274433004332\n",
      "Epoch 223, Loss: 0.4556183453192943\n",
      "Epoch 224, Loss: 0.4539742075855367\n",
      "Epoch 225, Loss: 0.45344011267889306\n",
      "Epoch 226, Loss: 0.4542595373211642\n",
      "Epoch 227, Loss: 0.4534706408839997\n",
      "Epoch 228, Loss: 0.4524211349573474\n",
      "Epoch 229, Loss: 0.4527217780953512\n",
      "Epoch 230, Loss: 0.45258088163553384\n",
      "Epoch 231, Loss: 0.4517596820771173\n",
      "Epoch 232, Loss: 0.45158482928655086\n",
      "Epoch 233, Loss: 0.4518071280880513\n",
      "Epoch 234, Loss: 0.4517654873484232\n",
      "Epoch 235, Loss: 0.4511512157119408\n",
      "Epoch 236, Loss: 0.45055980366823306\n",
      "Epoch 237, Loss: 0.4502523555736359\n",
      "Epoch 238, Loss: 0.45019729845515055\n",
      "Epoch 239, Loss: 0.4503587934853364\n",
      "Epoch 240, Loss: 0.45074143457023375\n",
      "Epoch 241, Loss: 0.4528586163675198\n",
      "Epoch 242, Loss: 0.45445747783924634\n",
      "Epoch 243, Loss: 0.4545922694156887\n",
      "Epoch 244, Loss: 0.4495286950339436\n",
      "Epoch 245, Loss: 0.4531609257892924\n",
      "Epoch 246, Loss: 0.45475711879432545\n",
      "Epoch 247, Loss: 0.44950209264830365\n",
      "Epoch 248, Loss: 0.45362870402648975\n",
      "Epoch 249, Loss: 0.4497173693335331\n",
      "Epoch 250, Loss: 0.4520415968754111\n",
      "Epoch 251, Loss: 0.45019850657358146\n",
      "Epoch 252, Loss: 0.45025884217280604\n",
      "Epoch 253, Loss: 0.45072535097173166\n",
      "Epoch 254, Loss: 0.4488955952501538\n",
      "Epoch 255, Loss: 0.4501585045453185\n",
      "Epoch 256, Loss: 0.44813524686731027\n",
      "Epoch 257, Loss: 0.44961674516007877\n",
      "Epoch 258, Loss: 0.44778563741653266\n",
      "Epoch 259, Loss: 0.4490705162612374\n",
      "Epoch 260, Loss: 0.4482919004710167\n",
      "Epoch 261, Loss: 0.44802548690870764\n",
      "Epoch 262, Loss: 0.4483711206838895\n",
      "Epoch 263, Loss: 0.44704795104865896\n",
      "Epoch 264, Loss: 0.4479374233877575\n",
      "Epoch 265, Loss: 0.4468172482873088\n",
      "Epoch 266, Loss: 0.44723940443449467\n",
      "Epoch 267, Loss: 0.4471086451477674\n",
      "Epoch 268, Loss: 0.4464159728724833\n",
      "Epoch 269, Loss: 0.4470232844251374\n",
      "Epoch 270, Loss: 0.4464297083431481\n",
      "Epoch 271, Loss: 0.44605992150520196\n",
      "Epoch 272, Loss: 0.44649033630075974\n",
      "Epoch 273, Loss: 0.44601406915673614\n",
      "Epoch 274, Loss: 0.4455574432783866\n",
      "Epoch 275, Loss: 0.44582738723830206\n",
      "Epoch 276, Loss: 0.44576909264941955\n",
      "Epoch 277, Loss: 0.44517734216521765\n",
      "Epoch 278, Loss: 0.44524478397494843\n",
      "Epoch 279, Loss: 0.4453893896278816\n",
      "Epoch 280, Loss: 0.4448819002675873\n",
      "Epoch 281, Loss: 0.444694424734129\n",
      "Epoch 282, Loss: 0.44490529566177034\n",
      "Epoch 283, Loss: 0.4447618895158909\n",
      "Epoch 284, Loss: 0.44449351105282164\n",
      "Epoch 285, Loss: 0.44416081257672624\n",
      "Epoch 286, Loss: 0.44406996930771286\n",
      "Epoch 287, Loss: 0.44420296492357847\n",
      "Epoch 288, Loss: 0.4442162656496547\n",
      "Epoch 289, Loss: 0.4443832036963249\n",
      "Epoch 290, Loss: 0.44428977307297685\n",
      "Epoch 291, Loss: 0.4443861712890498\n",
      "Epoch 292, Loss: 0.4440532581621308\n",
      "Epoch 293, Loss: 0.44380149139602393\n",
      "Epoch 294, Loss: 0.44316949013033796\n",
      "Epoch 295, Loss: 0.44282546644266474\n",
      "Epoch 296, Loss: 0.4428728433761608\n",
      "Epoch 297, Loss: 0.443123321879751\n",
      "Epoch 298, Loss: 0.44346545122591835\n",
      "Epoch 299, Loss: 0.4437371043897013\n",
      "Epoch 300, Loss: 0.44465675928659193\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27456092949762445\n",
      "Valence RMSE: 0.27502839219706254\n",
      "Arousal RMSE: 0.2740926695457172\n",
      "Test R^2 score: tensor([0.1917, 0.4459], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3188247328254775\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship between the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.252418742693043\n",
      "Corresponding R^2 SCore: 0.4294175562780569\n",
      "Corresponding num_epochs: 111\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.4297347177742974\n",
      "Corresponding RMSE: 0.25242890470958096\n",
      "Corresponding num_epochs: 109\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjsuted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.35238747883047805\n",
      "Corresponding RMSE: 0.25242890470958096\n",
      "Corresponding num_epochs: 109\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.5094110673130416\n",
      "Corresponding RMSE: 0.2527105666519095\n",
      "Corresponding num_epochs: 110\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
